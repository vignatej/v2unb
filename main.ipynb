{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "# for load_student_model\n",
    "from typing import Any\n",
    "\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.checkpoint.c2_model_loading import align_and_update_state_dicts\n",
    "from fvcore.common.checkpoint import _IncompatibleKeys, _strip_prefix_if_present\n",
    "\n",
    "\n",
    "class DetectionTSCheckpointer(DetectionCheckpointer):\n",
    "    def _load_model(self, checkpoint):\n",
    "        if checkpoint.get(\"__author__\", None) == \"Caffe2\":\n",
    "            # pretrained model weight: only update student model\n",
    "            if checkpoint.get(\"matching_heuristics\", False):\n",
    "                self._convert_ndarray_to_tensor(checkpoint[\"model\"])\n",
    "                # convert weights by name-matching heuristics\n",
    "                checkpoint[\"model\"] = align_and_update_state_dicts(\n",
    "                    self.model.modelStudent.state_dict(),\n",
    "                    checkpoint[\"model\"],\n",
    "                    c2_conversion=checkpoint.get(\"__author__\", None) == \"Caffe2\",\n",
    "                )\n",
    "\n",
    "            # for non-caffe2 models, use standard ways to load it\n",
    "            incompatible = self._load_student_model(checkpoint)\n",
    "\n",
    "            model_buffers = dict(self.model.modelStudent.named_buffers(recurse=False))\n",
    "            for k in [\"pixel_mean\", \"pixel_std\"]:\n",
    "                # Ignore missing key message about pixel_mean/std.\n",
    "                # Though they may be missing in old checkpoints, they will be correctly\n",
    "                # initialized from config anyway.\n",
    "                if k in model_buffers:\n",
    "                    try:\n",
    "                        incompatible.missing_keys.remove(k)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            return incompatible\n",
    "\n",
    "        else:  # whole model\n",
    "            if checkpoint.get(\"matching_heuristics\", False):\n",
    "                self._convert_ndarray_to_tensor(checkpoint[\"model\"])\n",
    "                # convert weights by name-matching heuristics\n",
    "                checkpoint[\"model\"] = align_and_update_state_dicts(\n",
    "                    self.model.state_dict(),\n",
    "                    checkpoint[\"model\"],\n",
    "                    c2_conversion=checkpoint.get(\"__author__\", None) == \"Caffe2\",\n",
    "                )\n",
    "            # for non-caffe2 models, use standard ways to load it\n",
    "            incompatible = super()._load_model(checkpoint)\n",
    "\n",
    "            model_buffers = dict(self.model.named_buffers(recurse=False))\n",
    "            for k in [\"pixel_mean\", \"pixel_std\"]:\n",
    "                # Ignore missing key message about pixel_mean/std.\n",
    "                # Though they may be missing in old checkpoints, they will be correctly\n",
    "                # initialized from config anyway.\n",
    "                if k in model_buffers:\n",
    "                    try:\n",
    "                        incompatible.missing_keys.remove(k)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "            return incompatible\n",
    "\n",
    "    def _load_student_model(self, checkpoint: Any) -> _IncompatibleKeys:  # pyre-ignore\n",
    "        checkpoint_state_dict = checkpoint.pop(\"model\")\n",
    "        self._convert_ndarray_to_tensor(checkpoint_state_dict)\n",
    "\n",
    "        # if the state_dict comes from a model that was wrapped in a\n",
    "        # DataParallel or DistributedDataParallel during serialization,\n",
    "        # remove the \"module\" prefix before performing the matching.\n",
    "        _strip_prefix_if_present(checkpoint_state_dict, \"module.\")\n",
    "\n",
    "        # work around https://github.com/pytorch/pytorch/issues/24139\n",
    "        model_state_dict = self.model.modelStudent.state_dict()\n",
    "        incorrect_shapes = []\n",
    "        for k in list(checkpoint_state_dict.keys()):\n",
    "            if k in model_state_dict:\n",
    "                shape_model = tuple(model_state_dict[k].shape)\n",
    "                shape_checkpoint = tuple(checkpoint_state_dict[k].shape)\n",
    "                if shape_model != shape_checkpoint:\n",
    "                    incorrect_shapes.append((k, shape_checkpoint, shape_model))\n",
    "                    checkpoint_state_dict.pop(k)\n",
    "        # pyre-ignore\n",
    "        incompatible = self.model.modelStudent.load_state_dict(\n",
    "            checkpoint_state_dict, strict=False\n",
    "        )\n",
    "        return _IncompatibleKeys(\n",
    "            missing_keys=incompatible.missing_keys,\n",
    "            unexpected_keys=incompatible.unexpected_keys,\n",
    "            incorrect_shapes=incorrect_shapes,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import contextlib\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.utils.file_io import PathManager\n",
    "from fvcore.common.timer import Timer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "JSON_ANNOTATIONS_DIR = \"\"\n",
    "_SPLITS_COCO_FORMAT = {}\n",
    "_SPLITS_COCO_FORMAT[\"coco\"] = {\n",
    "    \"coco_2017_unlabel\": (\n",
    "        \"memcache_manifold://mobile_vision_dataset/tree/coco_unlabel2017\",\n",
    "        \"memcache_manifold://mobile_vision_dataset/tree/coco_unlabel2017/coco_jsons/image_info_unlabeled2017.json\",\n",
    "    ),\n",
    "    \"coco_2017_for_voc20\": (\n",
    "        \"coco\",\n",
    "        \"coco/annotations/google/instances_unlabeledtrainval20class.json\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def register_coco_unlabel():\n",
    "    for _, splits_per_dataset in _SPLITS_COCO_FORMAT.items():\n",
    "        for key, (image_root, json_file) in splits_per_dataset.items():\n",
    "            meta = {}\n",
    "            register_coco_unlabel_instances(key, meta, json_file, image_root)\n",
    "\n",
    "\n",
    "def register_coco_unlabel_instances(name, metadata, json_file, image_root):\n",
    "    \"\"\"\n",
    "    Register a dataset in COCO's json annotation format for\n",
    "    instance detection, instance segmentation and keypoint detection.\n",
    "    (i.e., Type 1 and 2 in http://cocodataset.org/#format-data.\n",
    "    `instances*.json` and `person_keypoints*.json` in the dataset).\n",
    "\n",
    "    This is an example of how to register a new dataset.\n",
    "    You can do something similar to this function, to register new datasets.\n",
    "\n",
    "    Args:\n",
    "        name (str): the name that identifies a dataset, e.g. \"coco_2014_train\".\n",
    "        metadata (dict): extra metadata associated with this dataset.  You can\n",
    "            leave it as an empty dict.\n",
    "        json_file (str): path to the json instance annotation file.\n",
    "        image_root (str or path-like): directory which contains all the images.\n",
    "    \"\"\"\n",
    "    assert isinstance(name, str), name\n",
    "    assert isinstance(json_file, (str, os.PathLike)), json_file\n",
    "    assert isinstance(image_root, (str, os.PathLike)), image_root\n",
    "\n",
    "    # 1. register a function which returns dicts\n",
    "    DatasetCatalog.register(\n",
    "        name, lambda: load_coco_unlabel_json(json_file, image_root, name)\n",
    "    )\n",
    "\n",
    "    # 2. Optionally, add metadata about this dataset,\n",
    "    # since they might be useful in evaluation, visualization or logging\n",
    "    MetadataCatalog.get(name).set(\n",
    "        json_file=json_file, image_root=image_root, evaluator_type=\"coco\", **metadata\n",
    "    )\n",
    "\n",
    "\n",
    "def load_coco_unlabel_json(\n",
    "    json_file, image_root, dataset_name=None, extra_annotation_keys=None\n",
    "):\n",
    "    from pycocotools.coco import COCO\n",
    "\n",
    "    timer = Timer()\n",
    "    json_file = PathManager.get_local_path(json_file)\n",
    "    with contextlib.redirect_stdout(io.StringIO()):\n",
    "        coco_api = COCO(json_file)\n",
    "    if timer.seconds() > 1:\n",
    "        logger.info(\n",
    "            \"Loading {} takes {:.2f} seconds.\".format(json_file, timer.seconds())\n",
    "        )\n",
    "\n",
    "    # sort indices for reproducible results\n",
    "    img_ids = sorted(coco_api.imgs.keys())\n",
    "\n",
    "    imgs = coco_api.loadImgs(img_ids)\n",
    "\n",
    "    logger.info(\"Loaded {} images in COCO format from {}\".format(len(imgs), json_file))\n",
    "\n",
    "    dataset_dicts = []\n",
    "\n",
    "    for img_dict in imgs:\n",
    "        record = {}\n",
    "        record[\"file_name\"] = os.path.join(image_root, img_dict[\"file_name\"])\n",
    "        record[\"height\"] = img_dict[\"height\"]\n",
    "        record[\"width\"] = img_dict[\"width\"]\n",
    "\n",
    "        dataset_dicts.append(record)\n",
    "\n",
    "    return dataset_dicts\n",
    "\n",
    "\n",
    "register_coco_unlabel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import random\n",
    "\n",
    "from PIL import ImageFilter\n",
    "\n",
    "\n",
    "class GaussianBlur:\n",
    "    \"\"\"\n",
    "    Gaussian blur augmentation in SimCLR https://arxiv.org/abs/2002.05709\n",
    "    Adapted from MoCo:\n",
    "    https://github.com/facebookresearch/moco/blob/master/moco/loader.py\n",
    "    Note that this implementation does not seem to be exactly the same as\n",
    "    described in SimCLR.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, x):\n",
    "        sigma = random.uniform(self.sigma[0], self.sigma[1])\n",
    "        x = x.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import logging\n",
    "\n",
    "from detectron2.data.common import AspectRatioGroupedDataset, MapDataset\n",
    "\n",
    "\n",
    "class MapDatasetTwoCrop(MapDataset):\n",
    "    \"\"\"\n",
    "    Map a function over the elements in a dataset.\n",
    "\n",
    "    This customized MapDataset transforms an image with two augmentations\n",
    "    as two inputs (queue and key).\n",
    "\n",
    "    Args:\n",
    "        dataset: a dataset where map function is applied.\n",
    "        map_func: a callable which maps the element in dataset. map_func is\n",
    "            responsible for error handling, when error happens, it needs to\n",
    "            return None so the MapDataset will randomly use other\n",
    "            elements from the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        retry_count = 0\n",
    "        cur_idx = int(idx)\n",
    "\n",
    "        while True:\n",
    "            data = self._map_func(self._dataset[cur_idx])\n",
    "            if data is not None:\n",
    "                self._fallback_candidates.add(cur_idx)\n",
    "                return data\n",
    "\n",
    "            # _map_func fails for this idx, use a random new index from the pool\n",
    "            retry_count += 1\n",
    "            self._fallback_candidates.discard(cur_idx)\n",
    "            cur_idx = self._rng.sample(self._fallback_candidates, k=1)[0]\n",
    "\n",
    "            if retry_count >= 3:\n",
    "                logger = logging.getLogger(__name__)\n",
    "                logger.warning(\n",
    "                    \"Failed to apply `_map_func` for idx: {}, retry count: {}\".format(\n",
    "                        idx, retry_count\n",
    "                    )\n",
    "                )\n",
    "\n",
    "\n",
    "class AspectRatioGroupedDatasetTwoCrop(AspectRatioGroupedDataset):\n",
    "    \"\"\"\n",
    "    Batch data that have similar aspect ratio together.\n",
    "    In this implementation, images whose aspect ratio < (or >) 1 will\n",
    "    be batched together.\n",
    "    This improves training speed because the images then need less padding\n",
    "    to form a batch.\n",
    "\n",
    "    It assumes the underlying dataset produces dicts with \"width\" and \"height\" keys.\n",
    "    It will then produce a list of original dicts with length = batch_size,\n",
    "    all with similar aspect ratios.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: an iterable. Each element must be a dict with keys\n",
    "                \"width\" and \"height\", which will be used to batch data.\n",
    "            batch_size (int):\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self._buckets = [[] for _ in range(2)]\n",
    "        self._buckets_key = [[] for _ in range(2)]\n",
    "        # Hard-coded two aspect ratio groups: w > h and w < h.\n",
    "        # Can add support for more aspect ratio groups, but doesn't seem useful\n",
    "\n",
    "    def __iter__(self):\n",
    "        for d in self.dataset:\n",
    "            # d is a tuple with len = 2\n",
    "            # It's two images (same size) from the same image instance\n",
    "            w, h = d[0][\"width\"], d[0][\"height\"]\n",
    "            bucket_id = 0 if w > h else 1\n",
    "\n",
    "            # bucket = bucket for normal images\n",
    "            bucket = self._buckets[bucket_id]\n",
    "            bucket.append(d[0])\n",
    "\n",
    "            # buckets_key = bucket for augmented images\n",
    "            buckets_key = self._buckets_key[bucket_id]\n",
    "            buckets_key.append(d[1])\n",
    "            if len(bucket) == self.batch_size:\n",
    "                yield (bucket[:], buckets_key[:])\n",
    "                del bucket[:]\n",
    "                del buckets_key[:]\n",
    "\n",
    "\n",
    "class AspectRatioGroupedSemiSupDatasetTwoCrop(AspectRatioGroupedDataset):\n",
    "    \"\"\"\n",
    "    Batch data that have similar aspect ratio together.\n",
    "    In this implementation, images whose aspect ratio < (or >) 1 will\n",
    "    be batched together.\n",
    "    This improves training speed because the images then need less padding\n",
    "    to form a batch.\n",
    "\n",
    "    It assumes the underlying dataset produces dicts with \"width\" and \"height\" keys.\n",
    "    It will then produce a list of original dicts with length = batch_size,\n",
    "    all with similar aspect ratios.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset: a tuple containing two iterable generators. （labeled and unlabeled data)\n",
    "               Each element must be a dict with keys \"width\" and \"height\", which will be used\n",
    "               to batch data.\n",
    "            batch_size (int):\n",
    "        \"\"\"\n",
    "\n",
    "        self.label_dataset, self.unlabel_dataset = dataset\n",
    "        self.batch_size_label = batch_size[0]\n",
    "        self.batch_size_unlabel = batch_size[1]\n",
    "\n",
    "        self._label_buckets = [[] for _ in range(2)]\n",
    "        self._label_buckets_key = [[] for _ in range(2)]\n",
    "        self._unlabel_buckets = [[] for _ in range(2)]\n",
    "        self._unlabel_buckets_key = [[] for _ in range(2)]\n",
    "        # Hard-coded two aspect ratio groups: w > h and w < h.\n",
    "        # Can add support for more aspect ratio groups, but doesn't seem useful\n",
    "\n",
    "    def __iter__(self):\n",
    "        label_bucket, unlabel_bucket = [], []\n",
    "        for d_label, d_unlabel in zip(self.label_dataset, self.unlabel_dataset):\n",
    "            # d is a tuple with len = 2\n",
    "            # It's two images (same size) from the same image instance\n",
    "            # d[0] is with strong augmentation, d[1] is with weak augmentation\n",
    "\n",
    "            # because we are grouping images with their aspect ratio\n",
    "            # label and unlabel buckets might not have the same number of data\n",
    "            # i.e., one could reach batch_size, while the other is still not\n",
    "            if len(label_bucket) != self.batch_size_label:\n",
    "                w, h = d_label[0][\"width\"], d_label[0][\"height\"]\n",
    "                label_bucket_id = 0 if w > h else 1\n",
    "                label_bucket = self._label_buckets[label_bucket_id]\n",
    "                label_bucket.append(d_label[0])\n",
    "                label_buckets_key = self._label_buckets_key[label_bucket_id]\n",
    "                label_buckets_key.append(d_label[1])\n",
    "\n",
    "            if len(unlabel_bucket) != self.batch_size_unlabel:\n",
    "                w, h = d_unlabel[0][\"width\"], d_unlabel[0][\"height\"]\n",
    "                unlabel_bucket_id = 0 if w > h else 1\n",
    "                unlabel_bucket = self._unlabel_buckets[unlabel_bucket_id]\n",
    "                unlabel_bucket.append(d_unlabel[0])\n",
    "                unlabel_buckets_key = self._unlabel_buckets_key[unlabel_bucket_id]\n",
    "                unlabel_buckets_key.append(d_unlabel[1])\n",
    "\n",
    "            # yield the batch of data until all buckets are full\n",
    "            if (\n",
    "                len(label_bucket) == self.batch_size_label\n",
    "                and len(unlabel_bucket) == self.batch_size_unlabel\n",
    "            ):\n",
    "                # label_strong, label_weak, unlabed_strong, unlabled_weak\n",
    "                yield (\n",
    "                    label_bucket[:],\n",
    "                    label_buckets_key[:],\n",
    "                    unlabel_bucket[:],\n",
    "                    unlabel_buckets_key[:],\n",
    "                )\n",
    "                del label_bucket[:]\n",
    "                del label_buckets_key[:]\n",
    "                del unlabel_bucket[:]\n",
    "                del unlabel_buckets_key[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import json\n",
    "import logging\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "from detectron2.data.build import (\n",
    "    build_batch_data_loader,\n",
    "    get_detection_dataset_dicts,\n",
    "    trivial_batch_collator,\n",
    "    worker_init_reset_seed,\n",
    ")\n",
    "from detectron2.data.common import DatasetFromList, MapDataset\n",
    "from detectron2.data.dataset_mapper import DatasetMapper\n",
    "from detectron2.data.samplers import (\n",
    "    InferenceSampler,\n",
    "    RepeatFactorTrainingSampler,\n",
    "    TrainingSampler,\n",
    ")\n",
    "from detectron2.utils.comm import get_world_size\n",
    "from detectron2.utils.file_io import PathManager\n",
    "# from ubteacher.data.common import AspectRatioGroupedSemiSupDatasetTwoCrop\n",
    "\n",
    "\"\"\"\n",
    "This file contains the default logic to build a dataloader for training or testing.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def divide_label_unlabel(\n",
    "    dataset_dicts, SupPercent, random_data_seed, random_data_seed_path\n",
    "):\n",
    "    num_all = len(dataset_dicts)\n",
    "    num_label = int(SupPercent / 100.0 * num_all)\n",
    "\n",
    "    # read from pre-generated data seed\n",
    "    with PathManager.open(random_data_seed_path, \"r\") as COCO_sup_file:\n",
    "        coco_random_idx = json.load(COCO_sup_file)\n",
    "\n",
    "    labeled_idx = np.array(coco_random_idx[str(SupPercent)][str(random_data_seed)])\n",
    "    assert labeled_idx.shape[0] == num_label, \"Number of READ_DATA is mismatched.\"\n",
    "\n",
    "    label_dicts = []\n",
    "    unlabel_dicts = []\n",
    "    labeled_idx = set(labeled_idx)\n",
    "\n",
    "    for i in range(len(dataset_dicts)):\n",
    "        if i in labeled_idx:\n",
    "            label_dicts.append(dataset_dicts[i])\n",
    "        else:\n",
    "            unlabel_dicts.append(dataset_dicts[i])\n",
    "\n",
    "    return label_dicts, unlabel_dicts\n",
    "\n",
    "\n",
    "# uesed by supervised-only baseline trainer\n",
    "def build_detection_semisup_train_loader(cfg, mapper=None):\n",
    "\n",
    "    dataset_dicts = get_detection_dataset_dicts(\n",
    "        cfg.DATASETS.TRAIN,\n",
    "        filter_empty=cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS,\n",
    "        min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE\n",
    "        if cfg.MODEL.KEYPOINT_ON\n",
    "        else 0,\n",
    "        proposal_files=cfg.DATASETS.PROPOSAL_FILES_TRAIN\n",
    "        if cfg.MODEL.LOAD_PROPOSALS\n",
    "        else None,\n",
    "    )\n",
    "\n",
    "    # Divide into labeled and unlabeled sets according to supervision percentage\n",
    "    label_dicts, unlabel_dicts = divide_label_unlabel(\n",
    "        dataset_dicts,\n",
    "        cfg.DATALOADER.SUP_PERCENT,\n",
    "        cfg.DATALOADER.RANDOM_DATA_SEED,\n",
    "        cfg.DATALOADER.RANDOM_DATA_SEED_PATH,\n",
    "    )\n",
    "\n",
    "    dataset = DatasetFromList(label_dicts, copy=False)\n",
    "\n",
    "    if mapper is None:\n",
    "        mapper = DatasetMapper(cfg, True)\n",
    "    dataset = MapDataset(dataset, mapper)\n",
    "\n",
    "    sampler_name = cfg.DATALOADER.SAMPLER_TRAIN\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"Using training sampler {}\".format(sampler_name))\n",
    "\n",
    "    if sampler_name == \"TrainingSampler\":\n",
    "        sampler = TrainingSampler(len(dataset))\n",
    "    elif sampler_name == \"RepeatFactorTrainingSampler\":\n",
    "        repeat_factors = (\n",
    "            RepeatFactorTrainingSampler.repeat_factors_from_category_frequency(\n",
    "                label_dicts, cfg.DATALOADER.REPEAT_THRESHOLD\n",
    "            )\n",
    "        )\n",
    "        sampler = RepeatFactorTrainingSampler(repeat_factors)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown training sampler: {}\".format(sampler_name))\n",
    "\n",
    "    # list num of labeled and unlabeled\n",
    "    logger.info(\"\bNumber of training samples \" + str(len(dataset)))\n",
    "    logger.info(\"Supervision percentage \" + str(cfg.DATALOADER.SUP_PERCENT))\n",
    "\n",
    "    return build_batch_data_loader(\n",
    "        dataset,\n",
    "        sampler,\n",
    "        cfg.SOLVER.IMS_PER_BATCH,\n",
    "        aspect_ratio_grouping=cfg.DATALOADER.ASPECT_RATIO_GROUPING,\n",
    "        num_workers=cfg.DATALOADER.NUM_WORKERS,\n",
    "    )\n",
    "\n",
    "\n",
    "# uesed by evaluation\n",
    "def build_detection_test_loader(cfg, dataset_name, mapper=None):\n",
    "    dataset_dicts = get_detection_dataset_dicts(\n",
    "        [dataset_name],\n",
    "        filter_empty=False,\n",
    "        proposal_files=[\n",
    "            cfg.DATASETS.PROPOSAL_FILES_TEST[\n",
    "                list(cfg.DATASETS.TEST).index(dataset_name)\n",
    "            ]\n",
    "        ]\n",
    "        if cfg.MODEL.LOAD_PROPOSALS\n",
    "        else None,\n",
    "    )\n",
    "    dataset = DatasetFromList(dataset_dicts)\n",
    "    if mapper is None:\n",
    "        mapper = DatasetMapper(cfg, False)\n",
    "    dataset = MapDataset(dataset, mapper)\n",
    "\n",
    "    sampler = InferenceSampler(len(dataset))\n",
    "    batch_sampler = torch.utils.data.sampler.BatchSampler(sampler, 1, drop_last=False)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        num_workers=cfg.DATALOADER.NUM_WORKERS,\n",
    "        batch_sampler=batch_sampler,\n",
    "        collate_fn=trivial_batch_collator,\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "# uesed by unbiased teacher trainer\n",
    "def build_detection_semisup_train_loader_two_crops(cfg, mapper=None):\n",
    "    if cfg.DATASETS.CROSS_DATASET:  # cross-dataset (e.g., coco-additional)\n",
    "        label_dicts = get_detection_dataset_dicts(\n",
    "            cfg.DATASETS.TRAIN_LABEL,\n",
    "            filter_empty=cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS,\n",
    "            min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE\n",
    "            if cfg.MODEL.KEYPOINT_ON\n",
    "            else 0,\n",
    "            proposal_files=cfg.DATASETS.PROPOSAL_FILES_TRAIN\n",
    "            if cfg.MODEL.LOAD_PROPOSALS\n",
    "            else None,\n",
    "        )\n",
    "        unlabel_dicts = get_detection_dataset_dicts(\n",
    "            cfg.DATASETS.TRAIN_UNLABEL,\n",
    "            filter_empty=False,\n",
    "            min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE\n",
    "            if cfg.MODEL.KEYPOINT_ON\n",
    "            else 0,\n",
    "            proposal_files=cfg.DATASETS.PROPOSAL_FILES_TRAIN\n",
    "            if cfg.MODEL.LOAD_PROPOSALS\n",
    "            else None,\n",
    "        )\n",
    "    else:  # different degree of supervision (e.g., COCO-supervision)\n",
    "        dataset_dicts = get_detection_dataset_dicts(\n",
    "            cfg.DATASETS.TRAIN,\n",
    "            filter_empty=cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS,\n",
    "            min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE\n",
    "            if cfg.MODEL.KEYPOINT_ON\n",
    "            else 0,\n",
    "            proposal_files=cfg.DATASETS.PROPOSAL_FILES_TRAIN\n",
    "            if cfg.MODEL.LOAD_PROPOSALS\n",
    "            else None,\n",
    "        )\n",
    "\n",
    "        # Divide into labeled and unlabeled sets according to supervision percentage\n",
    "        label_dicts, unlabel_dicts = divide_label_unlabel(\n",
    "            dataset_dicts,\n",
    "            cfg.DATALOADER.SUP_PERCENT,\n",
    "            cfg.DATALOADER.RANDOM_DATA_SEED,\n",
    "            cfg.DATALOADER.RANDOM_DATA_SEED_PATH,\n",
    "        )\n",
    "\n",
    "    label_dataset = DatasetFromList(label_dicts, copy=False)\n",
    "    # exclude the labeled set from unlabeled dataset\n",
    "    unlabel_dataset = DatasetFromList(unlabel_dicts, copy=False)\n",
    "    # include the labeled set in unlabel dataset\n",
    "    # unlabel_dataset = DatasetFromList(dataset_dicts, copy=False)\n",
    "\n",
    "    if mapper is None:\n",
    "        mapper = DatasetMapper(cfg, True)\n",
    "    label_dataset = MapDataset(label_dataset, mapper)\n",
    "    unlabel_dataset = MapDataset(unlabel_dataset, mapper)\n",
    "\n",
    "    sampler_name = cfg.DATALOADER.SAMPLER_TRAIN\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"Using training sampler {}\".format(sampler_name))\n",
    "    if sampler_name == \"TrainingSampler\":\n",
    "        label_sampler = TrainingSampler(len(label_dataset))\n",
    "        unlabel_sampler = TrainingSampler(len(unlabel_dataset))\n",
    "    elif sampler_name == \"RepeatFactorTrainingSampler\":\n",
    "        raise NotImplementedError(\"{} not yet supported.\".format(sampler_name))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown training sampler: {}\".format(sampler_name))\n",
    "    return build_semisup_batch_data_loader_two_crop(\n",
    "        (label_dataset, unlabel_dataset),\n",
    "        (label_sampler, unlabel_sampler),\n",
    "        cfg.SOLVER.IMG_PER_BATCH_LABEL,\n",
    "        cfg.SOLVER.IMG_PER_BATCH_UNLABEL,\n",
    "        aspect_ratio_grouping=cfg.DATALOADER.ASPECT_RATIO_GROUPING,\n",
    "        num_workers=cfg.DATALOADER.NUM_WORKERS,\n",
    "    )\n",
    "\n",
    "\n",
    "# batch data loader\n",
    "def build_semisup_batch_data_loader_two_crop(\n",
    "    dataset,\n",
    "    sampler,\n",
    "    total_batch_size_label,\n",
    "    total_batch_size_unlabel,\n",
    "    *,\n",
    "    aspect_ratio_grouping=False,\n",
    "    num_workers=0,\n",
    "):\n",
    "    world_size = get_world_size()\n",
    "    assert (\n",
    "        total_batch_size_label > 0 and total_batch_size_label % world_size == 0\n",
    "    ), \"Total label batch size ({}) must be divisible by the number of gpus ({}).\".format(\n",
    "        total_batch_size_label, world_size\n",
    "    )\n",
    "\n",
    "    assert (\n",
    "        total_batch_size_unlabel > 0 and total_batch_size_unlabel % world_size == 0\n",
    "    ), \"Total unlabel batch size ({}) must be divisible by the number of gpus ({}).\".format(\n",
    "        total_batch_size_label, world_size\n",
    "    )\n",
    "\n",
    "    batch_size_label = total_batch_size_label // world_size\n",
    "    batch_size_unlabel = total_batch_size_unlabel // world_size\n",
    "\n",
    "    label_dataset, unlabel_dataset = dataset\n",
    "    label_sampler, unlabel_sampler = sampler\n",
    "\n",
    "    if aspect_ratio_grouping:\n",
    "        label_data_loader = torch.utils.data.DataLoader(\n",
    "            label_dataset,\n",
    "            sampler=label_sampler,\n",
    "            num_workers=num_workers,\n",
    "            batch_sampler=None,\n",
    "            collate_fn=operator.itemgetter(\n",
    "                0\n",
    "            ),  # don't batch, but yield individual elements\n",
    "            worker_init_fn=worker_init_reset_seed,\n",
    "        )  # yield individual mapped dict\n",
    "        unlabel_data_loader = torch.utils.data.DataLoader(\n",
    "            unlabel_dataset,\n",
    "            sampler=unlabel_sampler,\n",
    "            num_workers=num_workers,\n",
    "            batch_sampler=None,\n",
    "            collate_fn=operator.itemgetter(\n",
    "                0\n",
    "            ),  # don't batch, but yield individual elements\n",
    "            worker_init_fn=worker_init_reset_seed,\n",
    "        )  # yield individual mapped dict\n",
    "        return AspectRatioGroupedSemiSupDatasetTwoCrop(\n",
    "            (label_data_loader, unlabel_data_loader),\n",
    "            (batch_size_label, batch_size_unlabel),\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError(\"ASPECT_RATIO_GROUPING = False is not supported yet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import logging\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "# from ubteacher.data.transforms.augmentation_impl import GaussianBlur\n",
    "\n",
    "\n",
    "def build_strong_augmentation(cfg, is_train):\n",
    "    \"\"\"\n",
    "    Create a list of :class:`Augmentation` from config.\n",
    "    Now it includes resizing and flipping.\n",
    "\n",
    "    Returns:\n",
    "        list[Augmentation]\n",
    "    \"\"\"\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "    augmentation = []\n",
    "    if is_train:\n",
    "        # This is simialr to SimCLR https://arxiv.org/abs/2002.05709\n",
    "        augmentation.append(\n",
    "            transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8)\n",
    "        )\n",
    "        augmentation.append(transforms.RandomGrayscale(p=0.2))\n",
    "        augmentation.append(transforms.RandomApply([GaussianBlur([0.1, 2.0])], p=0.5))\n",
    "\n",
    "        # randomcrop\n",
    "        randcrop_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.RandomErasing(\n",
    "                    p=0.7, scale=(0.05, 0.2), ratio=(0.3, 3.3), value=\"random\"\n",
    "                ),\n",
    "                transforms.RandomErasing(\n",
    "                    p=0.5, scale=(0.02, 0.2), ratio=(0.1, 6), value=\"random\"\n",
    "                ),\n",
    "                transforms.RandomErasing(\n",
    "                    p=0.3, scale=(0.02, 0.2), ratio=(0.05, 8), value=\"random\"\n",
    "                ),\n",
    "                transforms.ToPILImage(),\n",
    "            ]\n",
    "        )\n",
    "        augmentation.append(randcrop_transform)\n",
    "\n",
    "        logger.info(\"Augmentations used in training: \" + str(augmentation))\n",
    "    return transforms.Compose(augmentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "import detectron2.data.detection_utils as utils\n",
    "import detectron2.data.transforms as T\n",
    "import numpy as np\n",
    "import torch\n",
    "from detectron2.data.dataset_mapper import DatasetMapper\n",
    "from PIL import Image\n",
    "# from ubteacher.data.detection_utils import build_strong_augmentation\n",
    "\n",
    "\n",
    "class DatasetMapperTwoCropSeparate(DatasetMapper):\n",
    "    \"\"\"\n",
    "    This customized mapper produces two augmented images from a single image\n",
    "    instance. This mapper makes sure that the two augmented images have the same\n",
    "    cropping and thus the same size.\n",
    "\n",
    "    A callable which takes a dataset dict in Detectron2 Dataset format,\n",
    "    and map it into a format used by the model.\n",
    "\n",
    "    This is the default callable to be used to map your dataset dict into training data.\n",
    "    You may need to follow it to implement your own one for customized logic,\n",
    "    such as a different way to read or transform images.\n",
    "    See :doc:`/tutorials/data_loading` for details.\n",
    "\n",
    "    The callable currently does the following:\n",
    "\n",
    "    1. Read the image from \"file_name\"\n",
    "    2. Applies cropping/geometric transforms to the image and annotations\n",
    "    3. Prepare data and annotations to Tensor and :class:`Instances`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, is_train=True):\n",
    "        self.augmentation = utils.build_augmentation(cfg, is_train)\n",
    "        # include crop into self.augmentation\n",
    "        if cfg.INPUT.CROP.ENABLED and is_train:\n",
    "            self.augmentation.insert(\n",
    "                0, T.RandomCrop(cfg.INPUT.CROP.TYPE, cfg.INPUT.CROP.SIZE)\n",
    "            )\n",
    "            logging.getLogger(__name__).info(\n",
    "                \"Cropping used in training: \" + str(self.augmentation[0])\n",
    "            )\n",
    "            self.compute_tight_boxes = True\n",
    "        else:\n",
    "            self.compute_tight_boxes = False\n",
    "        self.strong_augmentation = build_strong_augmentation(cfg, is_train)\n",
    "\n",
    "        # fmt: off\n",
    "        self.img_format = cfg.INPUT.FORMAT\n",
    "        self.mask_on = cfg.MODEL.MASK_ON\n",
    "        self.mask_format = cfg.INPUT.MASK_FORMAT\n",
    "        self.keypoint_on = cfg.MODEL.KEYPOINT_ON\n",
    "        self.load_proposals = cfg.MODEL.LOAD_PROPOSALS\n",
    "        # fmt: on\n",
    "        if self.keypoint_on and is_train:\n",
    "            self.keypoint_hflip_indices = utils.create_keypoint_hflip_indices(\n",
    "                cfg.DATASETS.TRAIN\n",
    "            )\n",
    "        else:\n",
    "            self.keypoint_hflip_indices = None\n",
    "\n",
    "        if self.load_proposals:\n",
    "            self.proposal_min_box_size = cfg.MODEL.PROPOSAL_GENERATOR.MIN_SIZE\n",
    "            self.proposal_topk = (\n",
    "                cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TRAIN\n",
    "                if is_train\n",
    "                else cfg.DATASETS.PRECOMPUTED_PROPOSAL_TOPK_TEST\n",
    "            )\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __call__(self, dataset_dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.\n",
    "\n",
    "        Returns:\n",
    "            dict: a format that builtin models in detectron2 accept\n",
    "        \"\"\"\n",
    "        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\n",
    "        image = utils.read_image(dataset_dict[\"file_name\"], format=self.img_format)\n",
    "        utils.check_image_size(dataset_dict, image)\n",
    "\n",
    "        if \"sem_seg_file_name\" in dataset_dict:\n",
    "            sem_seg_gt = utils.read_image(\n",
    "                dataset_dict.pop(\"sem_seg_file_name\"), \"L\"\n",
    "            ).squeeze(2)\n",
    "        else:\n",
    "            sem_seg_gt = None\n",
    "\n",
    "        aug_input = T.StandardAugInput(image, sem_seg=sem_seg_gt)\n",
    "        transforms = aug_input.apply_augmentations(self.augmentation)\n",
    "        image_weak_aug, sem_seg_gt = aug_input.image, aug_input.sem_seg\n",
    "        image_shape = image_weak_aug.shape[:2]  # h, w\n",
    "\n",
    "        if sem_seg_gt is not None:\n",
    "            dataset_dict[\"sem_seg\"] = torch.as_tensor(sem_seg_gt.astype(\"long\"))\n",
    "\n",
    "        if self.load_proposals:\n",
    "            utils.transform_proposals(\n",
    "                dataset_dict,\n",
    "                image_shape,\n",
    "                transforms,\n",
    "                proposal_topk=self.proposal_topk,\n",
    "                min_box_size=self.proposal_min_box_size,\n",
    "            )\n",
    "\n",
    "        if not self.is_train:\n",
    "            dataset_dict.pop(\"annotations\", None)\n",
    "            dataset_dict.pop(\"sem_seg_file_name\", None)\n",
    "            return dataset_dict\n",
    "\n",
    "        if \"annotations\" in dataset_dict:\n",
    "            for anno in dataset_dict[\"annotations\"]:\n",
    "                if not self.mask_on:\n",
    "                    anno.pop(\"segmentation\", None)\n",
    "                if not self.keypoint_on:\n",
    "                    anno.pop(\"keypoints\", None)\n",
    "\n",
    "            annos = [\n",
    "                utils.transform_instance_annotations(\n",
    "                    obj,\n",
    "                    transforms,\n",
    "                    image_shape,\n",
    "                    keypoint_hflip_indices=self.keypoint_hflip_indices,\n",
    "                )\n",
    "                for obj in dataset_dict.pop(\"annotations\")\n",
    "                if obj.get(\"iscrowd\", 0) == 0\n",
    "            ]\n",
    "            instances = utils.annotations_to_instances(\n",
    "                annos, image_shape, mask_format=self.mask_format\n",
    "            )\n",
    "\n",
    "            if self.compute_tight_boxes and instances.has(\"gt_masks\"):\n",
    "                instances.gt_boxes = instances.gt_masks.get_bounding_boxes()\n",
    "\n",
    "            bboxes_d2_format = utils.filter_empty_instances(instances)\n",
    "            dataset_dict[\"instances\"] = bboxes_d2_format\n",
    "\n",
    "        # apply strong augmentation\n",
    "        # We use torchvision augmentation, which is not compatiable with\n",
    "        # detectron2, which use numpy format for images. Thus, we need to\n",
    "        # convert to PIL format first.\n",
    "        image_pil = Image.fromarray(image_weak_aug.astype(\"uint8\"), \"RGB\")\n",
    "        image_strong_aug = np.array(self.strong_augmentation(image_pil))\n",
    "        dataset_dict[\"image\"] = torch.as_tensor(\n",
    "            np.ascontiguousarray(image_strong_aug.transpose(2, 0, 1))\n",
    "        )\n",
    "\n",
    "        dataset_dict_key = copy.deepcopy(dataset_dict)\n",
    "        dataset_dict_key[\"image\"] = torch.as_tensor(\n",
    "            np.ascontiguousarray(image_weak_aug.transpose(2, 0, 1))\n",
    "        )\n",
    "        assert dataset_dict[\"image\"].size(1) == dataset_dict_key[\"image\"].size(1)\n",
    "        assert dataset_dict[\"image\"].size(2) == dataset_dict_key[\"image\"].size(2)\n",
    "        return (dataset_dict, dataset_dict_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _C: The specified procedure could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoco\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_coco_json\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DatasetEvaluator\n\u001b[1;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfast_eval_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COCOeval_opt\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstructures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Boxes, BoxMode, pairwise_iou\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfile_io\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PathManager\n",
      "File \u001b[1;32md:\\deep_learning_with_cuda\\detectron2\\detectron2\\evaluation\\fast_eval_api.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycocotools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcocoeval\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COCOeval\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdetectron2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _C\n\u001b[0;32m     10\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCOCOeval_opt\u001b[39;00m(COCOeval):\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _C: The specified procedure could not be found."
     ]
    }
   ],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "import contextlib\n",
    "import copy\n",
    "import io\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "\n",
    "import detectron2.utils.comm as comm\n",
    "import numpy as np\n",
    "import pycocotools.mask as mask_util\n",
    "import torch\n",
    "from detectron2.config import CfgNode\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.data.datasets.coco import convert_to_coco_json\n",
    "from detectron2.evaluation.evaluator import DatasetEvaluator\n",
    "from detectron2.evaluation.fast_eval_api import COCOeval_opt\n",
    "from detectron2.structures import Boxes, BoxMode, pairwise_iou\n",
    "from detectron2.utils.file_io import PathManager\n",
    "from detectron2.utils.logger import create_small_table\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "class COCOEvaluator(DatasetEvaluator):\n",
    "    \"\"\"\n",
    "    Evaluate AR for object proposals, AP for instance detection/segmentation, AP\n",
    "    for keypoint detection outputs using COCO's metrics.\n",
    "    See http://cocodataset.org/#detection-eval and\n",
    "    http://cocodataset.org/#keypoints-eval to understand its metrics.\n",
    "    The metrics range from 0 to 100 (instead of 0 to 1), where a -1 or NaN means\n",
    "    the metric cannot be computed (e.g. due to no predictions made).\n",
    "\n",
    "    In addition to COCO, this evaluator is able to support any bounding box detection,\n",
    "    instance segmentation, or keypoint detection dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name,\n",
    "        tasks=None,\n",
    "        distributed=True,\n",
    "        output_dir=None,\n",
    "        *,\n",
    "        use_fast_impl=True,\n",
    "        kpt_oks_sigmas=(),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataset_name (str): name of the dataset to be evaluated.\n",
    "                It must have either the following corresponding metadata:\n",
    "\n",
    "                    \"json_file\": the path to the COCO format annotation\n",
    "\n",
    "                Or it must be in detectron2's standard dataset format\n",
    "                so it can be converted to COCO format automatically.\n",
    "            tasks (tuple[str]): tasks that can be evaluated under the given\n",
    "                configuration. A task is one of \"bbox\", \"segm\", \"keypoints\".\n",
    "                By default, will infer this automatically from predictions.\n",
    "            distributed (True): if True, will collect results from all ranks and run evaluation\n",
    "                in the main process.\n",
    "                Otherwise, will only evaluate the results in the current process.\n",
    "            output_dir (str): optional, an output directory to dump all\n",
    "                results predicted on the dataset. The dump contains two files:\n",
    "\n",
    "                1. \"instances_predictions.pth\" a file that can be loaded with `torch.load` and\n",
    "                   contains all the results in the format they are produced by the model.\n",
    "                2. \"coco_instances_results.json\" a json file in COCO's result format.\n",
    "            use_fast_impl (bool): use a fast but **unofficial** implementation to compute AP.\n",
    "                Although the results should be very close to the official implementation in COCO\n",
    "                API, it is still recommended to compute results with the official API for use in\n",
    "                papers. The faster implementation also uses more RAM.\n",
    "            kpt_oks_sigmas (list[float]): The sigmas used to calculate keypoint OKS.\n",
    "                See http://cocodataset.org/#keypoints-eval\n",
    "                When empty, it will use the defaults in COCO.\n",
    "                Otherwise it should be the same length as ROI_KEYPOINT_HEAD.NUM_KEYPOINTS.\n",
    "        \"\"\"\n",
    "        self._logger = logging.getLogger(__name__)\n",
    "        self._distributed = distributed\n",
    "        self._output_dir = output_dir\n",
    "        self._use_fast_impl = use_fast_impl\n",
    "\n",
    "        if tasks is not None and isinstance(tasks, CfgNode):\n",
    "            kpt_oks_sigmas = (\n",
    "                tasks.TEST.KEYPOINT_OKS_SIGMAS if not kpt_oks_sigmas else kpt_oks_sigmas\n",
    "            )\n",
    "            self._logger.warn(\n",
    "                \"COCO Evaluator instantiated using config, this is deprecated behavior.\"\n",
    "                \" Please pass in explicit arguments instead.\"\n",
    "            )\n",
    "            self._tasks = None  # Infering it from predictions should be better\n",
    "        else:\n",
    "            self._tasks = tasks\n",
    "\n",
    "        self._cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "        self._metadata = MetadataCatalog.get(dataset_name)\n",
    "        if not hasattr(self._metadata, \"json_file\"):\n",
    "            self._logger.info(\n",
    "                f\"'{dataset_name}' is not registered by `register_coco_instances`.\"\n",
    "                \" Therefore trying to convert it to COCO format ...\"\n",
    "            )\n",
    "\n",
    "            cache_path = os.path.join(output_dir, f\"{dataset_name}_coco_format.json\")\n",
    "            self._metadata.json_file = cache_path\n",
    "            convert_to_coco_json(dataset_name, cache_path)\n",
    "\n",
    "        json_file = PathManager.get_local_path(self._metadata.json_file)\n",
    "        with contextlib.redirect_stdout(io.StringIO()):\n",
    "            self._coco_api = COCO(json_file)\n",
    "\n",
    "        # Test set json files do not contain annotations (evaluation must be\n",
    "        # performed using the COCO evaluation server).\n",
    "        self._do_evaluation = \"annotations\" in self._coco_api.dataset\n",
    "        if self._do_evaluation:\n",
    "            self._kpt_oks_sigmas = kpt_oks_sigmas\n",
    "\n",
    "    def reset(self):\n",
    "        self._predictions = []\n",
    "\n",
    "    def process(self, inputs, outputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs: the inputs to a COCO model (e.g., GeneralizedRCNN).\n",
    "                It is a list of dict. Each dict corresponds to an image and\n",
    "                contains keys like \"height\", \"width\", \"file_name\", \"image_id\".\n",
    "            outputs: the outputs of a COCO model. It is a list of dicts with key\n",
    "                \"instances\" that contains :class:`Instances`.\n",
    "        \"\"\"\n",
    "        for input, output in zip(inputs, outputs):\n",
    "            prediction = {\"image_id\": input[\"image_id\"]}\n",
    "\n",
    "            if \"instances\" in output:\n",
    "                instances = output[\"instances\"].to(self._cpu_device)\n",
    "                prediction[\"instances\"] = instances_to_coco_json(\n",
    "                    instances, input[\"image_id\"]\n",
    "                )\n",
    "            if \"proposals\" in output:\n",
    "                prediction[\"proposals\"] = output[\"proposals\"].to(self._cpu_device)\n",
    "            if len(prediction) > 1:\n",
    "                self._predictions.append(prediction)\n",
    "\n",
    "    def evaluate(self, img_ids=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_ids: a list of image IDs to evaluate on. Default to None for the whole dataset\n",
    "        \"\"\"\n",
    "        if self._distributed:\n",
    "            comm.synchronize()\n",
    "            predictions = comm.gather(self._predictions, dst=0)\n",
    "            predictions = list(itertools.chain(*predictions))\n",
    "\n",
    "            if not comm.is_main_process():\n",
    "                return {}\n",
    "        else:\n",
    "            predictions = self._predictions\n",
    "\n",
    "        if len(predictions) == 0:\n",
    "            self._logger.warning(\"[COCOEvaluator] Did not receive valid predictions.\")\n",
    "            return {}\n",
    "\n",
    "        if self._output_dir:\n",
    "            PathManager.mkdirs(self._output_dir)\n",
    "            file_path = os.path.join(self._output_dir, \"instances_predictions.pth\")\n",
    "            with PathManager.open(file_path, \"wb\") as f:\n",
    "                torch.save(predictions, f)\n",
    "\n",
    "        self._results = OrderedDict()\n",
    "        if \"proposals\" in predictions[0]:\n",
    "            self._eval_box_proposals(predictions)\n",
    "        if \"instances\" in predictions[0]:\n",
    "            self._eval_predictions(predictions, img_ids=img_ids)\n",
    "        # Copy so the caller can do whatever with results\n",
    "        return copy.deepcopy(self._results)\n",
    "\n",
    "    def _tasks_from_predictions(self, predictions):\n",
    "        \"\"\"\n",
    "        Get COCO API \"tasks\" (i.e. iou_type) from COCO-format predictions.\n",
    "        \"\"\"\n",
    "        tasks = {\"bbox\"}\n",
    "        for pred in predictions:\n",
    "            if \"segmentation\" in pred:\n",
    "                tasks.add(\"segm\")\n",
    "            if \"keypoints\" in pred:\n",
    "                tasks.add(\"keypoints\")\n",
    "        return sorted(tasks)\n",
    "\n",
    "    def _eval_predictions(self, predictions, img_ids=None):\n",
    "        \"\"\"\n",
    "        Evaluate predictions. Fill self._results with the metrics of the tasks.\n",
    "        \"\"\"\n",
    "        self._logger.info(\"Preparing results for COCO format ...\")\n",
    "        coco_results = list(itertools.chain(*[x[\"instances\"] for x in predictions]))\n",
    "        tasks = self._tasks or self._tasks_from_predictions(coco_results)\n",
    "\n",
    "        # unmap the category ids for COCO\n",
    "        if hasattr(self._metadata, \"thing_dataset_id_to_contiguous_id\"):\n",
    "            dataset_id_to_contiguous_id = (\n",
    "                self._metadata.thing_dataset_id_to_contiguous_id\n",
    "            )\n",
    "            all_contiguous_ids = list(dataset_id_to_contiguous_id.values())\n",
    "            num_classes = len(all_contiguous_ids)\n",
    "            assert (\n",
    "                min(all_contiguous_ids) == 0\n",
    "                and max(all_contiguous_ids) == num_classes - 1\n",
    "            )\n",
    "\n",
    "            reverse_id_mapping = {v: k for k, v in dataset_id_to_contiguous_id.items()}\n",
    "            for result in coco_results:\n",
    "                category_id = result[\"category_id\"]\n",
    "                assert category_id < num_classes, (\n",
    "                    f\"A prediction has class={category_id}, \"\n",
    "                    f\"but the dataset only has {num_classes} classes and \"\n",
    "                    f\"predicted class id should be in [0, {num_classes - 1}].\"\n",
    "                )\n",
    "                result[\"category_id\"] = reverse_id_mapping[category_id]\n",
    "\n",
    "        if self._output_dir:\n",
    "            file_path = os.path.join(self._output_dir, \"coco_instances_results.json\")\n",
    "            self._logger.info(\"Saving results to {}\".format(file_path))\n",
    "            with PathManager.open(file_path, \"w\") as f:\n",
    "                f.write(json.dumps(coco_results))\n",
    "                f.flush()\n",
    "\n",
    "        if not self._do_evaluation:\n",
    "            self._logger.info(\"Annotations are not available for evaluation.\")\n",
    "            return\n",
    "\n",
    "        self._logger.info(\n",
    "            \"Evaluating predictions with {} COCO API...\".format(\n",
    "                \"unofficial\" if self._use_fast_impl else \"official\"\n",
    "            )\n",
    "        )\n",
    "        for task in sorted(tasks):\n",
    "            assert task in {\"bbox\", \"segm\", \"keypoints\"}, f\"Got unknown task: {task}!\"\n",
    "            coco_eval = (\n",
    "                _evaluate_predictions_on_coco(\n",
    "                    self._coco_api,\n",
    "                    coco_results,\n",
    "                    task,\n",
    "                    kpt_oks_sigmas=self._kpt_oks_sigmas,\n",
    "                    use_fast_impl=self._use_fast_impl,\n",
    "                    img_ids=img_ids,\n",
    "                )\n",
    "                if len(coco_results) > 0\n",
    "                else None  # cocoapi does not handle empty results very well\n",
    "            )\n",
    "\n",
    "            res = self._derive_coco_results(\n",
    "                coco_eval, task, class_names=self._metadata.get(\"thing_classes\")\n",
    "            )\n",
    "            self._results[task] = res\n",
    "\n",
    "    def _eval_box_proposals(self, predictions):\n",
    "        \"\"\"\n",
    "        Evaluate the box proposals in predictions.\n",
    "        Fill self._results with the metrics for \"box_proposals\" task.\n",
    "        \"\"\"\n",
    "        if self._output_dir:\n",
    "            # Saving generated box proposals to file.\n",
    "            # Predicted box_proposals are in XYXY_ABS mode.\n",
    "            bbox_mode = BoxMode.XYXY_ABS.value\n",
    "            ids, boxes, objectness_logits = [], [], []\n",
    "            for prediction in predictions:\n",
    "                ids.append(prediction[\"image_id\"])\n",
    "                boxes.append(prediction[\"proposals\"].proposal_boxes.tensor.numpy())\n",
    "                objectness_logits.append(\n",
    "                    prediction[\"proposals\"].objectness_logits.numpy()\n",
    "                )\n",
    "\n",
    "            proposal_data = {\n",
    "                \"boxes\": boxes,\n",
    "                \"objectness_logits\": objectness_logits,\n",
    "                \"ids\": ids,\n",
    "                \"bbox_mode\": bbox_mode,\n",
    "            }\n",
    "            with PathManager.open(\n",
    "                os.path.join(self._output_dir, \"box_proposals.pkl\"), \"wb\"\n",
    "            ) as f:\n",
    "                pickle.dump(proposal_data, f)\n",
    "\n",
    "        if not self._do_evaluation:\n",
    "            self._logger.info(\"Annotations are not available for evaluation.\")\n",
    "            return\n",
    "\n",
    "        self._logger.info(\"Evaluating bbox proposals ...\")\n",
    "        res = {}\n",
    "        areas = {\"all\": \"\", \"small\": \"s\", \"medium\": \"m\", \"large\": \"l\"}\n",
    "        for limit in [100, 1000]:\n",
    "            for area, suffix in areas.items():\n",
    "                stats = _evaluate_box_proposals(\n",
    "                    predictions, self._coco_api, area=area, limit=limit\n",
    "                )\n",
    "                key = \"AR{}@{:d}\".format(suffix, limit)\n",
    "                res[key] = float(stats[\"ar\"].item() * 100)\n",
    "        self._logger.info(\"Proposal metrics: \\n\" + create_small_table(res))\n",
    "        self._results[\"box_proposals\"] = res\n",
    "\n",
    "    def _derive_coco_results(self, coco_eval, iou_type, class_names=None):\n",
    "        \"\"\"\n",
    "        Derive the desired score numbers from summarized COCOeval.\n",
    "\n",
    "        Args:\n",
    "            coco_eval (None or COCOEval): None represents no predictions from model.\n",
    "            iou_type (str):\n",
    "            class_names (None or list[str]): if provided, will use it to predict\n",
    "                per-category AP.\n",
    "\n",
    "        Returns:\n",
    "            a dict of {metric name: score}\n",
    "        \"\"\"\n",
    "\n",
    "        metrics = {\n",
    "            \"bbox\": [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"],\n",
    "            \"segm\": [\"AP\", \"AP50\", \"AP75\", \"APs\", \"APm\", \"APl\"],\n",
    "            \"keypoints\": [\"AP\", \"AP50\", \"AP75\", \"APm\", \"APl\"],\n",
    "        }[iou_type]\n",
    "\n",
    "        if coco_eval is None:\n",
    "            self._logger.warn(\"No predictions from the model!\")\n",
    "            return {metric: float(\"nan\") for metric in metrics}\n",
    "\n",
    "        # the standard metrics\n",
    "        results = {\n",
    "            metric: float(\n",
    "                coco_eval.stats[idx] * 100 if coco_eval.stats[idx] >= 0 else \"nan\"\n",
    "            )\n",
    "            for idx, metric in enumerate(metrics)\n",
    "        }\n",
    "        self._logger.info(\n",
    "            \"Evaluation results for {}: \\n\".format(iou_type)\n",
    "            + create_small_table(results)\n",
    "        )\n",
    "        if not np.isfinite(sum(results.values())):\n",
    "            self._logger.info(\"Some metrics cannot be computed and is shown as NaN.\")\n",
    "\n",
    "        if class_names is None or len(class_names) <= 1:\n",
    "            return results\n",
    "        # Compute per-category AP\n",
    "        # from https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L222-L252 # noqa\n",
    "        precisions = coco_eval.eval[\"precision\"]\n",
    "        # precision has dims (iou, recall, cls, area range, max dets)\n",
    "        assert len(class_names) == precisions.shape[2]\n",
    "\n",
    "        results_per_category = []\n",
    "        for idx, name in enumerate(class_names):\n",
    "            # area range index 0: all area ranges\n",
    "            # max dets index -1: typically 100 per image\n",
    "            precision = precisions[:, :, idx, 0, -1]\n",
    "            precision = precision[precision > -1]\n",
    "            ap = np.mean(precision) if precision.size else float(\"nan\")\n",
    "            results_per_category.append((\"{}\".format(name), float(ap * 100)))\n",
    "\n",
    "        # tabulate it\n",
    "        N_COLS = min(6, len(results_per_category) * 2)\n",
    "        results_flatten = list(itertools.chain(*results_per_category))\n",
    "        results_2d = itertools.zip_longest(\n",
    "            *[results_flatten[i::N_COLS] for i in range(N_COLS)]\n",
    "        )\n",
    "        table = tabulate(\n",
    "            results_2d,\n",
    "            tablefmt=\"pipe\",\n",
    "            floatfmt=\".3f\",\n",
    "            headers=[\"category\", \"AP\"] * (N_COLS // 2),\n",
    "            numalign=\"left\",\n",
    "        )\n",
    "        self._logger.info(\"Per-category {} AP: \\n\".format(iou_type) + table)\n",
    "\n",
    "        results.update({\"AP-\" + name: ap for name, ap in results_per_category})\n",
    "        return results\n",
    "\n",
    "\n",
    "def instances_to_coco_json(instances, img_id):\n",
    "    \"\"\"\n",
    "    Dump an \"Instances\" object to a COCO-format json that's used for evaluation.\n",
    "\n",
    "    Args:\n",
    "        instances (Instances):\n",
    "        img_id (int): the image id\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: list of json annotations in COCO format.\n",
    "    \"\"\"\n",
    "    num_instance = len(instances)\n",
    "    if num_instance == 0:\n",
    "        return []\n",
    "\n",
    "    boxes = instances.pred_boxes.tensor.numpy()\n",
    "    boxes = BoxMode.convert(boxes, BoxMode.XYXY_ABS, BoxMode.XYWH_ABS)\n",
    "    boxes = boxes.tolist()\n",
    "    scores = instances.scores.tolist()\n",
    "    classes = instances.pred_classes.tolist()\n",
    "\n",
    "    has_mask = instances.has(\"pred_masks\")\n",
    "    if has_mask:\n",
    "        # use RLE to encode the masks, because they are too large and takes memory\n",
    "        # since this evaluator stores outputs of the entire dataset\n",
    "        rles = [\n",
    "            mask_util.encode(np.array(mask[:, :, None], order=\"F\", dtype=\"uint8\"))[0]\n",
    "            for mask in instances.pred_masks\n",
    "        ]\n",
    "        for rle in rles:\n",
    "            # \"counts\" is an array encoded by mask_util as a byte-stream. Python3's\n",
    "            # json writer which always produces strings cannot serialize a bytestream\n",
    "            # unless you decode it. Thankfully, utf-8 works out (which is also what\n",
    "            # the pycocotools/_mask.pyx does).\n",
    "            rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
    "\n",
    "    has_keypoints = instances.has(\"pred_keypoints\")\n",
    "    if has_keypoints:\n",
    "        keypoints = instances.pred_keypoints\n",
    "\n",
    "    results = []\n",
    "    for k in range(num_instance):\n",
    "        result = {\n",
    "            \"image_id\": img_id,\n",
    "            \"category_id\": classes[k],\n",
    "            \"bbox\": boxes[k],\n",
    "            \"score\": scores[k],\n",
    "        }\n",
    "        if has_mask:\n",
    "            result[\"segmentation\"] = rles[k]\n",
    "        if has_keypoints:\n",
    "            # In COCO annotations,\n",
    "            # keypoints coordinates are pixel indices.\n",
    "            # However our predictions are floating point coordinates.\n",
    "            # Therefore we subtract 0.5 to be consistent with the annotation format.\n",
    "            # This is the inverse of data loading logic in `datasets/coco.py`.\n",
    "            keypoints[k][:, :2] -= 0.5\n",
    "            result[\"keypoints\"] = keypoints[k].flatten().tolist()\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "\n",
    "# inspired from Detectron:\n",
    "# https://github.com/facebookresearch/Detectron/blob/a6a835f5b8208c45d0dce217ce9bbda915f44df7/detectron/datasets/json_dataset_evaluator.py#L255 # noqa\n",
    "def _evaluate_box_proposals(\n",
    "    dataset_predictions, coco_api, thresholds=None, area=\"all\", limit=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate detection proposal recall metrics. This function is a much\n",
    "    faster alternative to the official COCO API recall evaluation code. However,\n",
    "    it produces slightly different results.\n",
    "    \"\"\"\n",
    "    # Record max overlap value for each gt box\n",
    "    # Return vector of overlap values\n",
    "    areas = {\n",
    "        \"all\": 0,\n",
    "        \"small\": 1,\n",
    "        \"medium\": 2,\n",
    "        \"large\": 3,\n",
    "        \"96-128\": 4,\n",
    "        \"128-256\": 5,\n",
    "        \"256-512\": 6,\n",
    "        \"512-inf\": 7,\n",
    "    }\n",
    "    area_ranges = [\n",
    "        [0**2, 1e5**2],  # all\n",
    "        [0**2, 32**2],  # small\n",
    "        [32**2, 96**2],  # medium\n",
    "        [96**2, 1e5**2],  # large\n",
    "        [96**2, 128**2],  # 96-128\n",
    "        [128**2, 256**2],  # 128-256\n",
    "        [256**2, 512**2],  # 256-512\n",
    "        [512**2, 1e5**2],\n",
    "    ]  # 512-inf\n",
    "    assert area in areas, \"Unknown area range: {}\".format(area)\n",
    "    area_range = area_ranges[areas[area]]\n",
    "    gt_overlaps = []\n",
    "    num_pos = 0\n",
    "\n",
    "    for prediction_dict in dataset_predictions:\n",
    "        predictions = prediction_dict[\"proposals\"]\n",
    "\n",
    "        # sort predictions in descending order\n",
    "        # TODO maybe remove this and make it explicit in the documentation\n",
    "        inds = predictions.objectness_logits.sort(descending=True)[1]\n",
    "        predictions = predictions[inds]\n",
    "\n",
    "        ann_ids = coco_api.getAnnIds(imgIds=prediction_dict[\"image_id\"])\n",
    "        anno = coco_api.loadAnns(ann_ids)\n",
    "        gt_boxes = [\n",
    "            BoxMode.convert(obj[\"bbox\"], BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
    "            for obj in anno\n",
    "            if obj[\"iscrowd\"] == 0\n",
    "        ]\n",
    "        gt_boxes = torch.as_tensor(gt_boxes).reshape(-1, 4)  # guard against no boxes\n",
    "        gt_boxes = Boxes(gt_boxes)\n",
    "        gt_areas = torch.as_tensor([obj[\"area\"] for obj in anno if obj[\"iscrowd\"] == 0])\n",
    "\n",
    "        if len(gt_boxes) == 0 or len(predictions) == 0:\n",
    "            continue\n",
    "\n",
    "        valid_gt_inds = (gt_areas >= area_range[0]) & (gt_areas <= area_range[1])\n",
    "        gt_boxes = gt_boxes[valid_gt_inds]\n",
    "\n",
    "        num_pos += len(gt_boxes)\n",
    "\n",
    "        if len(gt_boxes) == 0:\n",
    "            continue\n",
    "\n",
    "        if limit is not None and len(predictions) > limit:\n",
    "            predictions = predictions[:limit]\n",
    "\n",
    "        overlaps = pairwise_iou(predictions.proposal_boxes, gt_boxes)\n",
    "\n",
    "        _gt_overlaps = torch.zeros(len(gt_boxes))\n",
    "        for j in range(min(len(predictions), len(gt_boxes))):\n",
    "            # find which proposal box maximally covers each gt box\n",
    "            # and get the iou amount of coverage for each gt box\n",
    "            max_overlaps, argmax_overlaps = overlaps.max(dim=0)\n",
    "\n",
    "            # find which gt box is 'best' covered (i.e. 'best' = most iou)\n",
    "            gt_ovr, gt_ind = max_overlaps.max(dim=0)\n",
    "            assert gt_ovr >= 0\n",
    "            # find the proposal box that covers the best covered gt box\n",
    "            box_ind = argmax_overlaps[gt_ind]\n",
    "            # record the iou coverage of this gt box\n",
    "            _gt_overlaps[j] = overlaps[box_ind, gt_ind]\n",
    "            assert _gt_overlaps[j] == gt_ovr\n",
    "            # mark the proposal box and the gt box as used\n",
    "            overlaps[box_ind, :] = -1\n",
    "            overlaps[:, gt_ind] = -1\n",
    "\n",
    "        # append recorded iou coverage level\n",
    "        gt_overlaps.append(_gt_overlaps)\n",
    "    gt_overlaps = (\n",
    "        torch.cat(gt_overlaps, dim=0)\n",
    "        if len(gt_overlaps)\n",
    "        else torch.zeros(0, dtype=torch.float32)\n",
    "    )\n",
    "    gt_overlaps, _ = torch.sort(gt_overlaps)\n",
    "\n",
    "    if thresholds is None:\n",
    "        step = 0.05\n",
    "        thresholds = torch.arange(0.5, 0.95 + 1e-5, step, dtype=torch.float32)\n",
    "    recalls = torch.zeros_like(thresholds)\n",
    "    # compute recall for each iou threshold\n",
    "    for i, t in enumerate(thresholds):\n",
    "        recalls[i] = (gt_overlaps >= t).float().sum() / float(num_pos)\n",
    "    # ar = 2 * np.trapz(recalls, thresholds)\n",
    "    ar = recalls.mean()\n",
    "    return {\n",
    "        \"ar\": ar,\n",
    "        \"recalls\": recalls,\n",
    "        \"thresholds\": thresholds,\n",
    "        \"gt_overlaps\": gt_overlaps,\n",
    "        \"num_pos\": num_pos,\n",
    "    }\n",
    "\n",
    "\n",
    "def _evaluate_predictions_on_coco(\n",
    "    coco_gt,\n",
    "    coco_results,\n",
    "    iou_type,\n",
    "    kpt_oks_sigmas=None,\n",
    "    use_fast_impl=True,\n",
    "    img_ids=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate the coco results using COCOEval API.\n",
    "    \"\"\"\n",
    "    assert len(coco_results) > 0\n",
    "\n",
    "    if iou_type == \"segm\":\n",
    "        coco_results = copy.deepcopy(coco_results)\n",
    "        # When evaluating mask AP, if the results contain bbox, cocoapi will\n",
    "        # use the box area as the area of the instance, instead of the mask area.\n",
    "        # This leads to a different definition of small/medium/large.\n",
    "        # We remove the bbox field to let mask AP use mask area.\n",
    "        for c in coco_results:\n",
    "            c.pop(\"bbox\", None)\n",
    "\n",
    "    coco_dt = coco_gt.loadRes(coco_results)\n",
    "    coco_eval = (COCOeval_opt if use_fast_impl else COCOeval)(\n",
    "        coco_gt, coco_dt, iou_type\n",
    "    )\n",
    "    if img_ids is not None:\n",
    "        coco_eval.params.imgIds = img_ids\n",
    "\n",
    "    if iou_type == \"keypoints\":\n",
    "        # Use the COCO default keypoint OKS sigmas unless overrides are specified\n",
    "        if kpt_oks_sigmas:\n",
    "            assert hasattr(\n",
    "                coco_eval.params, \"kpt_oks_sigmas\"\n",
    "            ), \"pycocotools is too old!\"\n",
    "            coco_eval.params.kpt_oks_sigmas = np.array(kpt_oks_sigmas)\n",
    "        # COCOAPI requires every detection and every gt to have keypoints, so\n",
    "        # we just take the first entry from both\n",
    "        num_keypoints_dt = len(coco_results[0][\"keypoints\"]) // 3\n",
    "        num_keypoints_gt = len(next(iter(coco_gt.anns.values()))[\"keypoints\"]) // 3\n",
    "        num_keypoints_oks = len(coco_eval.params.kpt_oks_sigmas)\n",
    "        assert num_keypoints_oks == num_keypoints_dt == num_keypoints_gt, (\n",
    "            f\"[COCOEvaluator] Prediction contain {num_keypoints_dt} keypoints. \"\n",
    "            f\"Ground truth contains {num_keypoints_gt} keypoints. \"\n",
    "            f\"The length of cfg.TEST.KEYPOINT_OKS_SIGMAS is {num_keypoints_oks}. \"\n",
    "            \"They have to agree with each other. For meaning of OKS, please refer to \"\n",
    "            \"http://cocodataset.org/#keypoints-eval.\"\n",
    "        )\n",
    "\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    return coco_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "import datetime\n",
    "import logging\n",
    "import time\n",
    "from contextlib import contextmanager, ExitStack\n",
    "\n",
    "import torch\n",
    "from detectron2.evaluation.evaluator import DatasetEvaluators\n",
    "from detectron2.utils.comm import get_world_size\n",
    "from detectron2.utils.logger import log_every_n_seconds\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def inference_on_dataset(model, data_loader, evaluator, cfg):\n",
    "    \"\"\"\n",
    "    Run model on the data_loader and evaluate the metrics with evaluator.\n",
    "    Also benchmark the inference speed of `model.__call__` accurately.\n",
    "    The model will be used in eval mode.\n",
    "    Args:\n",
    "        model (callable): a callable which takes an object from\n",
    "            `data_loader` and returns some outputs.\n",
    "            If it's an nn.Module, it will be temporarily set to `eval` mode.\n",
    "            If you wish to evaluate a model in `training` mode instead, you can\n",
    "            wrap the given model and override its behavior of `.eval()` and `.train()`.\n",
    "        data_loader: an iterable object with a length.\n",
    "            The elements it generates will be the inputs to the model.\n",
    "        evaluator (DatasetEvaluator): the evaluator to run. Use `None` if you only want\n",
    "            to benchmark, but don't want to do any evaluation.\n",
    "    Returns:\n",
    "        The return value of `evaluator.evaluate()`\n",
    "    \"\"\"\n",
    "    num_devices = get_world_size()\n",
    "    # logger = logging.getLogger(__name__)\n",
    "    logger = logging.getLogger(\"detectron2.evaluation.evaluator\")\n",
    "    logger.info(\"Start inference on {} images\".format(len(data_loader)))\n",
    "\n",
    "    total = len(data_loader)  # inference data loader must have a fixed length\n",
    "    if evaluator is None:\n",
    "        # create a no-op evaluator\n",
    "        evaluator = DatasetEvaluators([])\n",
    "    evaluator.reset()\n",
    "\n",
    "    num_warmup = min(5, total - 1)\n",
    "    start_time = time.perf_counter()\n",
    "    total_compute_time = 0\n",
    "    with ExitStack() as stack:\n",
    "        if isinstance(model, nn.Module):\n",
    "            stack.enter_context(inference_context(model))\n",
    "        stack.enter_context(torch.no_grad())\n",
    "\n",
    "        for idx, inputs in enumerate(data_loader):\n",
    "            if idx == num_warmup:\n",
    "                start_time = time.perf_counter()\n",
    "                total_compute_time = 0\n",
    "\n",
    "            start_compute_time = time.perf_counter()\n",
    "            outputs = model(inputs, nms_method=cfg.MODEL.FCOS.NMS_CRITERIA_TEST)\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.synchronize()\n",
    "            total_compute_time += time.perf_counter() - start_compute_time\n",
    "            evaluator.process(inputs, outputs)\n",
    "\n",
    "            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)\n",
    "            seconds_per_img = total_compute_time / iters_after_start\n",
    "            if idx >= num_warmup * 2 or seconds_per_img > 5:\n",
    "                total_seconds_per_img = (\n",
    "                    time.perf_counter() - start_time\n",
    "                ) / iters_after_start\n",
    "                eta = datetime.timedelta(\n",
    "                    seconds=int(total_seconds_per_img * (total - idx - 1))\n",
    "                )\n",
    "                log_every_n_seconds(\n",
    "                    logging.INFO,\n",
    "                    \"Inference done {}/{}. {:.4f} s / img. ETA={}\".format(\n",
    "                        idx + 1, total, seconds_per_img, str(eta)\n",
    "                    ),\n",
    "                    n=5,\n",
    "                    name=\"detectron2.evaluation.evaluator\",\n",
    "                )\n",
    "\n",
    "    # Measure the time only for this worker (before the synchronization barrier)\n",
    "    total_time = time.perf_counter() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=total_time))\n",
    "    # NOTE this format is parsed by grep\n",
    "    logger.info(\n",
    "        \"Total inference time: {} ({:.6f} s / img per device, on {} devices)\".format(\n",
    "            total_time_str, total_time / (total - num_warmup), num_devices\n",
    "        )\n",
    "    )\n",
    "    total_compute_time_str = str(datetime.timedelta(seconds=int(total_compute_time)))\n",
    "    logger.info(\n",
    "        \"Total inference pure compute time: {} ({:.6f} s / img per device, on {} devices)\".format(\n",
    "            total_compute_time_str,\n",
    "            total_compute_time / (total - num_warmup),\n",
    "            num_devices,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    results = evaluator.evaluate()\n",
    "    # An evaluator may return None when not in main process.\n",
    "    # Replace it by an empty dict instead to make it easier for downstream code to handle\n",
    "    if results is None:\n",
    "        results = {}\n",
    "    return results\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def inference_context(model):\n",
    "    \"\"\"\n",
    "    A context where the model is temporarily changed to eval mode,\n",
    "    and restored to previous mode afterwards.\n",
    "    Args:\n",
    "        model: a torch Module\n",
    "    \"\"\"\n",
    "    training_mode = model.training\n",
    "    model.eval()\n",
    "    yield\n",
    "    model.train(training_mode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most part of this file is from AdelaiDet\n",
    "# https://github.com/aim-uofa/AdelaiDet\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class IOULoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Intersetion Over Union (IoU) loss which supports three\n",
    "    different IoU computations:\n",
    "\n",
    "    * IoU\n",
    "    * Linear IoU\n",
    "    * gIoU\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loc_loss_type=\"iou\"):\n",
    "        super(IOULoss, self).__init__()\n",
    "        self.loc_loss_type = loc_loss_type\n",
    "\n",
    "    def forward(self, pred, target, weight=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: Nx4 predicted bounding boxes\n",
    "            target: Nx4 target bounding boxes\n",
    "            weight: N loss weight for each instance\n",
    "        \"\"\"\n",
    "        pred_left = pred[:, 0]\n",
    "        pred_top = pred[:, 1]\n",
    "        pred_right = pred[:, 2]\n",
    "        pred_bottom = pred[:, 3]\n",
    "\n",
    "        target_left = target[:, 0]\n",
    "        target_top = target[:, 1]\n",
    "        target_right = target[:, 2]\n",
    "        target_bottom = target[:, 3]\n",
    "\n",
    "        target_aera = (target_left + target_right) * \\\n",
    "            (target_top + target_bottom)\n",
    "        pred_aera = (pred_left + pred_right) * (pred_top + pred_bottom)\n",
    "\n",
    "        w_intersect = torch.min(pred_left, target_left) + torch.min(\n",
    "            pred_right, target_right\n",
    "        )\n",
    "        h_intersect = torch.min(pred_bottom, target_bottom) + torch.min(\n",
    "            pred_top, target_top\n",
    "        )\n",
    "\n",
    "        g_w_intersect = torch.max(pred_left, target_left) + torch.max(\n",
    "            pred_right, target_right\n",
    "        )\n",
    "        g_h_intersect = torch.max(pred_bottom, target_bottom) + torch.max(\n",
    "            pred_top, target_top\n",
    "        )\n",
    "        ac_uion = g_w_intersect * g_h_intersect\n",
    "\n",
    "        area_intersect = w_intersect * h_intersect\n",
    "        area_union = target_aera + pred_aera - area_intersect\n",
    "\n",
    "        ious = (area_intersect + 1.0) / (area_union + 1.0)\n",
    "        gious = ious - (ac_uion - area_union) / ac_uion\n",
    "        if self.loc_loss_type == \"iou\":\n",
    "            losses = -torch.log(ious)\n",
    "        elif self.loc_loss_type == \"linear_iou\":\n",
    "            losses = 1 - ious\n",
    "        elif self.loc_loss_type == \"giou\":\n",
    "            losses = 1 - gious\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if weight is not None:\n",
    "            return (losses * weight).sum()\n",
    "        else:\n",
    "            return losses.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is from AdelaiDet\n",
    "# https://github.com/aim-uofa/AdelaiDet\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class KLLoss(nn.Module):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(KLLoss, self).__init__()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input,\n",
    "        input_std,\n",
    "        target,\n",
    "        weight=None,\n",
    "        iou_weight=None,\n",
    "        beta=1.0,\n",
    "        loss_denorm=None,\n",
    "        method=\"weight_ctr_sum\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: Nx4 predicted bounding boxes\n",
    "            target: Nx4 target bounding boxes\n",
    "            weight: N loss weight for each instance\n",
    "        \"\"\"\n",
    "        # TODO: check beta\n",
    "\n",
    "        # TODO: check bbox_inside_weights, bbox_outside_weights, getlossscale\n",
    "\n",
    "        if beta < 1e-5:\n",
    "            # if beta == 0, then torch.where will result in nan gradients when\n",
    "            # the chain rule is applied due to pytorch implementation details\n",
    "            # (the False branch \"0.5 * n ** 2 / 0\" has an incoming gradient of\n",
    "            # zeros, rather than \"no gradient\"). To avoid this issue, we define\n",
    "            # small values of beta to be exactly l1 loss.\n",
    "            loss = torch.abs(input - target)\n",
    "        else:\n",
    "            n = torch.abs(input - target)\n",
    "            cond = n < beta\n",
    "            l1_smooth = torch.where(cond, 0.5 * n**2 / beta, n - 0.5 * beta)\n",
    "            # loss = torch.exp(-input_std)*l1_smooth.detach() + 0.5*input_std + l1_smooth\n",
    "            loss = torch.exp(-input_std) * l1_smooth + 0.5 * input_std\n",
    "\n",
    "            if method == \"weight_ctr_sum\":\n",
    "                assert weight is not None\n",
    "                loss = loss.sum(dim=1)\n",
    "                return (loss * weight).sum()\n",
    "            elif method == \"weight_ctr_mean\":\n",
    "                assert weight is not None\n",
    "                assert loss_denorm is not None\n",
    "                loss = loss.sum(dim=1)\n",
    "                return (loss * weight).sum() / loss_denorm\n",
    "            elif method == \"sum\":\n",
    "                return loss.sum()\n",
    "            elif method == \"mean\":\n",
    "                return loss.mean()\n",
    "            else:\n",
    "                raise ValueError(\"No defined regression loss method\")\n",
    "\n",
    "\n",
    "class NLLoss(nn.Module):\n",
    "    \"\"\" \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NLLoss, self).__init__()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input,\n",
    "        input_std,\n",
    "        target,\n",
    "        weight=None,\n",
    "        iou_weight=None,\n",
    "        beta=1.0,\n",
    "        loss_denorm=None,\n",
    "        method=\"weight_ctr_sum\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pred: Nx4 predicted bounding boxes; before sigmoid\n",
    "            target: Nx4 target bounding boxes\n",
    "            weight: N loss weight for each instance\n",
    "        \"\"\"\n",
    "        # TODO: check bbox_inside_weights, bbox_outside_weights, getlossscale\n",
    "        mean = input\n",
    "        sigma = input_std.sigmoid()\n",
    "        sigma_sq = torch.square(sigma)\n",
    "\n",
    "        # smooth l1 ?\n",
    "        # Gradient explosion and predict log(2*sigma) instead?\n",
    "        first_term = torch.square(target - mean) / (2 * sigma_sq)\n",
    "        second_term = 0.5 * torch.log(sigma_sq)\n",
    "        sum_before_iou = (first_term + second_term).sum(dim=1) + 2 * torch.log(\n",
    "            2 * torch.Tensor([math.pi]).cuda()\n",
    "        )\n",
    "        loss_mean = (sum_before_iou * iou_weight).mean()\n",
    "        return loss_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is from AdelaiDet\n",
    "# https://github.com/aim-uofa/AdelaiDet\n",
    "\n",
    "\n",
    "from detectron2.layers import batched_nms\n",
    "\n",
    "\n",
    "def ml_nms(\n",
    "    boxlist, nms_thresh, max_proposals=-1, score_field=\"scores\", label_field=\"labels\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs non-maximum suppression on a boxlist, with scores specified\n",
    "    in a boxlist field via score_field.\n",
    "\n",
    "    Args:\n",
    "        boxlist (detectron2.structures.Boxes):\n",
    "        nms_thresh (float):\n",
    "        max_proposals (int): if > 0, then only the top max_proposals are kept\n",
    "            after non-maximum suppression\n",
    "        score_field (str):\n",
    "    \"\"\"\n",
    "    if nms_thresh <= 0:\n",
    "        return boxlist\n",
    "    boxes = boxlist.pred_boxes.tensor\n",
    "    scores = boxlist.scores\n",
    "    labels = boxlist.pred_classes\n",
    "    keep = batched_nms(boxes, scores, labels, nms_thresh)\n",
    "    if max_proposals > 0:\n",
    "        keep = keep[:max_proposals]\n",
    "    boxlist = boxlist[keep]\n",
    "    return boxlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is from AdelaiDet\n",
    "# https://github.com/aim-uofa/AdelaiDet\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.nn import init, Module, Parameter\n",
    "\n",
    "\n",
    "class NaiveGroupNorm(Module):\n",
    "    r\"\"\"NaiveGroupNorm implements Group Normalization with the high-level matrix operations in PyTorch.\n",
    "    It is a temporary solution to export GN by ONNX before the official GN can be exported by ONNX.\n",
    "    The usage of NaiveGroupNorm is exactly the same as the official :class:`torch.nn.GroupNorm`.\n",
    "    Args:\n",
    "        num_groups (int): number of groups to separate the channels into\n",
    "        num_channels (int): number of channels expected in input\n",
    "        eps: a value added to the denominator for numerical stability. Default: 1e-5\n",
    "        affine: a boolean value that when set to ``True``, this module\n",
    "            has learnable per-channel affine parameters initialized to ones (for weights)\n",
    "            and zeros (for biases). Default: ``True``.\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, C, *)` where :math:`C=\\text{num\\_channels}`\n",
    "        - Output: :math:`(N, C, *)` (same shape as input)\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        >>> input = torch.randn(20, 6, 10, 10)\n",
    "        >>> # Separate 6 channels into 3 groups\n",
    "        >>> m = NaiveGroupNorm(3, 6)\n",
    "        >>> # Separate 6 channels into 6 groups (equivalent with InstanceNorm)\n",
    "        >>> m = NaiveGroupNorm(6, 6)\n",
    "        >>> # Put all 6 channels into a single group (equivalent with LayerNorm)\n",
    "        >>> m = NaiveGroupNorm(1, 6)\n",
    "        >>> # Activating the module\n",
    "        >>> output = m(input)\n",
    "\n",
    "    .. _`Group Normalization`: https://arxiv.org/abs/1803.08494\n",
    "    \"\"\"\n",
    "    __constants__ = [\"num_groups\", \"num_channels\", \"eps\", \"affine\", \"weight\", \"bias\"]\n",
    "\n",
    "    def __init__(self, num_groups, num_channels, eps=1e-5, affine=True):\n",
    "        super(NaiveGroupNorm, self).__init__()\n",
    "        self.num_groups = num_groups\n",
    "        self.num_channels = num_channels\n",
    "        self.eps = eps\n",
    "        self.affine = affine\n",
    "        if self.affine:\n",
    "            self.weight = Parameter(torch.Tensor(num_channels))\n",
    "            self.bias = Parameter(torch.Tensor(num_channels))\n",
    "        else:\n",
    "            self.register_parameter(\"weight\", None)\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.affine:\n",
    "            init.ones_(self.weight)\n",
    "            init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, input):\n",
    "        N, C, H, W = input.size()\n",
    "        assert C % self.num_groups == 0\n",
    "        input = input.reshape(N, self.num_groups, -1)\n",
    "        mean = input.mean(dim=-1, keepdim=True)\n",
    "        var = (input**2).mean(dim=-1, keepdim=True) - mean**2\n",
    "        std = torch.sqrt(var + self.eps)\n",
    "\n",
    "        input = (input - mean) / std\n",
    "        input = input.reshape(N, C, H, W)\n",
    "        if self.affine:\n",
    "            input = input * self.weight.reshape(1, C, 1, 1) + self.bias.reshape(\n",
    "                1, C, 1, 1\n",
    "            )\n",
    "        return input\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"{num_groups}, {num_channels}, eps={eps}, \" \"affine={affine}\".format(\n",
    "            **self.__dict__\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is modified from the AdeliaDet\n",
    "\n",
    "import fvcore.nn.weight_init as weight_init\n",
    "import torch.nn.functional as F\n",
    "from detectron2.layers import ShapeSpec\n",
    "from detectron2.modeling.backbone import build_resnet_backbone, FPN\n",
    "from detectron2.modeling.backbone.build import BACKBONE_REGISTRY\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LastLevelP6P7(nn.Module):\n",
    "    \"\"\"\n",
    "    This module is used in RetinaNet and FCOS to generate extra layers, P6 and P7 from\n",
    "    C5 or P5 feature.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, in_features=\"res5\"):\n",
    "        super().__init__()\n",
    "        self.num_levels = 2\n",
    "        self.in_feature = in_features\n",
    "        self.p6 = nn.Conv2d(in_channels, out_channels, 3, 2, 1)\n",
    "        self.p7 = nn.Conv2d(out_channels, out_channels, 3, 2, 1)\n",
    "        for module in [self.p6, self.p7]:\n",
    "            weight_init.c2_xavier_fill(module)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p6 = self.p6(x)\n",
    "        p7 = self.p7(F.relu(p6))\n",
    "        return [p6, p7]\n",
    "\n",
    "\n",
    "class LastLevelP6(nn.Module):\n",
    "    \"\"\"\n",
    "    This module is used in FCOS to generate extra layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, in_features=\"res5\"):\n",
    "        super().__init__()\n",
    "        self.num_levels = 1\n",
    "        self.in_feature = in_features\n",
    "        self.p6 = nn.Conv2d(in_channels, out_channels, 3, 2, 1)\n",
    "        for module in [self.p6]:\n",
    "            weight_init.c2_xavier_fill(module)\n",
    "\n",
    "    def forward(self, x):\n",
    "        p6 = self.p6(x)\n",
    "        return [p6]\n",
    "\n",
    "\n",
    "@BACKBONE_REGISTRY.register()\n",
    "def build_fcos_resnet_fpn_backbone(cfg, input_shape: ShapeSpec):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        cfg: a detectron2 CfgNode\n",
    "\n",
    "    Returns:\n",
    "        backbone (Backbone): backbone module, must be a subclass of :class:`Backbone`.\n",
    "    \"\"\"\n",
    "    bottom_up = build_resnet_backbone(cfg, input_shape)\n",
    "    in_features = cfg.MODEL.FPN.IN_FEATURES\n",
    "    out_channels = cfg.MODEL.FPN.OUT_CHANNELS\n",
    "    top_levels = cfg.MODEL.FCOS.TOP_LEVELS\n",
    "    in_channels_top = out_channels\n",
    "    if top_levels == 2:\n",
    "        top_block = LastLevelP6P7(in_channels_top, out_channels, \"p5\")\n",
    "    if top_levels == 1:\n",
    "        top_block = LastLevelP6(in_channels_top, out_channels, \"p5\")\n",
    "    elif top_levels == 0:\n",
    "        top_block = None\n",
    "    backbone = FPN(\n",
    "        bottom_up=bottom_up,\n",
    "        in_features=in_features,\n",
    "        out_channels=out_channels,\n",
    "        norm=cfg.MODEL.FPN.NORM,\n",
    "        top_block=top_block,\n",
    "        fuse_type=cfg.MODEL.FPN.FUSE_TYPE,\n",
    "    )\n",
    "    return backbone\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from detectron2.utils.comm import get_world_size\n",
    "\n",
    "\n",
    "def reduce_sum(tensor):\n",
    "    world_size = get_world_size()\n",
    "    if world_size < 2:\n",
    "        return tensor\n",
    "    tensor = tensor.clone()\n",
    "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def aligned_bilinear(tensor, factor):\n",
    "    assert tensor.dim() == 4\n",
    "    assert factor >= 1\n",
    "    assert int(factor) == factor\n",
    "\n",
    "    if factor == 1:\n",
    "        return tensor\n",
    "\n",
    "    h, w = tensor.size()[2:]\n",
    "    tensor = F.pad(tensor, pad=(0, 1, 0, 1), mode=\"replicate\")\n",
    "    oh = factor * h + 1\n",
    "    ow = factor * w + 1\n",
    "    tensor = F.interpolate(tensor, size=(oh, ow), mode=\"bilinear\", align_corners=True)\n",
    "    tensor = F.pad(tensor, pad=(factor // 2, 0, factor // 2, 0), mode=\"replicate\")\n",
    "\n",
    "    return tensor[:, :, : oh - 1, : ow - 1]\n",
    "\n",
    "\n",
    "def compute_locations(h, w, stride, device):\n",
    "    shifts_x = torch.arange(\n",
    "        0, w * stride, step=stride, dtype=torch.float32, device=device\n",
    "    )\n",
    "    shifts_y = torch.arange(\n",
    "        0, h * stride, step=stride, dtype=torch.float32, device=device\n",
    "    )\n",
    "    shift_y, shift_x = torch.meshgrid(shifts_y, shifts_x)\n",
    "    shift_x = shift_x.reshape(-1)\n",
    "    shift_y = shift_y.reshape(-1)\n",
    "    locations = torch.stack((shift_x, shift_y), dim=1) + stride // 2\n",
    "    return locations\n",
    "\n",
    "\n",
    "def distance2bbox(points, distance, max_shape=None):\n",
    "    \"\"\"Decode distance prediction to bounding box.\n",
    "\n",
    "    Args:\n",
    "        points (Tensor): Shape (n, 2), [x, y].\n",
    "        distance (Tensor): Distance from the given point to 4\n",
    "            boundaries (left, top, right, bottom).\n",
    "        max_shape (tuple): Shape of the image.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Decoded bboxes.\n",
    "    \"\"\"\n",
    "    x1 = points[:, 0] - distance[:, 0]\n",
    "    y1 = points[:, 1] - distance[:, 1]\n",
    "    x2 = points[:, 0] + distance[:, 2]\n",
    "    y2 = points[:, 1] + distance[:, 3]\n",
    "    if max_shape is not None:\n",
    "        x1 = x1.clamp(min=0, max=max_shape[1])\n",
    "        y1 = y1.clamp(min=0, max=max_shape[0])\n",
    "        x2 = x2.clamp(min=0, max=max_shape[1])\n",
    "        y2 = y2.clamp(min=0, max=max_shape[0])\n",
    "    return torch.stack([x1, y1, x2, y2], -1)\n",
    "\n",
    "\n",
    "def bbox2distance(points, bbox, max_dis=None, eps=0.1):\n",
    "    \"\"\"Decode bounding box based on distances.\n",
    "\n",
    "    Args:\n",
    "        points (Tensor): Shape (n, 2), [x, y].\n",
    "        bbox (Tensor): Shape (n, 4), \"xyxy\" format\n",
    "        max_dis (float): Upper bound of the distance.\n",
    "        eps (float): a small value to ensure target < max_dis, instead <=\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Decoded distances.\n",
    "    \"\"\"\n",
    "    left = points[:, 0] - bbox[:, 0]\n",
    "    top = points[:, 1] - bbox[:, 1]\n",
    "    right = bbox[:, 2] - points[:, 0]\n",
    "    bottom = bbox[:, 3] - points[:, 1]\n",
    "    if max_dis is not None:\n",
    "        left = left.clamp(min=0, max=max_dis - eps)\n",
    "        top = top.clamp(min=0, max=max_dis - eps)\n",
    "        right = right.clamp(min=0, max=max_dis - eps)\n",
    "        bottom = bottom.clamp(min=0, max=max_dis - eps)\n",
    "    return torch.stack([left, top, right, bottom], -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "from bisect import bisect_right\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from detectron2.solver.lr_scheduler import _get_warmup_factor_at_iter\n",
    "\n",
    "\n",
    "class WarmupTwoStageMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(\n",
    "        self,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        milestones: List[int],\n",
    "        factor_list: List[int],\n",
    "        gamma: float = 0.1,\n",
    "        warmup_factor: float = 0.001,\n",
    "        warmup_iters: int = 1000,\n",
    "        warmup_method: str = \"linear\",\n",
    "        last_epoch: int = -1,\n",
    "    ):\n",
    "        if not list(milestones) == sorted(milestones):\n",
    "            raise ValueError(\n",
    "                \"Milestones should be a list of\" \" increasing integers. Got {}\",\n",
    "                milestones,\n",
    "            )\n",
    "        if len(milestones) + 1 != len(factor_list):\n",
    "            raise ValueError(\"Length of milestones should match length of factor_list.\")\n",
    "\n",
    "        self.milestones = milestones\n",
    "        self.gamma = gamma\n",
    "        self.warmup_factor = warmup_factor\n",
    "        self.warmup_iters = warmup_iters\n",
    "        self.warmup_method = warmup_method\n",
    "        self.factor_list = factor_list\n",
    "\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self) -> List[float]:\n",
    "\n",
    "        warmup_factor = _get_warmup_factor_at_iter(\n",
    "            self.warmup_method, self.last_epoch, self.warmup_iters, self.warmup_factor\n",
    "        )\n",
    "\n",
    "        return [\n",
    "            base_lr\n",
    "            * warmup_factor\n",
    "            * self.factor_list[bisect_right(self.milestones, self.last_epoch)]\n",
    "            for base_lr in self.base_lrs\n",
    "        ]\n",
    "\n",
    "    def _compute_values(self) -> List[float]:\n",
    "        # The new interface\n",
    "        return self.get_lr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import torch\n",
    "from detectron2.config import CfgNode\n",
    "from detectron2.solver.lr_scheduler import WarmupCosineLR, WarmupMultiStepLR\n",
    "\n",
    "# from .lr_scheduler import WarmupTwoStageMultiStepLR\n",
    "\n",
    "\n",
    "def build_lr_scheduler(\n",
    "    cfg: CfgNode, optimizer: torch.optim.Optimizer\n",
    ") -> torch.optim.lr_scheduler._LRScheduler:\n",
    "    \"\"\"\n",
    "    Build a LR scheduler from config.\n",
    "    \"\"\"\n",
    "    name = cfg.SOLVER.LR_SCHEDULER_NAME\n",
    "    if name == \"WarmupMultiStepLR\":\n",
    "        return WarmupMultiStepLR(\n",
    "            optimizer,\n",
    "            cfg.SOLVER.STEPS,\n",
    "            cfg.SOLVER.GAMMA,\n",
    "            warmup_factor=cfg.SOLVER.WARMUP_FACTOR,\n",
    "            warmup_iters=cfg.SOLVER.WARMUP_ITERS,\n",
    "            warmup_method=cfg.SOLVER.WARMUP_METHOD,\n",
    "        )\n",
    "    elif name == \"WarmupCosineLR\":\n",
    "        return WarmupCosineLR(\n",
    "            optimizer,\n",
    "            cfg.SOLVER.MAX_ITER,\n",
    "            warmup_factor=cfg.SOLVER.WARMUP_FACTOR,\n",
    "            warmup_iters=cfg.SOLVER.WARMUP_ITERS,\n",
    "            warmup_method=cfg.SOLVER.WARMUP_METHOD,\n",
    "        )\n",
    "    elif name == \"WarmupTwoStageMultiStepLR\":\n",
    "        return WarmupTwoStageMultiStepLR(\n",
    "            optimizer,\n",
    "            cfg.SOLVER.STEPS,\n",
    "            factor_list=cfg.SOLVER.FACTOR_LIST,\n",
    "            gamma=cfg.SOLVER.GAMMA,\n",
    "            warmup_factor=cfg.SOLVER.WARMUP_FACTOR,\n",
    "            warmup_iters=cfg.SOLVER.WARMUP_ITERS,\n",
    "            warmup_method=cfg.SOLVER.WARMUP_METHOD,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown LR scheduler: {}\".format(name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most part of this file is modified from AdelaiDet\n",
    "# https://github.com/aim-uofa/AdelaiDet\n",
    "\n",
    "\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from detectron2.layers import cat\n",
    "from detectron2.structures import Boxes, Instances\n",
    "from detectron2.utils.comm import get_world_size\n",
    "from fvcore.nn import sigmoid_focal_loss_jit\n",
    "from torch import nn\n",
    "# from ubteacher.layers import IOULoss, KLLoss, ml_nms, NLLoss\n",
    "# from ubteacher.utils.comm import reduce_sum\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "INF = 100000000\n",
    "\n",
    "\"\"\"\n",
    "Shape shorthand in this module:\n",
    "\n",
    "    N: number of images in the minibatch\n",
    "    L: number of feature maps per image on which RPN is run\n",
    "    Hi, Wi: height and width of the i-th feature map\n",
    "    4: size of the box parameterization\n",
    "\n",
    "Naming convention:\n",
    "\n",
    "    labels: refers to the ground-truth class of an position.\n",
    "\n",
    "    reg_targets: refers to the 4-d (left, top, right, bottom) distances that parameterize the ground-truth box.\n",
    "\n",
    "    logits_pred: predicted classification scores in [-inf, +inf];\n",
    "\n",
    "    reg_pred: the predicted (left, top, right, bottom), corresponding to reg_targets\n",
    "\n",
    "    ctrness_pred: predicted centerness scores\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Integral(nn.Module):\n",
    "    \"\"\"A fixed layer for calculating integral result from distribution.\n",
    "    This layer calculates the target location by :math: `sum{P(y_i) * y_i}`,\n",
    "    P(y_i) denotes the softmax vector that represents the discrete distribution\n",
    "    y_i denotes the discrete set, usually {0, 1, 2, ..., reg_max}\n",
    "    Args:\n",
    "        reg_max (int): The maximal value of the discrete set. Default: 16. You\n",
    "            may want to reset it according to your new dataset or related\n",
    "            settings.\n",
    "\n",
    "    From generalized focal loss v2\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, reg_max=16):\n",
    "        super(Integral, self).__init__()\n",
    "        self.reg_max = reg_max\n",
    "        self.register_buffer(\n",
    "            \"project\", torch.linspace(0, self.reg_max, self.reg_max + 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward feature from the regression head to get integral result of\n",
    "        bounding box location.\n",
    "        Args:\n",
    "            x (Tensor): Features of the regression head, shape (N, 4*(n+1)),\n",
    "                n is self.reg_max.\n",
    "        Returns:\n",
    "            x (Tensor): Integral result of box locations, i.e., distance\n",
    "                offsets from the box center in four directions, shape (N, 4).\n",
    "        \"\"\"\n",
    "        x = F.softmax(x.reshape(-1, self.reg_max + 1), dim=1)\n",
    "        x = F.linear(x, self.project.type_as(x)).reshape(-1, 4)\n",
    "        return x\n",
    "\n",
    "\n",
    "def compute_ctrness_targets(reg_targets):\n",
    "    if len(reg_targets) == 0:\n",
    "        return reg_targets.new_zeros(len(reg_targets))\n",
    "    left_right = reg_targets[:, [0, 2]]\n",
    "    top_bottom = reg_targets[:, [1, 3]]\n",
    "    ctrness = (left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * (\n",
    "        top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0]\n",
    "    )\n",
    "    return torch.sqrt(ctrness)\n",
    "\n",
    "\n",
    "def compute_iou_targets(pred, target):\n",
    "    \"\"\"\n",
    "    reg_pred: (# of instnaces, 4) in normalized format\n",
    "    reg_targets: (# of instances, 4) in normalized format\n",
    "\n",
    "    ctrness_targets = compute_iou_targets(\n",
    "        reg_pred.detach(),\n",
    "        instances.reg_targets)\n",
    "\n",
    "    \"\"\"\n",
    "    if len(target) == 0:\n",
    "        return target.new_zeros(len(target))\n",
    "\n",
    "    pred_left = pred[:, 0]\n",
    "    pred_top = pred[:, 1]\n",
    "    pred_right = pred[:, 2]\n",
    "    pred_bottom = pred[:, 3]\n",
    "\n",
    "    target_left = target[:, 0]\n",
    "    target_top = target[:, 1]\n",
    "    target_right = target[:, 2]\n",
    "    target_bottom = target[:, 3]\n",
    "\n",
    "    target_aera = (target_left + target_right) * (target_top + target_bottom)\n",
    "    pred_aera = (pred_left + pred_right) * (pred_top + pred_bottom)\n",
    "\n",
    "    w_intersect = torch.min(pred_left, target_left) + torch.min(\n",
    "        pred_right, target_right\n",
    "    )\n",
    "    h_intersect = torch.min(pred_bottom, target_bottom) + torch.min(\n",
    "        pred_top, target_top\n",
    "    )\n",
    "\n",
    "    area_intersect = w_intersect * h_intersect\n",
    "    area_union = target_aera + pred_aera - area_intersect\n",
    "\n",
    "    ious = (area_intersect + 1.0) / (area_union + 1.0)\n",
    "\n",
    "    return ious\n",
    "\n",
    "\n",
    "class FCOSOutputs(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(FCOSOutputs, self).__init__()\n",
    "\n",
    "        self.focal_loss_alpha = cfg.MODEL.FCOS.LOSS_ALPHA\n",
    "        self.focal_loss_gamma = cfg.MODEL.FCOS.LOSS_GAMMA\n",
    "        self.center_sample = cfg.MODEL.FCOS.CENTER_SAMPLE\n",
    "        self.radius = cfg.MODEL.FCOS.POS_RADIUS\n",
    "        self.pre_nms_thresh_train = cfg.MODEL.FCOS.INFERENCE_TH_TRAIN\n",
    "        self.pre_nms_topk_train = cfg.MODEL.FCOS.PRE_NMS_TOPK_TRAIN\n",
    "        self.post_nms_topk_train = cfg.MODEL.FCOS.POST_NMS_TOPK_TRAIN\n",
    "\n",
    "        self.pre_nms_thresh_test = cfg.MODEL.FCOS.INFERENCE_TH_TEST\n",
    "        self.pre_nms_topk_test = cfg.MODEL.FCOS.PRE_NMS_TOPK_TEST\n",
    "        self.post_nms_topk_test = cfg.MODEL.FCOS.POST_NMS_TOPK_TEST\n",
    "        self.nms_thresh = cfg.MODEL.FCOS.NMS_TH\n",
    "        self.thresh_with_ctr = cfg.MODEL.FCOS.THRESH_WITH_CTR\n",
    "\n",
    "        self.num_classes = cfg.MODEL.FCOS.NUM_CLASSES\n",
    "        self.strides = cfg.MODEL.FCOS.FPN_STRIDES\n",
    "\n",
    "        # box loss weight\n",
    "        self.cls_loss_weight = cfg.SEMISUPNET.SOFT_CLS_LABEL\n",
    "        self.cls_loss_method = cfg.SEMISUPNET.CLS_LOSS_METHOD\n",
    "\n",
    "        # bin offset classification\n",
    "        self.reg_discrete = cfg.MODEL.FCOS.REG_DISCRETE\n",
    "        self.reg_max = cfg.MODEL.FCOS.REG_MAX\n",
    "        self.fpn_stride = torch.tensor(cfg.MODEL.FCOS.FPN_STRIDES).cuda().float()\n",
    "        self.dfl_loss_weight = cfg.MODEL.FCOS.DFL_WEIGHT\n",
    "        self.unify_ctrcls = cfg.MODEL.FCOS.UNIFY_CTRCLS\n",
    "\n",
    "        # kl loss\n",
    "        self.kl_loss = cfg.MODEL.FCOS.KL_LOSS\n",
    "        self.kl_loss_type = cfg.MODEL.FCOS.KL_LOSS_TYPE  # 'klloss' or 'nlloss'\n",
    "        self.kl_loss_weight = cfg.MODEL.FCOS.KLLOSS_WEIGHT\n",
    "\n",
    "        self.loc_fun_all = cfg.MODEL.FCOS.LOC_FUN_ALL\n",
    "\n",
    "        # unsupervised regression loss\n",
    "        self.reg_unsup_loss = cfg.SEMISUPNET.CONSIST_REG_LOSS\n",
    "\n",
    "        #  KL loss  or IoU loss\n",
    "        if self.kl_loss:\n",
    "            if self.kl_loss_type == \"klloss\":\n",
    "                self.kl_loc_loss_func = KLLoss()\n",
    "            elif self.kl_loss_type == \"nlloss\":\n",
    "                self.kl_loc_loss_func = NLLoss()\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "        self.loc_loss_func = IOULoss(cfg.MODEL.FCOS.LOC_LOSS_TYPE)\n",
    "\n",
    "        # Quality estimation\n",
    "        self.quality_est = cfg.MODEL.FCOS.QUALITY_EST\n",
    "\n",
    "        # TS better classification\n",
    "        self.cls_loss_pseudo_method = cfg.SEMISUPNET.CLS_LOSS_PSEUDO_METHOD\n",
    "        self.tsbetter_cls_sigma = cfg.MODEL.FCOS.TSBETTER_CLS_SIGMA\n",
    "\n",
    "        # TS better\n",
    "        self.tsbetter_reg = cfg.SEMISUPNET.TS_BETTER\n",
    "        self.tsbetter_reg_cert = cfg.SEMISUPNET.TS_BETTER_CERT\n",
    "\n",
    "        # Ratio\n",
    "        # self.fg_bg_ratio = cfg.MODEL.FCOS.FG_BG_RATIO\n",
    "\n",
    "        # generate sizes of interest\n",
    "        soi = []\n",
    "        prev_size = -1\n",
    "        for s in cfg.MODEL.FCOS.SIZES_OF_INTEREST:\n",
    "            soi.append([prev_size, s])\n",
    "            prev_size = s\n",
    "        soi.append([prev_size, INF])\n",
    "        self.sizes_of_interest = soi\n",
    "\n",
    "        self.integral = Integral(self.reg_max)\n",
    "\n",
    "    # loss\n",
    "    # supervised loss branch\n",
    "    def losses(\n",
    "        self,\n",
    "        logits_pred,\n",
    "        reg_pred,\n",
    "        ctrness_pred,\n",
    "        locations,\n",
    "        gt_instances,\n",
    "        reg_pred_std=None,\n",
    "        top_feats=None,\n",
    "        ignore_near=False,\n",
    "        branch=\"\",\n",
    "    ):\n",
    "        training_targets = self._get_ground_truth(locations, gt_instances, ignore_near)\n",
    "\n",
    "        instances = Instances((0, 0))\n",
    "        instances.labels = cat(\n",
    "            [x.reshape(-1) for x in training_targets[\"labels\"]],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        instances.box_weights = cat(\n",
    "            [x.reshape(-1) for x in training_targets[\"box_weights\"]],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # ignore some samples during training\n",
    "        instances.keep_locations = cat(\n",
    "            [x.reshape(-1) for x in training_targets[\"keep_locations\"]],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        instances.gt_inds = cat(\n",
    "            [x.reshape(-1) for x in training_targets[\"target_inds\"]],\n",
    "            dim=0,\n",
    "        )\n",
    "        instances.im_inds = cat(\n",
    "            [x.reshape(-1) for x in training_targets[\"im_inds\"]], dim=0\n",
    "        )\n",
    "        instances.reg_targets = cat(\n",
    "            [x.reshape(-1, 4) for x in training_targets[\"reg_targets\"]],\n",
    "            dim=0,\n",
    "        )\n",
    "        instances.locations = cat(\n",
    "            [x.reshape(-1, 2) for x in training_targets[\"locations\"]], dim=0\n",
    "        )\n",
    "        instances.fpn_levels = cat(\n",
    "            [x.reshape(-1) for x in training_targets[\"fpn_levels\"]], dim=0\n",
    "        )\n",
    "\n",
    "        instances.logits_pred = cat(\n",
    "            [x.permute(0, 2, 3, 1).reshape(-1, self.num_classes) for x in logits_pred],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        if self.reg_discrete:\n",
    "            instances.reg_pred = cat(\n",
    "                [\n",
    "                    x.permute(0, 2, 3, 1).reshape(-1, 4 * (self.reg_max + 1))\n",
    "                    for x in reg_pred\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "        else:\n",
    "            instances.reg_pred = cat(\n",
    "                [x.permute(0, 2, 3, 1).reshape(-1, 4) for x in reg_pred],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        if self.kl_loss:\n",
    "            assert reg_pred_std is not None\n",
    "            instances.reg_pred_std = cat(\n",
    "                [x.permute(0, 2, 3, 1).reshape(-1, 4) for x in reg_pred_std],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        instances.ctrness_pred = cat(\n",
    "            [x.permute(0, 2, 3, 1).reshape(-1) for x in ctrness_pred],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        if len(top_feats) > 0:\n",
    "            instances.top_feats = cat(\n",
    "                [\n",
    "                    # Reshape: (N, -1, Hi, Wi) -> (N*Hi*Wi, -1)\n",
    "                    x.permute(0, 2, 3, 1).reshape(-1, x.size(1))\n",
    "                    for x in top_feats\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        if branch == \"labeled\":\n",
    "            return self.fcos_losses(instances)\n",
    "        else:\n",
    "            raise ValueError(\"Incorrect branch name\")\n",
    "\n",
    "    def fcos_losses(self, instances):\n",
    "\n",
    "        losses = {}\n",
    "        if instances.keep_locations.sum() > 0:  # some instances are not ignored\n",
    "            instances = instances[instances.keep_locations]\n",
    "\n",
    "        num_classes = instances.logits_pred.size(1)\n",
    "        assert num_classes == self.num_classes\n",
    "\n",
    "        labels = instances.labels.flatten()\n",
    "        pos_inds = torch.nonzero(labels != num_classes).squeeze(1)\n",
    "        num_pos_local = pos_inds.numel()\n",
    "        num_gpus = get_world_size()\n",
    "        total_num_pos = reduce_sum(pos_inds.new_tensor([num_pos_local])).item()\n",
    "        num_pos_avg = max(total_num_pos / num_gpus, 1.0)\n",
    "\n",
    "        # prepare one_hot (N, 1000)\n",
    "        class_target = torch.zeros_like(instances.logits_pred)\n",
    "        class_target[pos_inds, labels[pos_inds]] = 1\n",
    "\n",
    "        # classification loss (unifying branch or individual branch)\n",
    "        if self.cls_loss_method == \"focal\":\n",
    "            class_loss_all = sigmoid_focal_loss_jit(\n",
    "                instances.logits_pred,\n",
    "                class_target,\n",
    "                alpha=self.focal_loss_alpha,\n",
    "                gamma=self.focal_loss_gamma,\n",
    "                reduction=\"none\",\n",
    "            )\n",
    "            ## sum over class dimension\n",
    "            weighted_class_loss = class_loss_all.sum(1)\n",
    "            class_loss = weighted_class_loss.sum() / num_pos_avg\n",
    "\n",
    "        ## only compute the centerness loss and regression loss for the foreground classes\n",
    "        instances = instances[pos_inds]\n",
    "        instances.pos_inds = pos_inds\n",
    "        ##  process regression prediction (from discrete to continous)\n",
    "        ##  we find this helps the unsupervised loss\n",
    "        if self.reg_discrete and pos_inds.numel() > 0:  # offset bin classification\n",
    "            pred_ltrb_discrete = instances.reg_pred\n",
    "            pred_ltrb_scalar = self.integral(pred_ltrb_discrete)\n",
    "            reg_pred = pred_ltrb_scalar\n",
    "        else:\n",
    "            reg_pred = instances.reg_pred\n",
    "\n",
    "        # process target for centerness loss\n",
    "        if self.quality_est == \"centerness\":\n",
    "            ctrness_targets = compute_ctrness_targets(instances.reg_targets)\n",
    "        elif self.quality_est == \"iou\":\n",
    "            # pos_decode_bbox_pred: xyxy, pos_decode_bbox_targets: xyxy\n",
    "            ctrness_targets = compute_iou_targets(\n",
    "                reg_pred.detach(), instances.reg_targets\n",
    "            )\n",
    "\n",
    "        ctrness_targets_sum = ctrness_targets.sum()\n",
    "        loss_denorm = max(reduce_sum(ctrness_targets_sum).item() / num_gpus, 1e-6)\n",
    "        instances.gt_ctrs = ctrness_targets\n",
    "\n",
    "        iou_targets = compute_iou_targets(reg_pred.detach(), instances.reg_targets)\n",
    "\n",
    "        if pos_inds.numel() > 0:\n",
    "            # cetnerness loss\n",
    "            ctrness_loss = (\n",
    "                F.binary_cross_entropy_with_logits(\n",
    "                    instances.ctrness_pred, ctrness_targets, reduction=\"sum\"\n",
    "                )\n",
    "                / num_pos_avg\n",
    "            )\n",
    "\n",
    "            # regression loss\n",
    "            if self.kl_loss:\n",
    "                reg_pred_std = instances.reg_pred_std\n",
    "\n",
    "                if self.kl_loss_type == \"klloss\":\n",
    "                    kl_loss = self.kl_loss_weight * self.kl_loc_loss_func(\n",
    "                        reg_pred,\n",
    "                        reg_pred_std,\n",
    "                        instances.reg_targets,\n",
    "                        loss_denorm=loss_denorm,\n",
    "                        weight=ctrness_targets,\n",
    "                        iou_weight=iou_targets,\n",
    "                        method=self.loc_fun_all,\n",
    "                    )\n",
    "\n",
    "                    iou_loss = (\n",
    "                        self.loc_loss_func(\n",
    "                            reg_pred, instances.reg_targets, ctrness_targets\n",
    "                        )\n",
    "                        / loss_denorm\n",
    "                    )\n",
    "                    reg_loss = self.kl_loss_weight * kl_loss + iou_loss\n",
    "\n",
    "                elif self.kl_loss_type == \"nlloss\":\n",
    "                    nlloss = self.kl_loss_weight * self.kl_loc_loss_func(\n",
    "                        reg_pred,\n",
    "                        reg_pred_std,\n",
    "                        instances.reg_targets,\n",
    "                        loss_denorm=loss_denorm,\n",
    "                        weight=ctrness_targets,\n",
    "                        iou_weight=iou_targets,\n",
    "                        method=self.loc_fun_all,\n",
    "                    )\n",
    "\n",
    "                    iou_loss = (\n",
    "                        self.loc_loss_func(\n",
    "                            reg_pred, instances.reg_targets, ctrness_targets\n",
    "                        )\n",
    "                        / loss_denorm\n",
    "                    )\n",
    "                    reg_loss = self.kl_loss_weight * nlloss + iou_loss\n",
    "\n",
    "            else:\n",
    "                # IoU loss\n",
    "                reg_loss = (\n",
    "                    self.loc_loss_func(reg_pred, instances.reg_targets, ctrness_targets)\n",
    "                    / loss_denorm\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            reg_loss = torch.tensor(0).cuda()\n",
    "            ctrness_loss = torch.tensor(0).cuda()\n",
    "            loss_denorm = 1.0\n",
    "\n",
    "        if instances.keep_locations.sum() == 0:\n",
    "            class_loss = class_loss * 0\n",
    "            reg_loss = reg_loss * 0\n",
    "            ctrness_loss = ctrness_loss * 0\n",
    "            loss_denorm = 1.0\n",
    "\n",
    "        losses_all = {\n",
    "            \"loss_fcos_cls\": class_loss,\n",
    "            \"loss_fcos_loc\": reg_loss,\n",
    "            \"loss_fcos_ctr\": ctrness_loss,\n",
    "        }\n",
    "\n",
    "        losses.update(losses_all)\n",
    "        extras = {\"instances\": instances, \"loss_denorm\": loss_denorm}\n",
    "        return extras, losses\n",
    "\n",
    "    # unsupervised loss branch\n",
    "    def pseudo_losses(\n",
    "        self,\n",
    "        logits_pred,\n",
    "        reg_pred,\n",
    "        ctrness_pred,\n",
    "        locations,\n",
    "        gt_instances,\n",
    "        reg_pred_std=None,\n",
    "        top_feats=None,\n",
    "        ignore_near=False,\n",
    "        branch=\"\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Return the losses from a set of GFocal predictions and their associated ground-truth.\n",
    "\n",
    "        Returns:\n",
    "            dict[loss name -> loss value]: A dict mapping from loss name to loss value.\n",
    "        \"\"\"\n",
    "        assert branch == \"unlabeled\"\n",
    "\n",
    "        extras = {}\n",
    "        losses = {}\n",
    "\n",
    "        # cls pseudo-labels --> cls and centerness, reg pseudo-labels --> reg\n",
    "        return_list = {\"cls\": [\"cls\", \"ctr\"], \"reg\": [\"reg\"]}\n",
    "        for labeltype in gt_instances.keys():\n",
    "            training_target = self._get_ground_truth(\n",
    "                locations, gt_instances[labeltype], ignore_near\n",
    "            )\n",
    "            instances = self.prepare_instance(\n",
    "                training_targets=training_target,\n",
    "                logits_pred=logits_pred,\n",
    "                reg_pred=reg_pred,\n",
    "                ctrness_pred=ctrness_pred,\n",
    "                reg_pred_std=reg_pred_std,\n",
    "                top_feats=top_feats,\n",
    "            )\n",
    "            extras_each, losses_each = self.fcos_pseudo_losses(\n",
    "                instances, return_loss=return_list[labeltype], labeltype=labeltype\n",
    "            )\n",
    "            extras.update(extras_each)\n",
    "            losses.update(losses_each)\n",
    "\n",
    "        return extras, losses\n",
    "\n",
    "    def fcos_pseudo_losses(self, instances, return_loss, labeltype=\"\"):\n",
    "\n",
    "        return_instances = instances\n",
    "        losses = {}\n",
    "\n",
    "        # compute pos_inds and num_pos_avg\n",
    "        num_classes = instances.logits_pred.size(1)\n",
    "        assert num_classes == self.num_classes\n",
    "        labels = instances.labels.flatten()\n",
    "        pos_inds = torch.nonzero(labels != num_classes).squeeze(1)\n",
    "        num_pos_local = pos_inds.numel()\n",
    "        num_gpus = get_world_size()\n",
    "        total_num_pos = reduce_sum(pos_inds.new_tensor([num_pos_local])).item()\n",
    "        num_pos_avg = max(total_num_pos / num_gpus, 1.0)\n",
    "\n",
    "        # classification loss\n",
    "        if \"cls\" in return_loss:\n",
    "            class_loss = self.class_loss(\n",
    "                instances, pos_inds, labels, num_classes, num_pos_avg\n",
    "            )\n",
    "            losses.update(class_loss)\n",
    "\n",
    "        # only compute the centerness loss and regression loss for the foreground classes\n",
    "        instances = instances[pos_inds]\n",
    "        instances.pos_inds = pos_inds\n",
    "\n",
    "        # prepare centerness ground-truth labels\n",
    "        ctrness_targets = compute_ctrness_targets(instances.reg_targets)\n",
    "        ctrness_targets_sum = ctrness_targets.sum()\n",
    "        loss_denorm = max(reduce_sum(ctrness_targets_sum).item() / num_gpus, 1e-6)\n",
    "        instances.gt_ctrs = ctrness_targets\n",
    "\n",
    "        if pos_inds.numel() > 0:\n",
    "            # centerness loss\n",
    "            if \"ctr\" in return_loss:\n",
    "                ctrness_loss = (\n",
    "                    F.binary_cross_entropy_with_logits(\n",
    "                        instances.ctrness_pred, ctrness_targets, reduction=\"sum\"\n",
    "                    )\n",
    "                    / num_pos_avg\n",
    "                )\n",
    "                if self.unify_ctrcls:\n",
    "                    ctrness_loss = ctrness_loss * 0\n",
    "                losses[\"loss_fcos_ctr\"] = ctrness_loss\n",
    "\n",
    "            if \"reg\" in return_loss:\n",
    "                # process regressiion prediction\n",
    "                if self.reg_discrete and pos_inds.numel() > 0:\n",
    "                    # offset bin classification, we find this slightly improves unsupervised loss\n",
    "                    pred_ltrb_discrete = instances.reg_pred\n",
    "                    pred_ltrb_scalar = self.integral(pred_ltrb_discrete)\n",
    "                    reg_pred = pred_ltrb_scalar\n",
    "                else:\n",
    "                    # continous output\n",
    "                    reg_pred = instances.reg_pred\n",
    "\n",
    "                # regression loss\n",
    "                if self.kl_loss:  # kl loss\n",
    "                    assert instances.has(\"reg_pred_std\")\n",
    "                    reg_pred_std = instances.reg_pred_std\n",
    "                    if self.reg_unsup_loss == \"ts_locvar_better_nms_nll_l1\":\n",
    "                        loc_conf_student = 1 - instances.reg_pred_std.sigmoid()\n",
    "                        loc_conf_teacher = 1 - instances.boundary_vars.sigmoid()\n",
    "                        select = (loc_conf_teacher > self.tsbetter_reg_cert) * (\n",
    "                            loc_conf_teacher > loc_conf_student + self.tsbetter_reg\n",
    "                        )\n",
    "\n",
    "                        losses[\"teacher_better_student\"] = select.sum()\n",
    "\n",
    "                        reg_student = reg_pred\n",
    "                        reg_teacher = instances.reg_targets\n",
    "\n",
    "                        if select.sum() > 0:\n",
    "                            reg_loss = F.smooth_l1_loss(\n",
    "                                reg_student[select], reg_teacher[select], beta=0.0\n",
    "                            )\n",
    "                        else:\n",
    "                            reg_loss = torch.tensor(0).cuda()\n",
    "\n",
    "                    else:\n",
    "                        iou_targets = compute_iou_targets(\n",
    "                            reg_pred.detach(), instances.reg_targets\n",
    "                        )\n",
    "\n",
    "                        reg_loss = self.kl_loss_weight * self.kl_loc_loss_func(\n",
    "                            reg_pred,\n",
    "                            reg_pred_std,\n",
    "                            instances.reg_targets,\n",
    "                            loss_denorm=loss_denorm,\n",
    "                            weight=ctrness_targets,\n",
    "                            iou_weight=iou_targets,\n",
    "                            method=self.loc_fun_all,\n",
    "                        )\n",
    "\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "\n",
    "                losses[\"loss_fcos_loc\"] = reg_loss\n",
    "\n",
    "        else:\n",
    "            if \"ctr\" in return_loss:\n",
    "                losses[\"loss_fcos_ctr\"] = torch.tensor(0).cuda()\n",
    "\n",
    "            loss_denorm = 1.0\n",
    "\n",
    "            if \"reg\" in return_loss:\n",
    "                losses[\"loss_fcos_loc\"] = torch.tensor(0).cuda()\n",
    "                losses[\"teacher_better_student\"] = torch.tensor(0).cuda()\n",
    "\n",
    "        # final check for multiple gpu running\n",
    "        extras = {\n",
    "            \"instances_\" + labeltype: return_instances,\n",
    "            \"loss_denorm\": loss_denorm,\n",
    "        }\n",
    "        return extras, losses\n",
    "\n",
    "    # classification loss (for unsupervised branch)\n",
    "    def class_loss(self, instances, pos_inds, labels, num_classes, num_pos_avg):\n",
    "\n",
    "        losses = {}\n",
    "        class_target = torch.zeros_like(instances.logits_pred)\n",
    "        class_target[pos_inds, labels[pos_inds]] = 1\n",
    "\n",
    "        ### Classification loss (unifying branch or individual branch)\n",
    "        # unifying centerness and classification\n",
    "        # we find this leads to worse results\n",
    "        class_loss_all = sigmoid_focal_loss_jit(\n",
    "            instances.logits_pred,\n",
    "            class_target,\n",
    "            alpha=self.focal_loss_alpha,\n",
    "            gamma=self.focal_loss_gamma,\n",
    "            reduction=\"none\",\n",
    "        )\n",
    "\n",
    "        weighted_class_loss = class_loss_all.sum(1)\n",
    "        class_loss = weighted_class_loss.sum() / num_pos_avg\n",
    "        losses[\"loss_fcos_cls\"] = class_loss\n",
    "\n",
    "        return losses\n",
    "\n",
    "    # other functions\n",
    "    def _transpose(self, training_targets, num_loc_list):\n",
    "        \"\"\"\n",
    "        This function is used to transpose image first training targets to level first ones\n",
    "        :return: level first training targets\n",
    "        \"\"\"\n",
    "        for im_i in range(len(training_targets)):\n",
    "            training_targets[im_i] = torch.split(\n",
    "                training_targets[im_i], num_loc_list, dim=0\n",
    "            )\n",
    "\n",
    "        targets_level_first = []\n",
    "        for targets_per_level in zip(*training_targets):\n",
    "            targets_level_first.append(torch.cat(targets_per_level, dim=0))\n",
    "        return targets_level_first\n",
    "\n",
    "    def _get_ground_truth(self, locations, gt_instances, ignore_near=False):\n",
    "        num_loc_list = [len(loc) for loc in locations]\n",
    "        # compute locations to size ranges\n",
    "        loc_to_size_range = []\n",
    "        for lo, loc_per_level in enumerate(locations):\n",
    "            loc_to_size_range_per_level = loc_per_level.new_tensor(\n",
    "                self.sizes_of_interest[lo]\n",
    "            )\n",
    "            loc_to_size_range.append(\n",
    "                loc_to_size_range_per_level[None].expand(num_loc_list[lo], -1)\n",
    "            )\n",
    "            # [prev_layer_size, this layer_size ]\n",
    "            # [[-1,64], .... ,[64,128],...,[128,256], ...,[256,512],... [512,100000]]\n",
    "\n",
    "        loc_to_size_range = torch.cat(\n",
    "            loc_to_size_range, dim=0\n",
    "        )  # size [L1+L2+...+L5, 2]\n",
    "        locations = torch.cat(locations, dim=0)  # size [L1+L2+...+L5, 2]\n",
    "\n",
    "        # compute the reg, label target for each element\n",
    "        training_targets = self.compute_targets_for_locations(\n",
    "            locations, gt_instances, loc_to_size_range, num_loc_list, ignore_near\n",
    "        )\n",
    "\n",
    "        training_targets[\"locations\"] = [\n",
    "            locations.clone() for _ in range(len(gt_instances))\n",
    "        ]\n",
    "        training_targets[\"im_inds\"] = [\n",
    "            locations.new_ones(locations.size(0), dtype=torch.long) * i\n",
    "            for i in range(len(gt_instances))\n",
    "        ]\n",
    "\n",
    "        # transpose im first training_targets to level first ones\n",
    "        training_targets = {\n",
    "            k: self._transpose(v, num_loc_list) for k, v in training_targets.items()\n",
    "        }\n",
    "\n",
    "        training_targets[\"fpn_levels\"] = [\n",
    "            loc.new_ones(len(loc), dtype=torch.long) * level\n",
    "            for level, loc in enumerate(training_targets[\"locations\"])\n",
    "        ]\n",
    "\n",
    "        # we normalize reg_targets by FPN's strides here\n",
    "        # reg_targets is normalized for each level!\n",
    "        #  this is ltrb format\n",
    "        reg_targets = training_targets[\"reg_targets\"]\n",
    "        for la in range(len(reg_targets)):\n",
    "            reg_targets[la] = reg_targets[la] / float(self.strides[la])\n",
    "\n",
    "        return training_targets\n",
    "\n",
    "    def get_sample_region(\n",
    "        self, boxes, strides, num_loc_list, loc_xs, loc_ys, bitmasks=None, radius=1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        boxes: size:[# of GT boxes, 4(x1,y1,x2,y2)]\n",
    "        strides: [8,16,32,64,128]\n",
    "        num_loc_list: [15200, 3800, 950 ,247, 70]\n",
    "        loc_xs: size[20267]\n",
    "        loc_ys: size[20267]\n",
    "        bitmasks:\n",
    "        radius\n",
    "        \"\"\"\n",
    "\n",
    "        if bitmasks is not None:\n",
    "            _, h, w = bitmasks.size()\n",
    "\n",
    "            ys = torch.arange(0, h, dtype=torch.float32, device=bitmasks.device)\n",
    "            xs = torch.arange(0, w, dtype=torch.float32, device=bitmasks.device)\n",
    "\n",
    "            m00 = bitmasks.sum(dim=-1).sum(dim=-1).clamp(min=1e-6)\n",
    "            m10 = (bitmasks * xs).sum(dim=-1).sum(dim=-1)\n",
    "            m01 = (bitmasks * ys[:, None]).sum(dim=-1).sum(dim=-1)\n",
    "            center_x = m10 / m00\n",
    "            center_y = m01 / m00\n",
    "        else:\n",
    "            # gt box center Size[number of bbox]\n",
    "            center_x = boxes[..., [0, 2]].sum(dim=-1) * 0.5\n",
    "            center_y = boxes[..., [1, 3]].sum(dim=-1) * 0.5\n",
    "\n",
    "        num_gts = boxes.shape[0]\n",
    "        K = len(loc_xs)\n",
    "        boxes = boxes[None].expand(K, num_gts, 4)\n",
    "        center_x = center_x[None].expand(K, num_gts)\n",
    "        center_y = center_y[None].expand(K, num_gts)\n",
    "        center_gt = boxes.new_zeros(boxes.shape)\n",
    "        # no gt\n",
    "        if center_x.numel() == 0 or center_x[..., 0].sum() == 0:\n",
    "            return loc_xs.new_zeros(loc_xs.shape, dtype=torch.uint8)\n",
    "\n",
    "        # compute the bbox region (it is shrinked into the region near the center point)\n",
    "        # ! Center point region is not object-size variant\n",
    "        beg = 0\n",
    "        for level, num_loc in enumerate(num_loc_list):\n",
    "            end = beg + num_loc\n",
    "            stride = strides[level] * radius\n",
    "            xmin = center_x[beg:end] - stride  # center x shift\n",
    "            ymin = center_y[beg:end] - stride\n",
    "            xmax = center_x[beg:end] + stride\n",
    "            ymax = center_y[beg:end] + stride\n",
    "            # limit sample region in gt: gt only has 1.5 pixel away from the center\n",
    "            center_gt[beg:end, :, 0] = torch.where(\n",
    "                xmin > boxes[beg:end, :, 0], xmin, boxes[beg:end, :, 0]\n",
    "            )\n",
    "            center_gt[beg:end, :, 1] = torch.where(\n",
    "                ymin > boxes[beg:end, :, 1], ymin, boxes[beg:end, :, 1]\n",
    "            )\n",
    "            center_gt[beg:end, :, 2] = torch.where(\n",
    "                xmax > boxes[beg:end, :, 2], boxes[beg:end, :, 2], xmax\n",
    "            )\n",
    "            center_gt[beg:end, :, 3] = torch.where(\n",
    "                ymax > boxes[beg:end, :, 3], boxes[beg:end, :, 3], ymax\n",
    "            )\n",
    "            beg = end\n",
    "        #\n",
    "        left = loc_xs[:, None] - center_gt[..., 0]\n",
    "        right = center_gt[..., 2] - loc_xs[:, None]\n",
    "        top = loc_ys[:, None] - center_gt[..., 1]\n",
    "        bottom = center_gt[..., 3] - loc_ys[:, None]\n",
    "        center_bbox = torch.stack((left, top, right, bottom), -1)\n",
    "        inside_gt_bbox_mask = center_bbox.min(-1)[0] > 0\n",
    "        return inside_gt_bbox_mask\n",
    "\n",
    "    def compute_targets_for_locations(\n",
    "        self, locations, targets, size_ranges, num_loc_list, ignore_near=False\n",
    "    ):\n",
    "\n",
    "        labels = []\n",
    "        reg_targets = []\n",
    "        target_inds = []\n",
    "        keep_locations = []\n",
    "        box_weights = []\n",
    "        boundary_vars = []\n",
    "\n",
    "        xs, ys = locations[:, 0], locations[:, 1]\n",
    "\n",
    "        num_targets = 0\n",
    "        for im_i in range(len(targets)):  # image-wise operation\n",
    "            targets_per_im = targets[im_i]\n",
    "            bboxes = targets_per_im.gt_boxes.tensor\n",
    "            labels_per_im = targets_per_im.gt_classes\n",
    "\n",
    "            # box weight weights\n",
    "            if targets_per_im.has(\"scores\") and self.cls_loss_weight:\n",
    "                box_weights_per_im = targets_per_im.scores\n",
    "            else:\n",
    "                box_weights_per_im = torch.ones_like(targets_per_im.gt_classes)\n",
    "\n",
    "            # box weight weights\n",
    "            if targets_per_im.has(\"reg_pred_std\"):\n",
    "                boundary_var_per_im = targets_per_im.reg_pred_std\n",
    "            else:\n",
    "                boundary_var_per_im = torch.zeros_like(targets_per_im.gt_boxes.tensor)\n",
    "\n",
    "            # no gt\n",
    "            if bboxes.numel() == 0:\n",
    "                # no bboxes then all labels are background\n",
    "                labels.append(\n",
    "                    labels_per_im.new_zeros(locations.size(0)) + self.num_classes\n",
    "                )\n",
    "                # no bboxes then all boxes weights are zeros\n",
    "                box_weights.append(box_weights_per_im.new_zeros(locations.size(0)))\n",
    "                reg_targets.append(locations.new_zeros((locations.size(0), 4)))\n",
    "                boundary_vars.append(locations.new_zeros((locations.size(0), 4)))\n",
    "                target_inds.append(labels_per_im.new_zeros(locations.size(0)) - 1)\n",
    "                keep_locations.append(torch.zeros(xs.shape[0]).to(bool).cuda())\n",
    "                continue\n",
    "            area = targets_per_im.gt_boxes.area()\n",
    "\n",
    "            l = xs[:, None] - bboxes[:, 0][None]\n",
    "            t = ys[:, None] - bboxes[:, 1][None]\n",
    "            r = bboxes[:, 2][None] - xs[:, None]\n",
    "            b = bboxes[:, 3][None] - ys[:, None]\n",
    "            reg_targets_per_im = torch.stack([l, t, r, b], dim=2)\n",
    "\n",
    "            if self.center_sample:\n",
    "                if targets_per_im.has(\"gt_bitmasks_full\"):\n",
    "                    bitmasks = targets_per_im.gt_bitmasks_full\n",
    "                else:\n",
    "                    bitmasks = None\n",
    "                is_in_boxes = self.get_sample_region(\n",
    "                    bboxes,\n",
    "                    self.strides,\n",
    "                    num_loc_list,\n",
    "                    xs,\n",
    "                    ys,\n",
    "                    bitmasks=bitmasks,\n",
    "                    radius=self.radius,\n",
    "                )\n",
    "            else:\n",
    "                is_in_boxes = reg_targets_per_im.min(dim=2)[0] > 0\n",
    "\n",
    "            if ignore_near:\n",
    "                # ignore all pixels inside the boxes\n",
    "                is_ignore = reg_targets_per_im.min(dim=2)[0] > 0\n",
    "                keep_location_bg = ~(is_ignore.sum(1) > 0)\n",
    "\n",
    "                # keep all pixel inside the box\n",
    "                keep_location_fg = is_in_boxes.sum(1) > 0\n",
    "                keep_location = keep_location_bg + keep_location_fg\n",
    "            else:\n",
    "                # keep all\n",
    "                keep_location = torch.ones(is_in_boxes.shape[0]).to(bool).cuda()\n",
    "\n",
    "            # filter out these box is too small or too big for each scale\n",
    "            max_reg_targets_per_im = reg_targets_per_im.max(dim=2)[0]\n",
    "            # limit the regression range for each location\n",
    "            is_cared_in_the_level = (max_reg_targets_per_im >= size_ranges[:, [0]]) & (\n",
    "                max_reg_targets_per_im <= size_ranges[:, [1]]\n",
    "            )\n",
    "\n",
    "            # compute the area for each gt box\n",
    "            locations_to_gt_area = area[None].repeat(len(locations), 1)\n",
    "            # set points (outside box/small region) as background\n",
    "            locations_to_gt_area[is_in_boxes == 0] = INF\n",
    "            # set points with too large displacement or too small displacement as background\n",
    "            locations_to_gt_area[is_cared_in_the_level == 0] = INF\n",
    "\n",
    "            # if there are still more than one objects for a location,\n",
    "            # we choose the one with minimal area\n",
    "            locations_to_min_area, locations_to_gt_inds = locations_to_gt_area.min(\n",
    "                dim=1\n",
    "            )\n",
    "\n",
    "            # use the minial area as creteria to choose ground-truth boxes of regression for each point\n",
    "            reg_targets_per_im = reg_targets_per_im[\n",
    "                range(len(locations)), locations_to_gt_inds\n",
    "            ]\n",
    "\n",
    "            # regard object in different image as different instance\n",
    "            target_inds_per_im = locations_to_gt_inds + num_targets\n",
    "            num_targets += len(targets_per_im)\n",
    "\n",
    "            labels_per_im = labels_per_im[locations_to_gt_inds]\n",
    "            labels_per_im[locations_to_min_area == INF] = self.num_classes\n",
    "\n",
    "            # TODO: background weight is 1.0 for now; we could try to use average score as background weights\n",
    "            box_weights_per_im = box_weights_per_im[locations_to_gt_inds]\n",
    "            box_weights_per_im[locations_to_min_area == INF] = 1.0\n",
    "\n",
    "            boundary_var_per_im = boundary_var_per_im[locations_to_gt_inds]\n",
    "            boundary_var_per_im[locations_to_min_area == INF] = 99999.0\n",
    "\n",
    "            labels.append(labels_per_im)\n",
    "            box_weights.append(box_weights_per_im)\n",
    "            reg_targets.append(reg_targets_per_im)\n",
    "            target_inds.append(target_inds_per_im)\n",
    "            keep_locations.append(keep_location)\n",
    "            boundary_vars.append(boundary_var_per_im)\n",
    "\n",
    "        return {\n",
    "            \"labels\": labels,\n",
    "            \"box_weights\": box_weights,\n",
    "            \"reg_targets\": reg_targets,\n",
    "            \"target_inds\": target_inds,\n",
    "            \"keep_locations\": keep_locations,\n",
    "            \"boundary_vars\": boundary_vars,\n",
    "        }\n",
    "\n",
    "    def prepare_instance(\n",
    "        self,\n",
    "        training_targets,\n",
    "        logits_pred,\n",
    "        reg_pred,\n",
    "        ctrness_pred,\n",
    "        reg_pred_std=None,\n",
    "        top_feats=None,\n",
    "    ):\n",
    "        # Collect all logits and regression predictions over feature maps\n",
    "        # and images to arrive at the same shape as the labels and targets\n",
    "        # The final ordering is L, N, H, W from slowest to fastest axis.\n",
    "        instances = Instances((0, 0))\n",
    "        instances.labels = cat(\n",
    "            [\n",
    "                # Reshape: (N, 1, Hi, Wi) -> (N*Hi*Wi,)\n",
    "                x.reshape(-1)\n",
    "                for x in training_targets[\"labels\"]\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # add soft weight for each labels\n",
    "        instances.box_weights = cat(\n",
    "            [\n",
    "                # Reshape: (N, 1, Hi, Wi) -> (N*Hi*Wi,)\n",
    "                x.reshape(-1)\n",
    "                for x in training_targets[\"box_weights\"]\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        # ignore some samples during training\n",
    "        instances.keep_locations = cat(\n",
    "            [\n",
    "                # Reshape: (N, 1, Hi, Wi) -> (N*Hi*Wi,)\n",
    "                x.reshape(-1)\n",
    "                for x in training_targets[\"keep_locations\"]\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        instances.gt_inds = cat(\n",
    "            [\n",
    "                # Reshape: (N, 1, Hi, Wi) -> (N*Hi*Wi,)\n",
    "                x.reshape(-1)\n",
    "                for x in training_targets[\"target_inds\"]\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        instances.im_inds = cat(\n",
    "            [x.reshape(-1) for x in training_targets[\"im_inds\"]], dim=0\n",
    "        )\n",
    "        instances.reg_targets = cat(\n",
    "            [\n",
    "                # Reshape: (N, Hi, Wi, 4) -> (N*Hi*Wi, 4)\n",
    "                x.reshape(-1, 4)\n",
    "                for x in training_targets[\"reg_targets\"]\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "        instances.locations = cat(\n",
    "            [x.reshape(-1, 2) for x in training_targets[\"locations\"]], dim=0\n",
    "        )\n",
    "        instances.fpn_levels = cat(\n",
    "            [x.reshape(-1) for x in training_targets[\"fpn_levels\"]], dim=0\n",
    "        )\n",
    "\n",
    "        if \"boundary_vars\" in training_targets:\n",
    "            instances.boundary_vars = cat(\n",
    "                [\n",
    "                    # Reshape: (N, Hi, Wi, 4) -> (N*Hi*Wi, 4)\n",
    "                    x.reshape(-1, 4)\n",
    "                    for x in training_targets[\"boundary_vars\"]\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        instances.logits_pred = cat(\n",
    "            [\n",
    "                # Reshape: (N, C, Hi, Wi) -> (N, Hi, Wi, C) -> (N*Hi*Wi, C)\n",
    "                x.permute(0, 2, 3, 1).reshape(-1, self.num_classes)\n",
    "                for x in logits_pred\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        if self.reg_discrete:\n",
    "            instances.reg_pred = cat(\n",
    "                [\n",
    "                    # Reshape: (N, B, Hi, Wi) -> (N, Hi, Wi, B) -> (N*Hi*Wi, B)\n",
    "                    x.permute(0, 2, 3, 1).reshape(-1, 4 * (self.reg_max + 1))\n",
    "                    for x in reg_pred\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "        else:\n",
    "            instances.reg_pred = cat(\n",
    "                [\n",
    "                    # Reshape: (N, B, Hi, Wi) -> (N, Hi, Wi, B) -> (N*Hi*Wi, B)\n",
    "                    x.permute(0, 2, 3, 1).reshape(-1, 4)\n",
    "                    for x in reg_pred\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        if self.kl_loss:\n",
    "            assert reg_pred_std is not None\n",
    "            instances.reg_pred_std = cat(\n",
    "                [\n",
    "                    # Reshape: (N, B, Hi, Wi) -> (N, Hi, Wi, B) -> (N*Hi*Wi, B)\n",
    "                    x.permute(0, 2, 3, 1).reshape(-1, 4)\n",
    "                    for x in reg_pred_std\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        instances.ctrness_pred = cat(\n",
    "            [\n",
    "                # Reshape: (N, 1, Hi, Wi) -> (N*Hi*Wi,)\n",
    "                x.permute(0, 2, 3, 1).reshape(-1)\n",
    "                for x in ctrness_pred\n",
    "            ],\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        if len(top_feats) > 0:\n",
    "            instances.top_feats = cat(\n",
    "                [\n",
    "                    # Reshape: (N, -1, Hi, Wi) -> (N*Hi*Wi, -1)\n",
    "                    x.permute(0, 2, 3, 1).reshape(-1, x.size(1))\n",
    "                    for x in top_feats\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "\n",
    "        return instances\n",
    "\n",
    "    def predict_proposals(\n",
    "        self,\n",
    "        logits_pred,\n",
    "        reg_pred,\n",
    "        ctrness_pred,\n",
    "        locations,\n",
    "        image_sizes,\n",
    "        reg_pred_std=None,\n",
    "        top_feats=None,\n",
    "        nms_method=\"cls_n_ctr\",\n",
    "    ):\n",
    "\n",
    "        if self.training:\n",
    "            self.pre_nms_thresh = self.pre_nms_thresh_train\n",
    "            self.pre_nms_topk = self.pre_nms_topk_train\n",
    "            self.post_nms_topk = self.post_nms_topk_train\n",
    "        else:\n",
    "            self.pre_nms_thresh = self.pre_nms_thresh_test\n",
    "            self.pre_nms_topk = self.pre_nms_topk_test\n",
    "            self.post_nms_topk = self.post_nms_topk_test\n",
    "\n",
    "        sampled_boxes = []\n",
    "\n",
    "        bundle = {\n",
    "            \"l\": locations,\n",
    "            \"o\": logits_pred,\n",
    "            \"r\": reg_pred,\n",
    "            \"c\": ctrness_pred,\n",
    "            \"s\": self.strides,\n",
    "        }\n",
    "\n",
    "        if len(top_feats) > 0:\n",
    "            bundle[\"t\"] = top_feats\n",
    "\n",
    "        if reg_pred_std is not None:\n",
    "            bundle[\"r_std\"] = reg_pred_std\n",
    "\n",
    "        # each iteration = 1 scale\n",
    "        for i, per_bundle in enumerate(zip(*bundle.values())):\n",
    "            # get per-level bundle\n",
    "            per_bundle = dict(zip(bundle.keys(), per_bundle))\n",
    "            # recall that during training, we normalize regression targets with FPN's stride.\n",
    "            # we denormalize them here.\n",
    "\n",
    "            l = per_bundle[\"l\"]\n",
    "            o = per_bundle[\"o\"]\n",
    "\n",
    "            if self.reg_discrete:  # discrete to scalar\n",
    "                bs = per_bundle[\"r\"].shape[0]\n",
    "                imgw = per_bundle[\"r\"].shape[2]\n",
    "                imgh = per_bundle[\"r\"].shape[3]\n",
    "                reg_discre_raw = (\n",
    "                    per_bundle[\"r\"]\n",
    "                    .permute(0, 2, 3, 1)\n",
    "                    .reshape(-1, 4 * (self.reg_max + 1))\n",
    "                )\n",
    "                scalar_r = self.integral(reg_discre_raw).reshape(bs, imgw, imgh, 4)\n",
    "                scalar_r = scalar_r.permute(0, 3, 1, 2)\n",
    "                r = scalar_r * per_bundle[\"s\"]\n",
    "\n",
    "                r_cls = (per_bundle[\"r\"], per_bundle[\"s\"])\n",
    "            else:\n",
    "                r = per_bundle[\"r\"] * per_bundle[\"s\"]\n",
    "                r_cls = None\n",
    "\n",
    "            c = per_bundle[\"c\"]\n",
    "            t = per_bundle[\"t\"] if \"t\" in bundle else None\n",
    "\n",
    "            r_std = per_bundle[\"r_std\"] if \"r_std\" in bundle else None\n",
    "\n",
    "            sampled_boxes.append(\n",
    "                self.forward_for_single_feature_map(\n",
    "                    l, o, r, r_cls, c, image_sizes, r_std, t, nms_method\n",
    "                )\n",
    "            )\n",
    "\n",
    "            for per_im_sampled_boxes in sampled_boxes[-1]:\n",
    "                per_im_sampled_boxes.fpn_levels = (\n",
    "                    l.new_ones(len(per_im_sampled_boxes), dtype=torch.long) * i\n",
    "                )\n",
    "\n",
    "        # nms\n",
    "        boxlists = list(zip(*sampled_boxes))\n",
    "        boxlists = [Instances.cat(boxlist) for boxlist in boxlists]\n",
    "        boxlists = self.select_over_all_levels(boxlists)\n",
    "\n",
    "        return boxlists\n",
    "\n",
    "    def forward_for_single_feature_map(\n",
    "        self,\n",
    "        locations,\n",
    "        logits_pred,\n",
    "        reg_pred,\n",
    "        reg_pred_cls,\n",
    "        ctrness_pred,\n",
    "        image_sizes,\n",
    "        reg_pred_std=None,\n",
    "        top_feat=None,\n",
    "        nms_method=\"cls_n_ctr\",\n",
    "    ):\n",
    "        N, C, H, W = logits_pred.shape\n",
    "        # put in the same format as locations\n",
    "        logits_pred = logits_pred.view(N, C, H, W).permute(0, 2, 3, 1)\n",
    "        logits_pred = logits_pred.reshape(N, -1, C).sigmoid()\n",
    "        box_regression = reg_pred.view(N, 4, H, W).permute(0, 2, 3, 1)\n",
    "        box_regression = box_regression.reshape(N, -1, 4)\n",
    "        ctrness_pred = ctrness_pred.view(N, 1, H, W).permute(0, 2, 3, 1)\n",
    "        ctrness_pred = ctrness_pred.reshape(N, -1).sigmoid()\n",
    "\n",
    "        if reg_pred_cls is not None:\n",
    "            box_reg_cls = (\n",
    "                reg_pred_cls[0]\n",
    "                .view(N, 4 * (self.reg_max + 1), H, W)\n",
    "                .permute(0, 2, 3, 1)\n",
    "            )\n",
    "            box_reg_cls = box_reg_cls.reshape(N, -1, 4 * (self.reg_max + 1))\n",
    "            scalar = reg_pred_cls[1]\n",
    "\n",
    "        if top_feat is not None:\n",
    "            top_feat = top_feat.view(N, -1, H, W).permute(0, 2, 3, 1)\n",
    "            top_feat = top_feat.reshape(N, H * W, -1)\n",
    "\n",
    "        if reg_pred_std is not None:\n",
    "            box_regression_std = reg_pred_std.view(N, 4, H, W).permute(0, 2, 3, 1)\n",
    "            box_regression_std = box_regression_std.reshape(N, -1, 4)\n",
    "\n",
    "        # if self.thresh_with_ctr is True, we multiply the classification\n",
    "        # scores with centerness scores before applying the threshold.\n",
    "        if self.thresh_with_ctr:\n",
    "            logits_pred = logits_pred * ctrness_pred[:, :, None]\n",
    "        candidate_inds = logits_pred > self.pre_nms_thresh\n",
    "        pre_nms_top_n = candidate_inds.reshape(N, -1).sum(1)\n",
    "        pre_nms_top_n = pre_nms_top_n.clamp(max=self.pre_nms_topk)\n",
    "        cls_confs = logits_pred\n",
    "\n",
    "        if not self.thresh_with_ctr:\n",
    "            if nms_method == \"cls_n_ctr\":\n",
    "                logits_pred = logits_pred * ctrness_pred[:, :, None]\n",
    "            elif nms_method == \"cls\":\n",
    "                logits_pred = logits_pred\n",
    "            elif nms_method == \"ctr\":\n",
    "                logits_pred = ctrness_pred[:, :, None]\n",
    "\n",
    "            elif nms_method == \"cls_n_loc\":\n",
    "                assert box_regression_std is not None\n",
    "                boundary_regression_std = 1 - box_regression_std.sigmoid()\n",
    "                box_reg_std = boundary_regression_std.mean(2)\n",
    "                logits_pred = logits_pred * box_reg_std[:, :, None]\n",
    "            else:  # default cls + ctr\n",
    "                logits_pred = logits_pred * ctrness_pred[:, :, None]\n",
    "\n",
    "        results = []\n",
    "        for i in range(N):  # each image\n",
    "            # select pixels larger than threshold (0.05)\n",
    "            per_box_cls = logits_pred[i]\n",
    "            per_candidate_inds = candidate_inds[i]\n",
    "            per_box_cls = per_box_cls[per_candidate_inds]\n",
    "\n",
    "            # get the index of pixel and its class prediction\n",
    "            per_candidate_nonzeros = per_candidate_inds.nonzero()\n",
    "            per_box_loc = per_candidate_nonzeros[:, 0]\n",
    "            per_class = per_candidate_nonzeros[:, 1]\n",
    "\n",
    "            per_box_regression = box_regression[i]\n",
    "            per_box_regression = per_box_regression[per_box_loc]\n",
    "\n",
    "            # for bin classification\n",
    "            if reg_pred_cls is not None:\n",
    "                per_box_reg_cls = box_reg_cls[i]\n",
    "                per_box_reg_cls = per_box_reg_cls[per_box_loc]\n",
    "\n",
    "            # for localization std\n",
    "            if reg_pred_std is not None:\n",
    "                per_box_regression_std = box_regression_std[i]\n",
    "                per_box_regression_std = per_box_regression_std[per_box_loc]\n",
    "\n",
    "            per_locations = locations[per_box_loc]\n",
    "\n",
    "            # centerness\n",
    "            per_centerness = ctrness_pred[i]\n",
    "            per_centerness = per_centerness[per_box_loc]\n",
    "            per_cls_conf = cls_confs[i]\n",
    "            per_cls_conf = per_cls_conf[per_candidate_inds]\n",
    "\n",
    "            if top_feat is not None:\n",
    "                per_top_feat = top_feat[i]\n",
    "                per_top_feat = per_top_feat[per_box_loc]\n",
    "\n",
    "            # select top k\n",
    "            per_pre_nms_top_n = pre_nms_top_n[i]\n",
    "\n",
    "            # check whether per_candidate boxes is too many\n",
    "            if per_candidate_inds.sum().item() > per_pre_nms_top_n.item():\n",
    "                per_box_cls, top_k_indices = per_box_cls.topk(\n",
    "                    per_pre_nms_top_n, sorted=False\n",
    "                )\n",
    "                per_class = per_class[top_k_indices]\n",
    "                per_box_regression = per_box_regression[top_k_indices]\n",
    "\n",
    "                if reg_pred_cls is not None:\n",
    "                    per_box_reg_cls = per_box_reg_cls[top_k_indices]\n",
    "\n",
    "                if reg_pred_std is not None:\n",
    "                    per_box_regression_std = per_box_regression_std[top_k_indices]\n",
    "\n",
    "                per_locations = per_locations[top_k_indices]\n",
    "                per_centerness = per_centerness[top_k_indices]\n",
    "                per_cls_conf = per_cls_conf[top_k_indices]\n",
    "\n",
    "                if top_feat is not None:\n",
    "                    per_top_feat = per_top_feat[top_k_indices]\n",
    "\n",
    "            detections = torch.stack(\n",
    "                [\n",
    "                    per_locations[:, 0] - per_box_regression[:, 0],\n",
    "                    per_locations[:, 1] - per_box_regression[:, 1],\n",
    "                    per_locations[:, 0] + per_box_regression[:, 2],\n",
    "                    per_locations[:, 1] + per_box_regression[:, 3],\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "            boxlist = Instances(image_sizes[i])\n",
    "            boxlist.pred_boxes = Boxes(detections)\n",
    "            if nms_method == \"cls_n_ctr\" or nms_method == \"cls_n_loc\":\n",
    "                boxlist.scores = torch.sqrt(per_box_cls)\n",
    "            elif nms_method == \"cls\" or nms_method == \"ctr\":\n",
    "                boxlist.scores = per_box_cls\n",
    "            else:  # default cls + ctr\n",
    "                raise ValueError(\"Undefined nms criteria\")\n",
    "\n",
    "            if reg_pred_cls is not None:\n",
    "                boxlist.reg_pred_cls = per_box_reg_cls\n",
    "                boxlist.reg_pred_cls_scalar = (\n",
    "                    torch.ones(per_box_reg_cls.shape[0]).cuda() * scalar\n",
    "                )\n",
    "\n",
    "            if reg_pred_std is not None:\n",
    "                boxlist.reg_pred_std = per_box_regression_std\n",
    "\n",
    "            # boxlist.scores = torch.sqrt(per_box_cls)\n",
    "            # boxlist.scores = torch.sqrt(per_box_cls)\n",
    "\n",
    "            boxlist.pred_classes = per_class\n",
    "            boxlist.locations = per_locations\n",
    "            boxlist.centerness = per_centerness\n",
    "            boxlist.cls_confid = per_cls_conf\n",
    "\n",
    "            if top_feat is not None:\n",
    "                boxlist.top_feat = per_top_feat\n",
    "            results.append(boxlist)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def select_over_all_levels(self, boxlists):\n",
    "        num_images = len(boxlists)\n",
    "        results = []\n",
    "        for i in range(num_images):\n",
    "            # multiclass nms\n",
    "            result = ml_nms(boxlists[i], self.nms_thresh)\n",
    "            number_of_detections = len(result)\n",
    "\n",
    "            # Limit to max_per_image detections **over all classes**\n",
    "            if number_of_detections > self.post_nms_topk > 0:\n",
    "                cls_scores = result.scores\n",
    "                image_thresh, _ = torch.kthvalue(\n",
    "                    cls_scores, number_of_detections - self.post_nms_topk + 1\n",
    "                )\n",
    "\n",
    "                # torch.topk()\n",
    "                keep = cls_scores >= image_thresh.item()\n",
    "                keep = torch.nonzero(keep).squeeze(1)\n",
    "                result = result[keep]\n",
    "            results.append(result)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most part of this file is modified from AdelaiDet\n",
    "# https://github.com/aim-uofa/AdelaiDet\n",
    "\n",
    "import math\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from detectron2.layers import NaiveSyncBatchNorm, ShapeSpec\n",
    "from detectron2.modeling.proposal_generator.build import PROPOSAL_GENERATOR_REGISTRY\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# from ubteacher.utils.comm import compute_locations\n",
    "\n",
    "# from .fcos_outputs import FCOSOutputs\n",
    "\n",
    "\n",
    "__all__ = [\"FCOS\"]\n",
    "\n",
    "INF = 100000000\n",
    "\n",
    "\n",
    "class Scale(nn.Module):\n",
    "    def __init__(self, init_value=1.0):\n",
    "        super(Scale, self).__init__()\n",
    "        self.scale = nn.Parameter(torch.FloatTensor([init_value]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input * self.scale\n",
    "\n",
    "\n",
    "class ModuleListDial(nn.ModuleList):\n",
    "    def __init__(self, modules=None):\n",
    "        super(ModuleListDial, self).__init__(modules)\n",
    "        self.cur_position = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self[self.cur_position](x)\n",
    "        self.cur_position += 1\n",
    "        if self.cur_position >= len(self):\n",
    "            self.cur_position = 0\n",
    "        return result\n",
    "\n",
    "\n",
    "@PROPOSAL_GENERATOR_REGISTRY.register()\n",
    "class FCOS(nn.Module):\n",
    "    def __init__(self, cfg, input_shape: Dict[str, ShapeSpec]):\n",
    "        super().__init__()\n",
    "        self.in_features = cfg.MODEL.FCOS.IN_FEATURES\n",
    "        self.fpn_strides = cfg.MODEL.FCOS.FPN_STRIDES\n",
    "        self.yield_proposal = cfg.MODEL.FCOS.YIELD_PROPOSAL\n",
    "\n",
    "        self.kl_loss = cfg.MODEL.FCOS.KL_LOSS\n",
    "        self.kl_loss_type = cfg.MODEL.FCOS.KL_LOSS_TYPE\n",
    "\n",
    "        self.fcos_head = FCOSHead(cfg, [input_shape[f] for f in self.in_features])\n",
    "        self.in_channels_to_top_module = self.fcos_head.in_channels_to_top_module\n",
    "\n",
    "        self.fcos_outputs = FCOSOutputs(cfg)\n",
    "\n",
    "    def forward_head(self, features, top_module=None):\n",
    "        features = [features[f] for f in self.in_features]\n",
    "\n",
    "        if self.kl_loss:\n",
    "            (\n",
    "                pred_class_logits,\n",
    "                pred_deltas,\n",
    "                reg_pred,\n",
    "                pred_centerness,\n",
    "                top_feats,\n",
    "                bbox_towers,\n",
    "            ) = self.fcos_head(features, top_module, self.yield_proposal)\n",
    "        else:\n",
    "            (\n",
    "                pred_class_logits,\n",
    "                pred_deltas,\n",
    "                pred_centerness,\n",
    "                top_feats,\n",
    "                bbox_towers,\n",
    "            ) = self.fcos_head(features, top_module, self.yield_proposal)\n",
    "\n",
    "        return pred_class_logits, pred_deltas, pred_centerness, top_feats, bbox_towers\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images,\n",
    "        features,\n",
    "        gt_instances=None,\n",
    "        top_module=None,\n",
    "        output_raw=False,\n",
    "        nms_method=\"cls_n_ctr\",\n",
    "        ignore_near=False,\n",
    "        branch=\"labeled\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            images (list[Tensor] or ImageList): images to be processed\n",
    "            targets (list[BoxList]): ground-truth boxes present in the image (optional)\n",
    "\n",
    "        Returns:\n",
    "            result (list[BoxList] or dict[Tensor]): the output from the model.\n",
    "                During training, it returns a dict[Tensor] which contains the losses.\n",
    "                During testing, it returns list[BoxList] contains additional fields\n",
    "                like `scores`, `labels` and `mask` (for Mask R-CNN models).\n",
    "\n",
    "        \"\"\"\n",
    "        features = [features[f] for f in self.in_features]\n",
    "\n",
    "        locations = self.compute_locations(features)\n",
    "\n",
    "        raw_output = {}\n",
    "        if self.kl_loss:\n",
    "            (\n",
    "                logits_pred,\n",
    "                reg_pred,\n",
    "                reg_pred_std,\n",
    "                ctrness_pred,\n",
    "                top_feats,\n",
    "                bbox_towers,\n",
    "            ) = self.fcos_head(features, top_module, self.yield_proposal)\n",
    "            raw_output[\"reg_pred_std\"] = reg_pred_std\n",
    "\n",
    "        else:\n",
    "            (\n",
    "                logits_pred,\n",
    "                reg_pred,\n",
    "                ctrness_pred,\n",
    "                top_feats,\n",
    "                bbox_towers,\n",
    "            ) = self.fcos_head(features, top_module, self.yield_proposal)\n",
    "            reg_pred_std = None\n",
    "        # accumlate feature pred for pseudo-labeling\n",
    "        raw_output[\"logits_pred\"] = logits_pred\n",
    "        raw_output[\"reg_pred\"] = reg_pred\n",
    "        raw_output[\"top_feats\"] = top_feats\n",
    "        raw_output[\"bbox_towers\"] = bbox_towers\n",
    "        raw_output[\"locations\"] = locations\n",
    "        raw_output[\"ctrness_pred\"] = ctrness_pred\n",
    "        raw_output[\"image_sizes\"] = images.image_sizes\n",
    "\n",
    "        results = {}\n",
    "        if self.yield_proposal:\n",
    "            results[\"features\"] = {f: b for f, b in zip(self.in_features, bbox_towers)}\n",
    "\n",
    "        if self.training:\n",
    "\n",
    "            if branch == \"labeled\":\n",
    "                results, losses = self.fcos_outputs.losses(\n",
    "                    logits_pred,\n",
    "                    reg_pred,\n",
    "                    ctrness_pred,\n",
    "                    locations,\n",
    "                    gt_instances,\n",
    "                    reg_pred_std,\n",
    "                    top_feats,\n",
    "                    ignore_near,\n",
    "                    branch=branch,\n",
    "                )\n",
    "            elif branch == \"unlabeled\":\n",
    "                results, losses = self.fcos_outputs.pseudo_losses(\n",
    "                    logits_pred,\n",
    "                    reg_pred,\n",
    "                    ctrness_pred,\n",
    "                    locations,\n",
    "                    gt_instances,\n",
    "                    reg_pred_std,\n",
    "                    top_feats,\n",
    "                    ignore_near,\n",
    "                    branch=branch,\n",
    "                )\n",
    "            elif branch == \"raw\":\n",
    "                results = {}\n",
    "                losses = {}\n",
    "            else:\n",
    "                raise ValueError(\"Unknown branch\")\n",
    "\n",
    "            if self.yield_proposal:\n",
    "                with torch.no_grad():\n",
    "                    results[\"proposals\"] = self.fcos_outputs.predict_proposals(\n",
    "                        logits_pred=logits_pred,\n",
    "                        reg_pred=reg_pred,\n",
    "                        ctrness_pred=ctrness_pred,\n",
    "                        locations=locations,\n",
    "                        image_sizes=images.image_sizes,\n",
    "                        reg_pred_std=reg_pred_std,\n",
    "                        top_feats=top_feats,\n",
    "                        nms_method=nms_method,\n",
    "                    )\n",
    "            if output_raw:\n",
    "                return results, losses, raw_output\n",
    "            else:\n",
    "                return results, losses\n",
    "\n",
    "        else:\n",
    "            results = self.fcos_outputs.predict_proposals(\n",
    "                logits_pred=logits_pred,\n",
    "                reg_pred=reg_pred,\n",
    "                ctrness_pred=ctrness_pred,\n",
    "                locations=locations,\n",
    "                image_sizes=images.image_sizes,\n",
    "                reg_pred_std=reg_pred_std,\n",
    "                top_feats=top_feats,\n",
    "                nms_method=nms_method,\n",
    "            )\n",
    "            if output_raw:\n",
    "                return results, {}, raw_output\n",
    "            else:\n",
    "                return results, {}\n",
    "\n",
    "    def compute_locations(self, features):\n",
    "        locations = []\n",
    "        for level, feature in enumerate(features):\n",
    "            h, w = feature.size()[-2:]\n",
    "            locations_per_level = compute_locations(\n",
    "                h, w, self.fpn_strides[level], feature.device\n",
    "            )\n",
    "            locations.append(locations_per_level)\n",
    "        return locations\n",
    "\n",
    "\n",
    "class FCOSHead(nn.Module):\n",
    "    def __init__(self, cfg, input_shape: List[ShapeSpec]):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            in_channels (int): number of channels of the input feature\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.reg_max = cfg.MODEL.FCOS.REG_MAX\n",
    "        self.reg_discrete = cfg.MODEL.FCOS.REG_DISCRETE\n",
    "        self.kl_loss = cfg.MODEL.FCOS.KL_LOSS\n",
    "\n",
    "        self.num_classes = cfg.MODEL.FCOS.NUM_CLASSES\n",
    "        self.fpn_strides = cfg.MODEL.FCOS.FPN_STRIDES\n",
    "        head_configs = {\n",
    "            \"cls\": (cfg.MODEL.FCOS.NUM_CLS_CONVS, cfg.MODEL.FCOS.USE_DEFORMABLE),\n",
    "            \"bbox\": (cfg.MODEL.FCOS.NUM_BOX_CONVS, cfg.MODEL.FCOS.USE_DEFORMABLE),\n",
    "            \"share\": (cfg.MODEL.FCOS.NUM_SHARE_CONVS, False),\n",
    "        }\n",
    "        norm = None if cfg.MODEL.FCOS.NORM == \"none\" else cfg.MODEL.FCOS.NORM\n",
    "        self.num_levels = len(input_shape)\n",
    "\n",
    "        in_channels = [s.channels for s in input_shape]\n",
    "        assert len(set(in_channels)) == 1, \"Each level must have the same channel!\"\n",
    "        in_channels = in_channels[0]\n",
    "\n",
    "        self.in_channels_to_top_module = in_channels\n",
    "\n",
    "        for head in head_configs:\n",
    "            tower = []\n",
    "            num_convs, use_deformable = head_configs[head]\n",
    "            for _ in range(num_convs):\n",
    "                conv_func = nn.Conv2d\n",
    "                tower.append(\n",
    "                    conv_func(\n",
    "                        in_channels,\n",
    "                        in_channels,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=1,\n",
    "                        bias=True,\n",
    "                    )\n",
    "                )\n",
    "                if norm == \"GN\":\n",
    "                    tower.append(nn.GroupNorm(32, in_channels))\n",
    "                elif norm == \"BN\":\n",
    "                    tower.append(\n",
    "                        ModuleListDial(\n",
    "                            [\n",
    "                                nn.BatchNorm2d(in_channels)\n",
    "                                for _ in range(self.num_levels)\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                elif norm == \"SyncBN\":\n",
    "                    tower.append(\n",
    "                        ModuleListDial(\n",
    "                            [\n",
    "                                NaiveSyncBatchNorm(in_channels)\n",
    "                                for _ in range(self.num_levels)\n",
    "                            ]\n",
    "                        )\n",
    "                    )\n",
    "                tower.append(nn.ReLU())\n",
    "            self.add_module(\"{}_tower\".format(head), nn.Sequential(*tower))\n",
    "\n",
    "        self.cls_logits = nn.Conv2d(\n",
    "            in_channels, self.num_classes, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        if self.reg_discrete:\n",
    "            self.bbox_pred = nn.Conv2d(\n",
    "                in_channels, 4 * (self.reg_max + 1), kernel_size=3, stride=1, padding=1\n",
    "            )\n",
    "        else:\n",
    "            self.bbox_pred = nn.Conv2d(\n",
    "                in_channels, 4, kernel_size=3, stride=1, padding=1\n",
    "            )\n",
    "\n",
    "        if self.kl_loss:\n",
    "            self.bbox_pred_std = nn.Conv2d(\n",
    "                in_channels, 4, kernel_size=3, stride=1, padding=1\n",
    "            )\n",
    "\n",
    "        self.ctrness = nn.Conv2d(in_channels, 1, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        if cfg.MODEL.FCOS.USE_SCALE:  # learnable scale\n",
    "            self.scales = nn.ModuleList(\n",
    "                [Scale(init_value=1.0) for _ in range(self.num_levels)]\n",
    "            )\n",
    "        else:\n",
    "            self.scales = None\n",
    "\n",
    "        # initialize\n",
    "        for modules in [\n",
    "            self.cls_tower,\n",
    "            self.bbox_tower,\n",
    "            self.share_tower,\n",
    "            self.cls_logits,\n",
    "            self.bbox_pred,\n",
    "            self.ctrness,\n",
    "        ]:\n",
    "            for lay in modules.modules():\n",
    "                if isinstance(lay, nn.Conv2d):\n",
    "                    torch.nn.init.normal_(lay.weight, std=0.01)\n",
    "                    torch.nn.init.constant_(lay.bias, 0)\n",
    "\n",
    "        if self.kl_loss:\n",
    "            torch.nn.init.normal_(\n",
    "                self.bbox_pred_std.weight, std=0.0001\n",
    "            )  # follows KL-Loss\n",
    "            torch.nn.init.constant_(self.bbox_pred_std.bias, 0)  # follows KL-Loss\n",
    "\n",
    "        # initialize the bias for focal loss\n",
    "        prior_prob = cfg.MODEL.FCOS.PRIOR_PROB\n",
    "        bias_value = -math.log((1 - prior_prob) / prior_prob)\n",
    "        torch.nn.init.constant_(self.cls_logits.bias, bias_value)\n",
    "\n",
    "    def forward(self, x, top_module=None, yield_bbox_towers=False):\n",
    "        logits = []\n",
    "        bbox_reg = []\n",
    "        bbox_reg_std = []\n",
    "        ctrness = []\n",
    "        top_feats = []\n",
    "        bbox_towers = []\n",
    "        for l, feature in enumerate(x):\n",
    "            feature = self.share_tower(feature)\n",
    "            cls_tower = self.cls_tower(feature)\n",
    "            bbox_tower = self.bbox_tower(feature)\n",
    "            if yield_bbox_towers:\n",
    "                bbox_towers.append(bbox_tower)\n",
    "\n",
    "            logits.append(self.cls_logits(cls_tower))\n",
    "            ctrness.append(self.ctrness(bbox_tower))\n",
    "            reg = self.bbox_pred(bbox_tower)\n",
    "\n",
    "            if self.scales is not None:\n",
    "                reg = self.scales[l](reg)\n",
    "            # Note that we use relu, as in the improved FCOS, instead of exp.\n",
    "\n",
    "            if self.reg_discrete:\n",
    "                # generalized focal loss use softmax\n",
    "                bbox_reg.append(reg)\n",
    "            else:\n",
    "                bbox_reg.append(F.relu(reg))\n",
    "\n",
    "            if self.kl_loss:\n",
    "                reg_std = self.bbox_pred_std(bbox_tower)\n",
    "                bbox_reg_std.append(reg_std)\n",
    "\n",
    "            if top_module is not None:\n",
    "                top_feats.append(top_module(bbox_tower))\n",
    "\n",
    "        if self.kl_loss:  # additional box prediction std output\n",
    "            return logits, bbox_reg, bbox_reg_std, ctrness, top_feats, bbox_towers\n",
    "        else:\n",
    "            return logits, bbox_reg, ctrness, top_feats, bbox_towers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def quality_focal_loss(pred, target, beta=2.0, use_sigmoid=True):\n",
    "    r\"\"\"Quality Focal Loss (QFL) is from `Generalized Focal Loss: Learning\n",
    "    Qualified and Distributed Bounding Boxes for Dense Object Detection\n",
    "    <https://arxiv.org/abs/2006.04388>`_.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted joint representation of classification\n",
    "            and quality (IoU) estimation with shape (N, C), C is the number of\n",
    "            classes.\n",
    "        target (tuple([torch.Tensor])): Target category label with shape (N,)\n",
    "            and target quality label with shape (N,).\n",
    "        beta (float): The beta parameter for calculating the modulating factor.\n",
    "            Defaults to 2.0.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Loss tensor with shape (N,).\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        len(target) == 2\n",
    "    ), \"\"\"target for QFL must be a tuple of two elements,\n",
    "        including category label and quality label, respectively\"\"\"\n",
    "    # label denotes the category id, score denotes the quality score\n",
    "    label, score = target\n",
    "    if use_sigmoid:\n",
    "        func = F.binary_cross_entropy_with_logits\n",
    "    else:\n",
    "        func = F.binary_cross_entropy\n",
    "\n",
    "    # negatives are supervised by 0 quality score\n",
    "    pred_sigmoid = pred.sigmoid() if use_sigmoid else pred\n",
    "    scale_factor = pred_sigmoid\n",
    "    zerolabel = scale_factor.new_zeros(pred.shape)\n",
    "    loss = func(pred, zerolabel, reduction=\"none\") * scale_factor.pow(beta)\n",
    "\n",
    "    # FG cat_id: [0, num_classes -1], BG cat_id: num_classes\n",
    "    bg_class_ind = pred.size(1)\n",
    "    pos = ((label >= 0) & (label < bg_class_ind)).nonzero().squeeze(1)\n",
    "    pos_label = label[pos].long()\n",
    "    # positives are supervised by bbox quality (IoU) score\n",
    "    scale_factor = score[pos] - pred_sigmoid[pos, pos_label]\n",
    "    loss[pos, pos_label] = func(\n",
    "        pred[pos, pos_label], score[pos], reduction=\"none\"\n",
    "    ) * scale_factor.abs().pow(beta)\n",
    "\n",
    "    # loss = loss.sum(dim=1, keepdim=False)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def kl_loss(\n",
    "    input: torch.Tensor,\n",
    "    input_std: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    beta: float,\n",
    "    reduction: str = \"none\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Smooth L1 loss defined in the Fast R-CNN paper as:\n",
    "                  | 0.5 * x ** 2 / beta   if abs(x) < beta\n",
    "    smoothl1(x) = |\n",
    "                  | abs(x) - 0.5 * beta   otherwise,\n",
    "    where x = input - target.\n",
    "    Smooth L1 loss is related to Huber loss, which is defined as:\n",
    "                | 0.5 * x ** 2                  if abs(x) < beta\n",
    "     huber(x) = |\n",
    "                | beta * (abs(x) - 0.5 * beta)  otherwise\n",
    "    Smooth L1 loss is equal to huber(x) / beta. This leads to the following\n",
    "    differences:\n",
    "     - As beta -> 0, Smooth L1 loss converges to L1 loss, while Huber loss\n",
    "       converges to a constant 0 loss.\n",
    "     - As beta -> +inf, Smooth L1 converges to a constant 0 loss, while Huber loss\n",
    "       converges to L2 loss.\n",
    "     - For Smooth L1 loss, as beta varies, the L1 segment of the loss has a constant\n",
    "       slope of 1. For Huber loss, the slope of the L1 segment is beta.\n",
    "    Smooth L1 loss can be seen as exactly L1 loss, but with the abs(x) < beta\n",
    "    portion replaced with a quadratic function such that at abs(x) = beta, its\n",
    "    slope is 1. The quadratic segment smooths the L1 loss near x = 0.\n",
    "    Args:\n",
    "        input (Tensor): input tensor of any shape\n",
    "        target (Tensor): target value tensor with the same shape as input\n",
    "        beta (float): L1 to L2 change point.\n",
    "            For beta values < 1e-5, L1 loss is computed.\n",
    "        reduction: 'none' | 'mean' | 'sum'\n",
    "                 'none': No reduction will be applied to the output.\n",
    "                 'mean': The output will be averaged.\n",
    "                 'sum': The output will be summed.\n",
    "    Returns:\n",
    "        The loss with the reduction option applied.\n",
    "    Note:\n",
    "        PyTorch's builtin \"Smooth L1 loss\" implementation does not actually\n",
    "        implement Smooth L1 loss, nor does it implement Huber loss. It implements\n",
    "        the special case of both in which they are equal (beta=1).\n",
    "        See: https://pytorch.org/docs/stable/nn.html#torch.nn.SmoothL1Loss.\n",
    "    \"\"\"\n",
    "    if beta < 1e-5:\n",
    "        # if beta == 0, then torch.where will result in nan gradients when\n",
    "        # the chain rule is applied due to pytorch implementation details\n",
    "        # (the False branch \"0.5 * n ** 2 / 0\" has an incoming gradient of\n",
    "        # zeros, rather than \"no gradient\"). To avoid this issue, we define\n",
    "        # small values of beta to be exactly l1 loss.\n",
    "        loss = torch.abs(input - target)\n",
    "    else:\n",
    "        n = torch.abs(input - target)\n",
    "        cond = n < beta\n",
    "        l1_smooth = torch.where(cond, 0.5 * n**2 / beta, n - 0.5 * beta)\n",
    "        # loss = torch.exp(-input_std)*l1_smooth.detach() + 0.5*input_std + l1_smooth\n",
    "        loss = torch.exp(-input_std) * l1_smooth + 0.5 * input_std\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        loss = loss.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        loss = loss.sum()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is from the mmdetection.\n",
    "import functools\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def reduce_loss(loss, reduction):\n",
    "    \"\"\"Reduce loss as specified.\n",
    "\n",
    "    Args:\n",
    "        loss (Tensor): Elementwise loss tensor.\n",
    "        reduction (str): Options are \"none\", \"mean\" and \"sum\".\n",
    "\n",
    "    Return:\n",
    "        Tensor: Reduced loss tensor.\n",
    "    \"\"\"\n",
    "    reduction_enum = F._Reduction.get_enum(reduction)\n",
    "    # none: 0, elementwise_mean:1, sum: 2\n",
    "    if reduction_enum == 0:\n",
    "        return loss\n",
    "    elif reduction_enum == 1:\n",
    "        return loss.mean()\n",
    "    elif reduction_enum == 2:\n",
    "        return loss.sum()\n",
    "\n",
    "\n",
    "def weight_reduce_loss(loss, weight=None, reduction=\"mean\", avg_factor=None):\n",
    "    \"\"\"Apply element-wise weight and reduce loss.\n",
    "\n",
    "    Args:\n",
    "        loss (Tensor): Element-wise loss.\n",
    "        weight (Tensor): Element-wise weights.\n",
    "        reduction (str): Same as built-in losses of PyTorch.\n",
    "        avg_factor (float): Avarage factor when computing the mean of losses.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: Processed loss values.\n",
    "    \"\"\"\n",
    "    # if weight is specified, apply element-wise weight\n",
    "    if weight is not None:\n",
    "        loss = loss * weight\n",
    "\n",
    "    # if avg_factor is not specified, just reduce the loss\n",
    "    if avg_factor is None:\n",
    "        loss = reduce_loss(loss, reduction)\n",
    "    else:\n",
    "        # if reduction is mean, then average the loss by avg_factor\n",
    "        if reduction == \"mean\":\n",
    "            loss = loss.sum() / avg_factor\n",
    "        # if reduction is 'none', then do nothing, otherwise raise an error\n",
    "        elif reduction != \"none\":\n",
    "            raise ValueError('avg_factor can not be used with reduction=\"sum\"')\n",
    "    return loss\n",
    "\n",
    "\n",
    "def weighted_loss(loss_func):\n",
    "    \"\"\"Create a weighted version of a given loss function.\n",
    "\n",
    "    To use this decorator, the loss function must have the signature like\n",
    "    `loss_func(pred, target, **kwargs)`. The function only needs to compute\n",
    "    element-wise loss without any reduction. This decorator will add weight\n",
    "    and reduction arguments to the function. The decorated function will have\n",
    "    the signature like `loss_func(pred, target, weight=None, reduction='mean',\n",
    "    avg_factor=None, **kwargs)`.\n",
    "\n",
    "    :Example:\n",
    "\n",
    "    >>> import torch\n",
    "    >>> @weighted_loss\n",
    "    >>> def l1_loss(pred, target):\n",
    "    >>>     return (pred - target).abs()\n",
    "\n",
    "    >>> pred = torch.Tensor([0, 2, 3])\n",
    "    >>> target = torch.Tensor([1, 1, 1])\n",
    "    >>> weight = torch.Tensor([1, 0, 1])\n",
    "\n",
    "    >>> l1_loss(pred, target)\n",
    "    tensor(1.3333)\n",
    "    >>> l1_loss(pred, target, weight)\n",
    "    tensor(1.)\n",
    "    >>> l1_loss(pred, target, reduction='none')\n",
    "    tensor([1., 1., 2.])\n",
    "    >>> l1_loss(pred, target, weight, avg_factor=2)\n",
    "    tensor(1.5000)\n",
    "    \"\"\"\n",
    "\n",
    "    @functools.wraps(loss_func)\n",
    "    def wrapper(pred, target, weight=None, reduction=\"mean\", avg_factor=None, **kwargs):\n",
    "        # get element-wise loss\n",
    "        loss = loss_func(pred, target, **kwargs)\n",
    "        loss = weight_reduce_loss(loss, weight, reduction, avg_factor)\n",
    "        return loss\n",
    "\n",
    "    return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "from detectron2.modeling.meta_arch.build import META_ARCH_REGISTRY\n",
    "from detectron2.modeling.meta_arch.rcnn import GeneralizedRCNN\n",
    "\n",
    "\n",
    "@META_ARCH_REGISTRY.register()\n",
    "class TwoStagePseudoLabGeneralizedRCNN(GeneralizedRCNN):\n",
    "    def forward(\n",
    "        self, batched_inputs, branch=\"supervised\", given_proposals=None, val_mode=False\n",
    "    ):\n",
    "        if (not self.training) and (not val_mode):\n",
    "            return self.inference(batched_inputs)\n",
    "\n",
    "        images = self.preprocess_image(batched_inputs)\n",
    "\n",
    "        if \"instances\" in batched_inputs[0]:\n",
    "            gt_instances = [x[\"instances\"].to(self.device) for x in batched_inputs]\n",
    "        else:\n",
    "            gt_instances = None\n",
    "\n",
    "        features = self.backbone(images.tensor)\n",
    "\n",
    "        if branch == \"supervised\":\n",
    "            # Region proposal network\n",
    "            proposals_rpn, proposal_losses = self.proposal_generator(\n",
    "                images, features, gt_instances\n",
    "            )\n",
    "\n",
    "            # # roi_head lower branch\n",
    "            _, detector_losses = self.roi_heads(\n",
    "                images, features, proposals_rpn, gt_instances, branch=branch\n",
    "            )\n",
    "\n",
    "            losses = {}\n",
    "            losses.update(detector_losses)\n",
    "            losses.update(proposal_losses)\n",
    "            return losses, [], [], None\n",
    "\n",
    "        elif branch == \"unsup_data_weak\":\n",
    "            # Region proposal network\n",
    "            proposals_rpn, _ = self.proposal_generator(\n",
    "                images, features, None, compute_loss=False\n",
    "            )\n",
    "\n",
    "            # roi_head lower branch (keep this for further production)  # notice that we do not use any target in ROI head to do inference !\n",
    "            proposals_roih, ROI_predictions = self.roi_heads(\n",
    "                images,\n",
    "                features,\n",
    "                proposals_rpn,\n",
    "                targets=None,\n",
    "                compute_loss=False,\n",
    "                branch=branch,\n",
    "            )\n",
    "\n",
    "            return {}, proposals_rpn, proposals_roih, ROI_predictions\n",
    "\n",
    "        elif branch == \"unsup_data_train\":  #\n",
    "\n",
    "            # Region proposal network\n",
    "            proposals_rpn, proposal_losses = self.proposal_generator(\n",
    "                images, features, gt_instances\n",
    "            )\n",
    "\n",
    "            # # roi_head lower branch\n",
    "            _, detector_losses = self.roi_heads(\n",
    "                images, features, proposals_rpn, gt_instances, branch=branch\n",
    "            )\n",
    "\n",
    "            losses = {}\n",
    "            losses.update(detector_losses)\n",
    "            losses.update(proposal_losses)\n",
    "            return losses, [], [], None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DataParallel, DistributedDataParallel\n",
    "\n",
    "\n",
    "class EnsembleTSModel(nn.Module):\n",
    "    def __init__(self, modelTeacher, modelStudent):\n",
    "        super(EnsembleTSModel, self).__init__()\n",
    "\n",
    "        if isinstance(modelTeacher, (DistributedDataParallel, DataParallel)):\n",
    "            modelTeacher = modelTeacher.module\n",
    "        if isinstance(modelStudent, (DistributedDataParallel, DataParallel)):\n",
    "            modelStudent = modelStudent.module\n",
    "\n",
    "        self.modelTeacher = modelTeacher\n",
    "        self.modelStudent = modelStudent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from detectron2.layers import cat\n",
    "from detectron2.modeling.box_regression import _dense_box_regression_loss\n",
    "from detectron2.modeling.proposal_generator import RPN\n",
    "from detectron2.modeling.proposal_generator.build import PROPOSAL_GENERATOR_REGISTRY\n",
    "from detectron2.structures import Boxes, ImageList, Instances, pairwise_iou\n",
    "from detectron2.utils.events import get_event_storage\n",
    "from detectron2.utils.memory import retry_if_cuda_oom\n",
    "\n",
    "\n",
    "@PROPOSAL_GENERATOR_REGISTRY.register()\n",
    "class PseudoLabRPN(RPN):\n",
    "    \"\"\"\n",
    "    Region Proposal Network, introduced by :paper:`Faster R-CNN`.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: ImageList,\n",
    "        features: Dict[str, torch.Tensor],\n",
    "        gt_instances: Optional[Instances] = None,\n",
    "        compute_loss: bool = True,\n",
    "        compute_val_loss: bool = False,\n",
    "    ):\n",
    "        features = [features[f] for f in self.in_features]\n",
    "        anchors = self.anchor_generator(features)\n",
    "\n",
    "        pred_objectness_logits, pred_anchor_deltas = self.rpn_head(features)\n",
    "        pred_objectness_logits = [\n",
    "            # (N, A, Hi, Wi) -> (N, Hi, Wi, A) -> (N, Hi*Wi*A)\n",
    "            score.permute(0, 2, 3, 1).flatten(1)\n",
    "            for score in pred_objectness_logits\n",
    "        ]\n",
    "        pred_anchor_deltas = [\n",
    "            # (N, A*B, Hi, Wi) -> (N, A, B, Hi, Wi) -> (N, Hi, Wi, A, B) -> (N, Hi*Wi*A, B)\n",
    "            x.view(\n",
    "                x.shape[0], -1, self.anchor_generator.box_dim, x.shape[-2], x.shape[-1]\n",
    "            )\n",
    "            .permute(0, 3, 4, 1, 2)\n",
    "            .flatten(1, -2)\n",
    "            for x in pred_anchor_deltas\n",
    "        ]\n",
    "\n",
    "        if (self.training and compute_loss) or compute_val_loss:\n",
    "\n",
    "            if gt_instances[0].has(\"scores\"):  # has confidence; then weight loss\n",
    "                gt_labels, gt_boxes, gt_confids = self.label_and_sample_anchors_pseudo(\n",
    "                    anchors, gt_instances\n",
    "                )\n",
    "            else:  # no confidence of each proposal\n",
    "                gt_labels, gt_boxes = self.label_and_sample_anchors(\n",
    "                    anchors, gt_instances\n",
    "                )\n",
    "                gt_confids = None\n",
    "\n",
    "            losses = self.losses(\n",
    "                anchors,\n",
    "                pred_objectness_logits,\n",
    "                gt_labels,\n",
    "                pred_anchor_deltas,\n",
    "                gt_boxes,\n",
    "                gt_confids,\n",
    "            )\n",
    "            losses = {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n",
    "        else:  # inference\n",
    "            losses = {}\n",
    "\n",
    "        proposals = self.predict_proposals(\n",
    "            anchors, pred_objectness_logits, pred_anchor_deltas, images.image_sizes\n",
    "        )\n",
    "\n",
    "        return proposals, losses\n",
    "\n",
    "    def label_and_sample_anchors_pseudo(\n",
    "        self, anchors: List[Boxes], gt_instances: List[Instances]\n",
    "    ) -> Tuple[List[torch.Tensor], List[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            anchors (list[Boxes]): anchors for each feature map.\n",
    "            gt_instances: the ground-truth instances for each image.\n",
    "\n",
    "        Returns:\n",
    "            list[Tensor]:\n",
    "                List of #img tensors. i-th element is a vector of labels whose length is\n",
    "                the total number of anchors across all feature maps R = sum(Hi * Wi * A).\n",
    "                Label values are in {-1, 0, 1}, with meanings: -1 = ignore; 0 = negative\n",
    "                class; 1 = positive class.\n",
    "            list[Tensor]:\n",
    "                i-th element is a Rx4 tensor. The values are the matched gt boxes for each\n",
    "                anchor. Values are undefined for those anchors not labeled as 1.\n",
    "            list[Tensor]:\n",
    "                i-th element is a R tensor. The values are the matched gt scores for each\n",
    "                anchor. Values are undefined for those anchors not labeled as 1.\n",
    "\n",
    "        \"\"\"\n",
    "        anchors = Boxes.cat(anchors)\n",
    "\n",
    "        gt_boxes = [x.gt_boxes for x in gt_instances]\n",
    "        image_sizes = [x.image_size for x in gt_instances]\n",
    "        scores = [x.scores for x in gt_instances]\n",
    "\n",
    "        del gt_instances\n",
    "\n",
    "        gt_labels = []\n",
    "        matched_gt_boxes = []\n",
    "        gt_confids = []\n",
    "\n",
    "        for image_size_i, gt_boxes_i, scores_i in zip(image_sizes, gt_boxes, scores):\n",
    "            \"\"\"\n",
    "            image_size_i: (h, w) for the i-th image\n",
    "            gt_boxes_i: ground-truth boxes for i-th image\n",
    "            \"\"\"\n",
    "            match_quality_matrix = retry_if_cuda_oom(pairwise_iou)(gt_boxes_i, anchors)\n",
    "            matched_idxs, gt_labels_i = retry_if_cuda_oom(self.anchor_matcher)(\n",
    "                match_quality_matrix\n",
    "            )\n",
    "            # Matching is memory-expensive and may result in CPU tensors. But the result is small\n",
    "            gt_labels_i = gt_labels_i.to(device=gt_boxes_i.device)\n",
    "            del match_quality_matrix\n",
    "\n",
    "            if self.anchor_boundary_thresh >= 0:\n",
    "                # Discard anchors that go out of the boundaries of the image\n",
    "                # NOTE: This is legacy functionality that is turned off by default in Detectron2\n",
    "                anchors_inside_image = anchors.inside_box(\n",
    "                    image_size_i, self.anchor_boundary_thresh\n",
    "                )\n",
    "                gt_labels_i[~anchors_inside_image] = -1\n",
    "\n",
    "            # A vector of labels (-1, 0, 1) for each anchor\n",
    "            gt_labels_i = self._subsample_labels(gt_labels_i)\n",
    "            if len(gt_boxes_i) == 0:\n",
    "                # These values won't be used anyway since the anchor is labeled as background\n",
    "                matched_gt_boxes_i = torch.zeros_like(anchors.tensor)\n",
    "                gt_confidence = torch.zeros_like(\n",
    "                    matched_idxs\n",
    "                )  # no boxes in the label --> no loss\n",
    "            else:\n",
    "                # TODO wasted indexing computation for ignored boxes\n",
    "                matched_gt_boxes_i = gt_boxes_i[matched_idxs].tensor\n",
    "                gt_confidence = scores_i[matched_idxs]\n",
    "\n",
    "            gt_labels.append(gt_labels_i)  # N,AHW\n",
    "            matched_gt_boxes.append(matched_gt_boxes_i)\n",
    "            gt_confids.append(gt_confidence)\n",
    "\n",
    "        return gt_labels, matched_gt_boxes, gt_confids\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def losses(\n",
    "        self,\n",
    "        anchors: List[Boxes],\n",
    "        pred_objectness_logits: List[torch.Tensor],\n",
    "        gt_labels: List[torch.Tensor],\n",
    "        pred_anchor_deltas: List[torch.Tensor],\n",
    "        gt_boxes: List[torch.Tensor],\n",
    "        gt_confids: List[torch.Tensor] = None,\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Return the losses from a set of RPN predictions and their associated ground-truth.\n",
    "\n",
    "        Args:\n",
    "            anchors (list[Boxes or RotatedBoxes]): anchors for each feature map, each\n",
    "                has shape (Hi*Wi*A, B), where B is box dimension (4 or 5).\n",
    "            pred_objectness_logits (list[Tensor]): A list of L elements.\n",
    "                Element i is a tensor of shape (N, Hi*Wi*A) representing\n",
    "                the predicted objectness logits for all anchors.\n",
    "            gt_labels (list[Tensor]): Output of :meth:`label_and_sample_anchors`.\n",
    "            pred_anchor_deltas (list[Tensor]): A list of L elements. Element i is a tensor of shape\n",
    "                (N, Hi*Wi*A, 4 or 5) representing the predicted \"deltas\" used to transform anchors\n",
    "                to proposals.\n",
    "            gt_boxes (list[Tensor]): Output of :meth:`label_and_sample_anchors`.\n",
    "\n",
    "        Returns:\n",
    "            dict[loss name -> loss value]: A dict mapping from loss name to loss value.\n",
    "                Loss names are: `loss_rpn_cls` for objectness classification and\n",
    "                `loss_rpn_loc` for proposal localization.\n",
    "        \"\"\"\n",
    "        num_images = len(gt_labels)\n",
    "        gt_labels = torch.stack(gt_labels)  # (N, sum(Hi*Wi*Ai))\n",
    "\n",
    "        # Log the number of positive/negative anchors per-image that's used in training\n",
    "        pos_mask = gt_labels == 1\n",
    "        num_pos_anchors = pos_mask.sum().item()\n",
    "        num_neg_anchors = (gt_labels == 0).sum().item()\n",
    "        storage = get_event_storage()\n",
    "        storage.put_scalar(\"rpn/num_pos_anchors\", num_pos_anchors / num_images)\n",
    "        storage.put_scalar(\"rpn/num_neg_anchors\", num_neg_anchors / num_images)\n",
    "\n",
    "        # localization loss is not weighted\n",
    "        localization_loss = _dense_box_regression_loss(\n",
    "            anchors,\n",
    "            self.box2box_transform,\n",
    "            pred_anchor_deltas,\n",
    "            gt_boxes,\n",
    "            pos_mask,\n",
    "            box_reg_loss_type=self.box_reg_loss_type,\n",
    "            smooth_l1_beta=self.smooth_l1_beta,\n",
    "        )\n",
    "\n",
    "        valid_mask = gt_labels >= 0\n",
    "        if gt_confids:  # weights\n",
    "            gt_confids = torch.stack(gt_confids)  # (N, sum(Hi*Wi*Ai))\n",
    "            objectness_loss = F.binary_cross_entropy_with_logits(\n",
    "                cat(pred_objectness_logits, dim=1)[valid_mask],\n",
    "                gt_labels[valid_mask].to(torch.float32),\n",
    "                weight=gt_confids[valid_mask],\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "        else:  # no weights\n",
    "            objectness_loss = F.binary_cross_entropy_with_logits(\n",
    "                cat(pred_objectness_logits, dim=1)[valid_mask],\n",
    "                gt_labels[valid_mask].to(torch.float32),\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "        normalizer = self.batch_size_per_image * num_images\n",
    "        losses = {\n",
    "            \"loss_rpn_cls\": objectness_loss / normalizer,\n",
    "            \"loss_rpn_loc\": localization_loss / normalizer,\n",
    "        }\n",
    "        losses = {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n",
    "        return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "_DEFAULT_SCALE_CLAMP = 1000.0 / 16\n",
    "\n",
    "\n",
    "__all__ = [\"Box2BoxXYXYTransform\"]\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "class Box2BoxXYXYTransform(object):\n",
    "    \"\"\"\n",
    "    The box-to-box transform defined in R-CNN. The transformation is parameterized\n",
    "    by 4 deltas: (dx, dy, dw, dh). The transformation scales the box's width and height\n",
    "    by exp(dw), exp(dh) and shifts a box's center by the offset (dx * width, dy * height).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        weights: Tuple[float, float, float, float],\n",
    "        scale_clamp: float = _DEFAULT_SCALE_CLAMP,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weights (4-element tuple): Scaling factors that are applied to the\n",
    "                (dx, dy, dw, dh) deltas. In Fast R-CNN, these were originally set\n",
    "                such that the deltas have unit variance; now they are treated as\n",
    "                hyperparameters of the system.\n",
    "            scale_clamp (float): When predicting deltas, the predicted box scaling\n",
    "                factors (dw and dh) are clamped such that they are <= scale_clamp.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.scale_clamp = scale_clamp\n",
    "\n",
    "    def get_deltas(self, src_boxes, target_boxes):\n",
    "        \"\"\"\n",
    "        Follow the KL-Loss implementation (CVPR'19)\n",
    "        https://github.com/yihui-he/KL-Loss/blob/1c67310c9f5a79cfa985fea241791ccedbdb7dcf/detectron/utils/boxes.py#L328-L353\n",
    "\n",
    "        Args:\n",
    "            src_boxes (Tensor): source boxes, e.g., object proposals\n",
    "            target_boxes (Tensor): target of the transformation, e.g., ground-truth\n",
    "                boxes.\n",
    "        \"\"\"\n",
    "        assert isinstance(src_boxes, torch.Tensor), type(src_boxes)\n",
    "        assert isinstance(target_boxes, torch.Tensor), type(target_boxes)\n",
    "\n",
    "        tgt_l = target_boxes[:, 0]\n",
    "        tgt_r = target_boxes[:, 2]\n",
    "        tgt_d = target_boxes[:, 1]\n",
    "        tgt_u = target_boxes[:, 3]\n",
    "\n",
    "        src_l = src_boxes[:, 0]\n",
    "        src_r = src_boxes[:, 2]\n",
    "        src_d = src_boxes[:, 1]\n",
    "        src_u = src_boxes[:, 3]\n",
    "\n",
    "        src_widths = src_r - src_l + 1.0\n",
    "        src_heights = src_u - src_d + 1.0\n",
    "\n",
    "        # kind of weird to use (10,10,10,10), but we just follow KL-loss\n",
    "        wx, wy, _, _ = self.weights\n",
    "        s2t_dl = wx * (tgt_l - src_l) / src_widths\n",
    "        s2t_dr = wx * (tgt_r - src_r) / src_widths\n",
    "        s2t_dd = wy * (tgt_d - src_d) / src_heights\n",
    "        s2t_du = wy * (tgt_u - src_u) / src_heights\n",
    "\n",
    "        deltas = torch.stack((s2t_dl, s2t_dr, s2t_dd, s2t_du), dim=1)\n",
    "        assert (\n",
    "            (src_widths > 0).all().item()\n",
    "        ), \"Input boxes to Box2BoxTransform are not valid!\"\n",
    "        return deltas\n",
    "\n",
    "    def apply_deltas(self, deltas, boxes):\n",
    "        \"\"\"\n",
    "        Follow the KL-Loss implementation (CVPR'19)\n",
    "        https://github.com/yihui-he/KL-Loss/blob/1c67310c9f5a79cfa985fea241791ccedbdb7dcf/detectron/utils/boxes.py#L208\n",
    "\n",
    "        Args:\n",
    "            deltas (Tensor): transformation deltas of shape (N, k*4), where k >= 1.\n",
    "                deltas[i] represents k potentially different class-specific\n",
    "                box transformations for the single box boxes[i].\n",
    "            boxes (Tensor): boxes to transform, of shape (N, 4)\n",
    "        \"\"\"\n",
    "        boxes = boxes.to(deltas.dtype)\n",
    "\n",
    "        widths = boxes[:, 2] - boxes[:, 0]\n",
    "        heights = boxes[:, 3] - boxes[:, 1]\n",
    "\n",
    "        left = boxes[:, 0]\n",
    "        right = boxes[:, 2]\n",
    "        down = boxes[:, 1]\n",
    "        up = boxes[:, 3]\n",
    "\n",
    "        wx, wy, _, _ = self.weights\n",
    "        dl = deltas[:, 0::4] / wx\n",
    "        dr = deltas[:, 1::4] / wx\n",
    "        dd = deltas[:, 2::4] / wy\n",
    "        du = deltas[:, 3::4] / wy\n",
    "\n",
    "        # Prevent sending too large values into torch.exp()\n",
    "        # dw = torch.clamp(dw, max=self.scale_clamp)\n",
    "        # dh = torch.clamp(dh, max=self.scale_clamp)\n",
    "        # dl = np.maximum(np.minimum(dl, cfg.BBOX_XFORM_CLIPe), -cfg.BBOX_XFORM_CLIPe)\n",
    "        # dr = np.maximum(np.minimum(dr, cfg.BBOX_XFORM_CLIPe), -cfg.BBOX_XFORM_CLIPe)\n",
    "        # dd = np.maximum(np.minimum(dd, cfg.BBOX_XFORM_CLIPe), -cfg.BBOX_XFORM_CLIPe)\n",
    "        # du = np.maximum(np.minimum(du, cfg.BBOX_XFORM_CLIPe), -cfg.BBOX_XFORM_CLIPe)\n",
    "\n",
    "        # Prevent sending too large values into np.exp()       # TODO: find out cfg.BBOX_XFORM_CLIPe\n",
    "        dl = torch.clamp(dl, max=self.scale_clamp, min=-self.scale_clamp)\n",
    "        dr = torch.clamp(dr, max=self.scale_clamp, min=-self.scale_clamp)\n",
    "        dd = torch.clamp(dd, max=self.scale_clamp, min=-self.scale_clamp)\n",
    "        du = torch.clamp(du, max=self.scale_clamp, min=-self.scale_clamp)\n",
    "\n",
    "        # pred_ctr_x = dl * widths[:, None] + left[:, None]\n",
    "        # pred_ctr_y = dr * heights[:, None] + right[:, None]\n",
    "\n",
    "        pred_l = dl * widths[:, None] + left[:, None]\n",
    "        pred_r = dr * widths[:, None] + right[:, None]\n",
    "        pred_d = dd * heights[:, None] + down[:, None]\n",
    "        pred_u = du * heights[:, None] + up[:, None]\n",
    "\n",
    "        pred_boxes = torch.zeros_like(deltas)\n",
    "        pred_boxes[:, 0::4] = pred_l\n",
    "        pred_boxes[:, 1::4] = pred_d\n",
    "        pred_boxes[:, 2::4] = pred_r\n",
    "        pred_boxes[:, 3::4] = pred_u\n",
    "        return pred_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from detectron2.data.detection_utils import convert_image_to_rgb\n",
    "from detectron2.modeling.backbone import build_backbone\n",
    "from detectron2.modeling.meta_arch.build import META_ARCH_REGISTRY\n",
    "from detectron2.modeling.postprocessing import detector_postprocess as d2_postprocesss\n",
    "from detectron2.modeling.proposal_generator import build_proposal_generator\n",
    "from detectron2.structures import ImageList\n",
    "from detectron2.utils.events import get_event_storage\n",
    "from detectron2.utils.logger import log_first_n\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def detector_postprocess(results, output_height, output_width, mask_threshold=0.5):\n",
    "    \"\"\"\n",
    "    In addition to the post processing of detectron2, we add scalign for\n",
    "    bezier control points.\n",
    "    \"\"\"\n",
    "    scale_x, scale_y = (\n",
    "        output_width / results.image_size[1],\n",
    "        output_height / results.image_size[0],\n",
    "    )\n",
    "    results = d2_postprocesss(results, output_height, output_width, mask_threshold)\n",
    "\n",
    "    # scale bezier points\n",
    "    if results.has(\"beziers\"):\n",
    "        beziers = results.beziers\n",
    "        # scale and clip in place\n",
    "        beziers[:, 0::2] *= scale_x\n",
    "        beziers[:, 1::2] *= scale_y\n",
    "        h, w = results.image_size\n",
    "        beziers[:, 0].clamp_(min=0, max=w)\n",
    "        beziers[:, 1].clamp_(min=0, max=h)\n",
    "        beziers[:, 6].clamp_(min=0, max=w)\n",
    "        beziers[:, 7].clamp_(min=0, max=h)\n",
    "        beziers[:, 8].clamp_(min=0, max=w)\n",
    "        beziers[:, 9].clamp_(min=0, max=h)\n",
    "        beziers[:, 14].clamp_(min=0, max=w)\n",
    "        beziers[:, 15].clamp_(min=0, max=h)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "@META_ARCH_REGISTRY.register()\n",
    "class PseudoProposalNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A meta architecture that only predicts object proposals.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.backbone = build_backbone(cfg)\n",
    "        self.proposal_generator = build_proposal_generator(\n",
    "            cfg, self.backbone.output_shape()\n",
    "        )\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"pixel_mean\", torch.Tensor(cfg.MODEL.PIXEL_MEAN).view(-1, 1, 1)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"pixel_std\", torch.Tensor(cfg.MODEL.PIXEL_STD).view(-1, 1, 1)\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.pixel_mean.device\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batched_inputs,\n",
    "        output_raw=False,\n",
    "        nms_method=\"cls_n_ctr\",\n",
    "        ignore_near=False,\n",
    "        branch=\"labeled\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            Same as in :class:`GeneralizedRCNN.forward`\n",
    "\n",
    "        Returns:\n",
    "            list[dict]:\n",
    "                Each dict is the output for one input image.\n",
    "                The dict contains one key \"proposals\" whose value is a\n",
    "                :class:`Instances` with keys \"proposal_boxes\" and \"objectness_logits\".\n",
    "        \"\"\"\n",
    "        images = [x[\"image\"].to(self.device) for x in batched_inputs]\n",
    "        images = [(x - self.pixel_mean) / self.pixel_std for x in images]\n",
    "        images = ImageList.from_tensors(images, self.backbone.size_divisibility)\n",
    "        features = self.backbone(images.tensor)\n",
    "\n",
    "        if \"instances\" in batched_inputs[0] and branch != \"teacher_weak\":\n",
    "            gt_instances = [x[\"instances\"].to(self.device) for x in batched_inputs]\n",
    "        elif \"targets\" in batched_inputs[0] and branch != \"teacher_weak\":\n",
    "            log_first_n(\n",
    "                logging.WARN,\n",
    "                \"'targets' in the model inputs is now renamed to 'instances'!\",\n",
    "                n=10,\n",
    "            )\n",
    "            gt_instances = [x[\"targets\"].to(self.device) for x in batched_inputs]\n",
    "        else:\n",
    "            gt_instances = None\n",
    "\n",
    "        if output_raw:\n",
    "            proposals, proposal_losses, raw_pred = self.proposal_generator(\n",
    "                images,\n",
    "                features,\n",
    "                gt_instances,\n",
    "                output_raw=output_raw,\n",
    "                nms_method=nms_method,\n",
    "                ignore_near=ignore_near,\n",
    "            )\n",
    "        else:\n",
    "            proposals, proposal_losses = self.proposal_generator(\n",
    "                images,\n",
    "                features,\n",
    "                gt_instances,\n",
    "                output_raw=output_raw,\n",
    "                nms_method=nms_method,\n",
    "                ignore_near=ignore_near,\n",
    "            )\n",
    "\n",
    "        # In training, the proposals are not useful at all but we generate them anyway.\n",
    "        # This makes RPN-only models about 5% slower.\n",
    "        if self.training:\n",
    "            if output_raw:\n",
    "                return proposal_losses, raw_pred\n",
    "            else:\n",
    "                return proposal_losses\n",
    "\n",
    "        if output_raw:\n",
    "            # output raw will not rescale\n",
    "            return proposals, raw_pred\n",
    "        else:\n",
    "            # standard output will rescale\n",
    "            processed_results = []\n",
    "            for results_per_image, input_per_image, image_size in zip(\n",
    "                proposals, batched_inputs, images.image_sizes\n",
    "            ):\n",
    "                height = input_per_image.get(\"height\", image_size[0])\n",
    "                width = input_per_image.get(\"width\", image_size[1])\n",
    "                ret = detector_postprocess(results_per_image, height, width)\n",
    "                processed_results.append({\"proposals\": ret})\n",
    "            return processed_results\n",
    "\n",
    "\n",
    "@META_ARCH_REGISTRY.register()\n",
    "class OneStageDetector(PseudoProposalNetwork):\n",
    "    \"\"\"\n",
    "    Same as :class:`detectron2.modeling.ProposalNetwork`.\n",
    "    Uses \"instances\" as the return key instead of using \"proposal\".\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        batched_inputs,\n",
    "        output_raw=False,\n",
    "        nms_method=\"cls_n_ctr\",\n",
    "        ignore_near=False,\n",
    "        branch=\"labeled\",\n",
    "    ):\n",
    "        # training\n",
    "        if self.training:\n",
    "            images = [x[\"image\"].to(self.device) for x in batched_inputs]\n",
    "            images = [(x - self.pixel_mean) / self.pixel_std for x in images]\n",
    "            images = ImageList.from_tensors(images, self.backbone.size_divisibility)\n",
    "            features = self.backbone(images.tensor)\n",
    "\n",
    "            # pseudo-labels for classification and regression\n",
    "            if (\n",
    "                \"instances_class\" in batched_inputs[0]\n",
    "                and \"instances_reg\" in batched_inputs[0]\n",
    "            ):\n",
    "                gt_instances_cls = [\n",
    "                    x[\"instances_class\"].to(self.device) for x in batched_inputs\n",
    "                ]\n",
    "                gt_instances_reg = [\n",
    "                    x[\"instances_reg\"].to(self.device) for x in batched_inputs\n",
    "                ]\n",
    "                gt_instances = {\"cls\": gt_instances_cls, \"reg\": gt_instances_reg}\n",
    "\n",
    "            elif \"instances\" in batched_inputs[0] and branch != \"teacher_weak\":\n",
    "                gt_instances = [x[\"instances\"].to(self.device) for x in batched_inputs]\n",
    "\n",
    "            elif \"targets\" in batched_inputs[0] and branch != \"teacher_weak\":\n",
    "                log_first_n(\n",
    "                    logging.WARN,\n",
    "                    \"'targets' in the model inputs is now renamed to 'instances'!\",\n",
    "                    n=10,\n",
    "                )\n",
    "                gt_instances = [x[\"targets\"].to(self.device) for x in batched_inputs]\n",
    "            else:\n",
    "                gt_instances = None\n",
    "\n",
    "            if output_raw:\n",
    "                proposals, proposal_losses, raw_pred = self.proposal_generator(\n",
    "                    images,\n",
    "                    features,\n",
    "                    gt_instances,\n",
    "                    output_raw=output_raw,\n",
    "                    ignore_near=ignore_near,\n",
    "                    branch=branch,\n",
    "                )\n",
    "            else:\n",
    "                proposals, proposal_losses = self.proposal_generator(\n",
    "                    images,\n",
    "                    features,\n",
    "                    gt_instances,\n",
    "                    output_raw=output_raw,\n",
    "                    ignore_near=ignore_near,\n",
    "                    branch=branch,\n",
    "                )\n",
    "\n",
    "            if self.training:\n",
    "                if output_raw:\n",
    "                    return proposal_losses, raw_pred, proposals\n",
    "                else:\n",
    "                    return proposal_losses\n",
    "\n",
    "        # inference\n",
    "        if output_raw:\n",
    "            proposal, raw_pred = super().forward(\n",
    "                batched_inputs,\n",
    "                output_raw=output_raw,\n",
    "                nms_method=nms_method,\n",
    "                branch=branch,\n",
    "            )\n",
    "            return proposal, raw_pred\n",
    "        else:\n",
    "            processed_results = super().forward(\n",
    "                batched_inputs,\n",
    "                output_raw=output_raw,\n",
    "                nms_method=nms_method,\n",
    "                branch=branch,\n",
    "            )\n",
    "            processed_results = [\n",
    "                {\"instances\": r[\"proposals\"]} for r in processed_results\n",
    "            ]\n",
    "            return processed_results\n",
    "\n",
    "    def visualize_training(self, batched_inputs, proposals, branch):\n",
    "        \"\"\"\n",
    "        A function used to visualize images and proposals. It shows ground truth\n",
    "        bounding boxes on the original image and up to 20 top-scoring predicted\n",
    "        object proposals on the original image. Users can implement different\n",
    "        visualization functions for different models.\n",
    "\n",
    "        Args:\n",
    "            batched_inputs (list): a list that contains input to the model.\n",
    "            proposals (list): a list that contains predicted proposals. Both\n",
    "                batched_inputs and proposals should have the same length.\n",
    "        \"\"\"\n",
    "        from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "        storage = get_event_storage()\n",
    "        max_vis_prop = 20\n",
    "\n",
    "        for input, prop in zip(batched_inputs, proposals):\n",
    "            if branch == \"labeled\":\n",
    "                img = input[\"image\"]\n",
    "                img = convert_image_to_rgb(img.permute(1, 2, 0), \"BGR\")\n",
    "                v_gt = Visualizer(img, None)\n",
    "                v_gt = v_gt.overlay_instances(\n",
    "                    boxes=input[\"instances\"].gt_boxes.to(\"cpu\")\n",
    "                )\n",
    "                anno_img = v_gt.get_image()\n",
    "                box_size = min(len(prop.pred_boxes), max_vis_prop)\n",
    "                v_pred = Visualizer(img, None)\n",
    "                v_pred = v_pred.overlay_instances(\n",
    "                    boxes=prop.pred_boxes[0:box_size].tensor.cpu().numpy()\n",
    "                )\n",
    "                prop_img = v_pred.get_image()\n",
    "                vis_img = np.concatenate((anno_img, prop_img), axis=1)\n",
    "                vis_img = vis_img.transpose(2, 0, 1)\n",
    "                vis_name = (\n",
    "                    branch\n",
    "                    + \" | Left: GT bounding boxes;      Right: Predicted proposals\"\n",
    "                )\n",
    "            elif branch == \"unlabeled\":\n",
    "                img_list = []\n",
    "                img = input[\"image\"]\n",
    "                img = convert_image_to_rgb(img.permute(1, 2, 0), \"BGR\")\n",
    "\n",
    "                # classification pseudo-set\n",
    "                if \"instances_class\" in input:\n",
    "                    v_gt = Visualizer(img, None)\n",
    "                    v_gt = v_gt.overlay_instances(\n",
    "                        boxes=input[\"instances_class\"].gt_boxes.to(\"cpu\")\n",
    "                    )\n",
    "                    anno_img = v_gt.get_image()\n",
    "                    img_list.append(anno_img)\n",
    "\n",
    "                # regression pseudo-set\n",
    "                if \"instances_reg\" in input:\n",
    "                    v_gt2 = Visualizer(img, None)\n",
    "                    v_gt2 = v_gt2.overlay_instances(\n",
    "                        boxes=input[\"instances_reg\"].gt_boxes.to(\"cpu\")\n",
    "                    )\n",
    "                    anno_reg_img = v_gt2.get_image()\n",
    "                    img_list.append(anno_reg_img)\n",
    "\n",
    "                box_size = min(len(prop.pred_boxes), max_vis_prop)\n",
    "                v_pred = Visualizer(img, None)\n",
    "                v_pred = v_pred.overlay_instances(\n",
    "                    boxes=prop.pred_boxes[0:box_size].tensor.cpu().numpy()\n",
    "                )\n",
    "                prop_img = v_pred.get_image()\n",
    "                img_list.append(prop_img)\n",
    "\n",
    "                vis_img = np.concatenate(tuple(img_list), axis=1)\n",
    "                vis_img = vis_img.transpose(2, 0, 1)\n",
    "\n",
    "                vis_name = (\n",
    "                    branch\n",
    "                    + \" | Left: Pseudo-Cls; Center: Pseudo-Reg; Right: Predicted proposals\"\n",
    "                )\n",
    "            else:\n",
    "                break\n",
    "            storage.put_image(vis_name, vis_img)\n",
    "            break  # only visualize one image in a batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "from detectron2.structures import Boxes\n",
    "from detectron2.structures.instances import Instances\n",
    "# from ubteacher.modeling.fcos.fcos_outputs import FCOSOutputs\n",
    "\n",
    "\n",
    "class PseudoGenerator:\n",
    "    def __init__(self, cfg):\n",
    "        self.fcos_output = FCOSOutputs(cfg)\n",
    "\n",
    "    def nms_from_dense(self, raw_output, nms_method):\n",
    "\n",
    "        assert nms_method in [\"cls\", \"ctr\", \"cls_n_ctr\", \"cls_n_loc\"]\n",
    "\n",
    "        logits_pred = raw_output[\"logits_pred\"]\n",
    "        reg_pred = raw_output[\"reg_pred\"]\n",
    "        top_feats = raw_output[\"top_feats\"]\n",
    "        locations = raw_output[\"locations\"]\n",
    "        ctrness_pred = raw_output[\"ctrness_pred\"]\n",
    "        image_sizes = raw_output[\"image_sizes\"]\n",
    "\n",
    "        reg_pred_std = None\n",
    "        if \"reg_pred_std\" in raw_output:\n",
    "            reg_pred_std = raw_output[\"reg_pred_std\"]\n",
    "\n",
    "        results = self.fcos_output.predict_proposals(\n",
    "            logits_pred,\n",
    "            reg_pred,\n",
    "            ctrness_pred,\n",
    "            locations,\n",
    "            image_sizes,\n",
    "            reg_pred_std,\n",
    "            top_feats,\n",
    "            nms_method,\n",
    "        )\n",
    "        return results\n",
    "\n",
    "    # generate\n",
    "    def process_pseudo_label(\n",
    "        self, proposals_rpn_unsup_k, cur_threshold, proposal_type, psedo_label_method=\"\"\n",
    "    ):\n",
    "        list_instances = []\n",
    "        num_proposal_output = 0.0\n",
    "        for proposal_bbox_inst in proposals_rpn_unsup_k:\n",
    "            # thresholding\n",
    "            if psedo_label_method == \"thresholding\":\n",
    "                proposal_bbox_inst = self.threshold_bbox(\n",
    "                    proposal_bbox_inst, thres=cur_threshold, proposal_type=proposal_type\n",
    "                )\n",
    "            elif psedo_label_method == \"thresholding_cls_ctr\":\n",
    "                proposal_bbox_inst = self.threshold_cls_ctr_bbox(\n",
    "                    proposal_bbox_inst, thres=cur_threshold\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Unkown pseudo label boxes methods\")\n",
    "            num_proposal_output += len(proposal_bbox_inst)\n",
    "            list_instances.append(proposal_bbox_inst)\n",
    "        num_proposal_output = num_proposal_output / len(proposals_rpn_unsup_k)\n",
    "        return list_instances, num_proposal_output\n",
    "\n",
    "    def threshold_bbox(self, proposal_bbox_inst, thres=0.7, proposal_type=\"roih\"):\n",
    "        # for fcos\n",
    "        if isinstance(proposal_bbox_inst, dict):\n",
    "            proposal_bbox_inst = proposal_bbox_inst[\"instances\"]\n",
    "\n",
    "        if proposal_type == \"rpn\":\n",
    "            valid_map = proposal_bbox_inst.objectness_logits > thres\n",
    "\n",
    "            # create instances containing boxes and gt_classes\n",
    "            image_shape = proposal_bbox_inst.image_size\n",
    "            new_proposal_inst = Instances(image_shape)\n",
    "\n",
    "            # create box\n",
    "            new_bbox_loc = proposal_bbox_inst.proposal_boxes.tensor[valid_map, :]\n",
    "            new_boxes = Boxes(new_bbox_loc)\n",
    "\n",
    "            # add boxes to instances\n",
    "            new_proposal_inst.gt_boxes = new_boxes\n",
    "            new_proposal_inst.objectness_logits = proposal_bbox_inst.objectness_logits[\n",
    "                valid_map\n",
    "            ]\n",
    "\n",
    "        elif proposal_type == \"roih\":\n",
    "            valid_map = proposal_bbox_inst.scores > thres\n",
    "\n",
    "            # create instances containing boxes and gt_classes\n",
    "            image_shape = proposal_bbox_inst.image_size\n",
    "            new_proposal_inst = Instances(image_shape)\n",
    "            # create box\n",
    "            new_bbox_loc = proposal_bbox_inst.pred_boxes.tensor[valid_map]\n",
    "            new_boxes = Boxes(new_bbox_loc)\n",
    "\n",
    "            # add boxes to instances\n",
    "            new_proposal_inst.gt_boxes = new_boxes\n",
    "            new_proposal_inst.gt_classes = proposal_bbox_inst.pred_classes[valid_map]\n",
    "            new_proposal_inst.scores = proposal_bbox_inst.scores[valid_map]\n",
    "            new_proposal_inst.centerness = proposal_bbox_inst.centerness[valid_map]\n",
    "            new_proposal_inst.cls_confid = proposal_bbox_inst.cls_confid[valid_map]\n",
    "            if proposal_bbox_inst.has(\"reg_pred_std\"):\n",
    "                new_proposal_inst.reg_pred_std = proposal_bbox_inst.reg_pred_std[\n",
    "                    valid_map\n",
    "                ]\n",
    "\n",
    "        return new_proposal_inst\n",
    "\n",
    "    def threshold_cls_ctr_bbox(self, proposal_bbox_inst, thres=(0.5, 0.5)):\n",
    "        # for fcos\n",
    "        if isinstance(proposal_bbox_inst, dict):\n",
    "            proposal_bbox_inst = proposal_bbox_inst[\"instances\"]\n",
    "        cls_map = proposal_bbox_inst.cls_confid > thres[0]\n",
    "        ctr_map = proposal_bbox_inst.centerness > thres[1]\n",
    "        valid_map = cls_map * ctr_map\n",
    "\n",
    "        # create instances containing boxes and gt_classes\n",
    "        image_shape = proposal_bbox_inst.image_size\n",
    "        new_proposal_inst = Instances(image_shape)\n",
    "        # create box\n",
    "        new_bbox_loc = proposal_bbox_inst.pred_boxes.tensor[valid_map]\n",
    "        new_boxes = Boxes(new_bbox_loc)\n",
    "\n",
    "        # add boxes to instances\n",
    "        new_proposal_inst.gt_boxes = new_boxes\n",
    "        new_proposal_inst.gt_classes = proposal_bbox_inst.pred_classes[valid_map]\n",
    "        new_proposal_inst.scores = proposal_bbox_inst.scores[valid_map]\n",
    "        new_proposal_inst.centerness = proposal_bbox_inst.centerness[valid_map]\n",
    "        new_proposal_inst.cls_confid = proposal_bbox_inst.cls_confid[valid_map]\n",
    "        if proposal_bbox_inst.has(\"reg_pred_std\"):\n",
    "            new_proposal_inst.reg_pred_std = proposal_bbox_inst.reg_pred_std[valid_map]\n",
    "\n",
    "        return new_proposal_inst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import math\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from detectron2.config import configurable\n",
    "from detectron2.layers import cat, cross_entropy, nonzero_tuple, ShapeSpec\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import (\n",
    "    _log_classification_stats,\n",
    "    fast_rcnn_inference,\n",
    "    FastRCNNOutputLayers,\n",
    ")\n",
    "from detectron2.structures import Boxes, Instances\n",
    "from fvcore.nn import giou_loss, smooth_l1_loss\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "# from ubteacher.modeling.box_regression import Box2BoxXYXYTransform\n",
    "\n",
    "\n",
    "def matched_boxlist_iou(boxes1: Boxes, boxes2: Boxes) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Compute pairwise intersection over union (IOU) of two sets of matched\n",
    "    boxes. The box order must be (xmin, ymin, xmax, ymax).\n",
    "    Similar to boxlist_iou, but computes only diagonal elements of the matrix\n",
    "    Arguments:\n",
    "        boxes1: (Boxes) bounding boxes, sized [N,4].\n",
    "        boxes2: (Boxes) bounding boxes, sized [N,4].\n",
    "    Returns:\n",
    "        (tensor) iou, sized [N].\n",
    "    \"\"\"\n",
    "    assert len(boxes1) == len(\n",
    "        boxes2\n",
    "    ), \"boxlists should have the same\" \"number of entries, got {}, {}\".format(\n",
    "        len(boxes1), len(boxes2)\n",
    "    )\n",
    "    area1 = boxes1.area()  # [N]\n",
    "    area2 = boxes2.area()  # [N]\n",
    "    box1, box2 = boxes1.tensor, boxes2.tensor\n",
    "    lt = torch.max(box1[:, :2], box2[:, :2])  # [N,2]\n",
    "    rb = torch.min(box1[:, 2:], box2[:, 2:])  # [N,2]\n",
    "    wh = (rb - lt).clamp(min=0)  # [N,2]\n",
    "    inter = wh[:, 0] * wh[:, 1]  # [N]\n",
    "    iou = inter / (area1 + area2 - inter)  # [N]\n",
    "    return iou\n",
    "\n",
    "\n",
    "class FastRCNNOutputs:\n",
    "    \"\"\"\n",
    "    An internal implementation that stores information about outputs of a Fast R-CNN head,\n",
    "    and provides methods that are used to decode the outputs of a Fast R-CNN head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        box2box_transform,\n",
    "        pred_class_logits,\n",
    "        pred_proposal_deltas,\n",
    "        proposals,\n",
    "        smooth_l1_beta=0.0,\n",
    "        box_reg_loss_type=\"smooth_l1\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            box2box_transform (Box2BoxTransform/Box2BoxTransformRotated):\n",
    "                box2box transform instance for proposal-to-detection transformations.\n",
    "            pred_class_logits (Tensor): A tensor of shape (R, K + 1) storing the predicted class\n",
    "                logits for all R predicted object instances.\n",
    "                Each row corresponds to a predicted object instance.\n",
    "            pred_proposal_deltas (Tensor): A tensor of shape (R, K * B) or (R, B) for\n",
    "                class-specific or class-agnostic regression. It stores the predicted deltas that\n",
    "                transform proposals into final box detections.\n",
    "                B is the box dimension (4 or 5).\n",
    "                When B is 4, each row is [dx, dy, dw, dh (, ....)].\n",
    "                When B is 5, each row is [dx, dy, dw, dh, da (, ....)].\n",
    "            proposals (list[Instances]): A list of N Instances, where Instances i stores the\n",
    "                proposals for image i, in the field \"proposal_boxes\".\n",
    "                When training, each Instances must have ground-truth labels\n",
    "                stored in the field \"gt_classes\" and \"gt_boxes\".\n",
    "                The total number of all instances must be equal to R.\n",
    "            smooth_l1_beta (float): The transition point between L1 and L2 loss in\n",
    "                the smooth L1 loss function. When set to 0, the loss becomes L1. When\n",
    "                set to +inf, the loss becomes constant 0.\n",
    "            box_reg_loss_type (str): Box regression loss type. One of: \"smooth_l1\", \"giou\"\n",
    "        \"\"\"\n",
    "        self.box2box_transform = box2box_transform\n",
    "        self.num_preds_per_image = [len(p) for p in proposals]\n",
    "        self.pred_class_logits = pred_class_logits\n",
    "        self.pred_proposal_deltas = pred_proposal_deltas\n",
    "        self.smooth_l1_beta = smooth_l1_beta\n",
    "        self.box_reg_loss_type = box_reg_loss_type\n",
    "\n",
    "        self.image_shapes = [x.image_size for x in proposals]\n",
    "\n",
    "        if len(proposals):\n",
    "            box_type = type(proposals[0].proposal_boxes)\n",
    "            # cat(..., dim=0) concatenates over all images in the batch\n",
    "            self.proposals = box_type.cat([p.proposal_boxes for p in proposals])\n",
    "            assert (\n",
    "                not self.proposals.tensor.requires_grad\n",
    "            ), \"Proposals should not require gradients!\"\n",
    "\n",
    "            # \"gt_classes\" exists if and only if training. But other gt fields may\n",
    "            # not necessarily exist in training for images that have no groundtruth.\n",
    "            if proposals[0].has(\"gt_classes\"):\n",
    "                self.gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n",
    "\n",
    "                # If \"gt_boxes\" does not exist, the proposals must be all negative and\n",
    "                # should not be included in regression loss computation.\n",
    "                # Here we just use proposal_boxes as an arbitrary placeholder because its\n",
    "                # value won't be used in self.box_reg_loss().\n",
    "                gt_boxes = [\n",
    "                    p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes\n",
    "                    for p in proposals\n",
    "                ]\n",
    "                self.gt_boxes = box_type.cat(gt_boxes)\n",
    "        else:\n",
    "            self.proposals = Boxes(\n",
    "                torch.zeros(0, 4, device=self.pred_proposal_deltas.device)\n",
    "            )\n",
    "        self._no_instances = len(self.proposals) == 0  # no instances found\n",
    "\n",
    "    def softmax_cross_entropy_loss(self):\n",
    "        \"\"\"\n",
    "        Deprecated\n",
    "        \"\"\"\n",
    "        _log_classification_stats(self.pred_class_logits, self.gt_classes)\n",
    "        return cross_entropy(self.pred_class_logits, self.gt_classes, reduction=\"mean\")\n",
    "\n",
    "    def box_reg_loss(self):\n",
    "        \"\"\"\n",
    "        Deprecated\n",
    "        \"\"\"\n",
    "        if self._no_instances:\n",
    "            return 0.0 * self.pred_proposal_deltas.sum()\n",
    "\n",
    "        box_dim = self.proposals.tensor.size(1)  # 4 or 5\n",
    "        cls_agnostic_bbox_reg = self.pred_proposal_deltas.size(1) == box_dim\n",
    "        device = self.pred_proposal_deltas.device\n",
    "\n",
    "        bg_class_ind = self.pred_class_logits.shape[1] - 1\n",
    "        # Box delta loss is only computed between the prediction for the gt class k\n",
    "        # (if 0 <= k < bg_class_ind) and the target; there is no loss defined on predictions\n",
    "        # for non-gt classes and background.\n",
    "        # Empty fg_inds should produce a valid loss of zero because reduction=sum.\n",
    "        fg_inds = nonzero_tuple(\n",
    "            (self.gt_classes >= 0) & (self.gt_classes < bg_class_ind)\n",
    "        )[0]\n",
    "\n",
    "        if cls_agnostic_bbox_reg:\n",
    "            # pred_proposal_deltas only corresponds to foreground class for agnostic\n",
    "            gt_class_cols = torch.arange(box_dim, device=device)\n",
    "        else:\n",
    "            # pred_proposal_deltas for class k are located in columns [b * k : b * k + b],\n",
    "            # where b is the dimension of box representation (4 or 5)\n",
    "            # Note that compared to Detectron1,\n",
    "            # we do not perform bounding box regression for background classes.\n",
    "            gt_class_cols = box_dim * self.gt_classes[fg_inds, None] + torch.arange(\n",
    "                box_dim, device=device\n",
    "            )\n",
    "\n",
    "        if self.box_reg_loss_type == \"smooth_l1\":\n",
    "            gt_proposal_deltas = self.box2box_transform.get_deltas(\n",
    "                self.proposals.tensor, self.gt_boxes.tensor\n",
    "            )\n",
    "            loss_box_reg = smooth_l1_loss(\n",
    "                self.pred_proposal_deltas[fg_inds[:, None], gt_class_cols],\n",
    "                gt_proposal_deltas[fg_inds],\n",
    "                self.smooth_l1_beta,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "        elif self.box_reg_loss_type == \"giou\":\n",
    "            fg_pred_boxes = self.box2box_transform.apply_deltas(\n",
    "                self.pred_proposal_deltas[fg_inds[:, None], gt_class_cols],\n",
    "                self.proposals.tensor[fg_inds],\n",
    "            )\n",
    "            loss_box_reg = giou_loss(\n",
    "                fg_pred_boxes,\n",
    "                self.gt_boxes.tensor[fg_inds],\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid bbox reg loss type '{self.box_reg_loss_type}'\")\n",
    "\n",
    "        loss_box_reg = loss_box_reg / self.gt_classes.numel()\n",
    "        return loss_box_reg\n",
    "\n",
    "    def losses(self):\n",
    "        \"\"\"\n",
    "        Deprecated\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"loss_cls\": self.softmax_cross_entropy_loss(),\n",
    "            \"loss_box_reg\": self.box_reg_loss(),\n",
    "        }\n",
    "\n",
    "    def predict_boxes(self):\n",
    "        \"\"\"\n",
    "        Deprecated\n",
    "        \"\"\"\n",
    "        pred = self.box2box_transform.apply_deltas(\n",
    "            self.pred_proposal_deltas, self.proposals.tensor\n",
    "        )\n",
    "        return pred.split(self.num_preds_per_image, dim=0)\n",
    "\n",
    "    def predict_probs(self):\n",
    "        \"\"\"\n",
    "        Deprecated\n",
    "        \"\"\"\n",
    "        probs = F.softmax(self.pred_class_logits, dim=-1)\n",
    "        return probs.split(self.num_preds_per_image, dim=0)\n",
    "\n",
    "\n",
    "# cross-entropy + variance prediction\n",
    "class FastRCNNCrossEntropyBoundaryVarOutputLayers(FastRCNNOutputLayers):\n",
    "    @configurable\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: ShapeSpec,\n",
    "        *,\n",
    "        box2box_transform,\n",
    "        num_classes: int,\n",
    "        test_score_thresh: float = 0.0,\n",
    "        test_nms_thresh: float = 0.5,\n",
    "        test_topk_per_image: int = 100,\n",
    "        cls_agnostic_bbox_reg: bool = False,\n",
    "        smooth_l1_beta: float = 0.0,\n",
    "        box_reg_loss_type: str = \"smooth_l1\",\n",
    "        box_pseudo_reg_loss_type: str = \"smooth_l1\",\n",
    "        loss_weight: Union[float, Dict[str, float]] = 1.0,\n",
    "        ts_better: float = 0.1,\n",
    "        t_cert: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        NOTE: this interface is experimental.\n",
    "        Args:\n",
    "            input_shape (ShapeSpec): shape of the input feature to this module\n",
    "            box2box_transform (Box2BoxTransform or Box2BoxTransformRotated):\n",
    "            num_classes (int): number of foreground classes\n",
    "            test_score_thresh (float): threshold to filter predictions results.\n",
    "            test_nms_thresh (float): NMS threshold for prediction results.\n",
    "            test_topk_per_image (int): number of top predictions to produce per image.\n",
    "            cls_agnostic_bbox_reg (bool): whether to use class agnostic for bbox regression\n",
    "            smooth_l1_beta (float): transition point from L1 to L2 loss. Only used if\n",
    "                `box_reg_loss_type` is \"smooth_l1\"\n",
    "            box_reg_loss_type (str): Box regression loss type. One of: \"smooth_l1\", \"giou\"\n",
    "            loss_weight (float|dict): weights to use for losses. Can be single float for weighting\n",
    "                all losses, or a dict of individual weightings. Valid dict keys are:\n",
    "                    * \"loss_cls\": applied to classification loss\n",
    "                    * \"loss_box_reg\": applied to box regression loss\n",
    "        \"\"\"\n",
    "        super(FastRCNNOutputLayers, self).__init__()\n",
    "        if isinstance(input_shape, int):  # some backward compatibility\n",
    "            input_shape = ShapeSpec(channels=input_shape)\n",
    "        self.num_classes = num_classes\n",
    "        input_size = (\n",
    "            input_shape.channels * (input_shape.width or 1) * (input_shape.height or 1)\n",
    "        )\n",
    "        # prediction layer for num_classes foreground classes and one background class (hence + 1)\n",
    "        self.cls_score = nn.Linear(input_size, num_classes + 1)\n",
    "        num_bbox_reg_classes = 1 if cls_agnostic_bbox_reg else num_classes\n",
    "        box_dim = len(box2box_transform.weights)\n",
    "        self.bbox_pred = nn.Linear(input_size, num_bbox_reg_classes * box_dim)\n",
    "\n",
    "        # box regression (std)\n",
    "        self.bbox_pred_std = nn.Linear(input_size, num_bbox_reg_classes * box_dim)\n",
    "\n",
    "        nn.init.normal_(self.cls_score.weight, std=0.01)\n",
    "        nn.init.normal_(self.bbox_pred.weight, std=0.001)\n",
    "        nn.init.normal_(self.bbox_pred_std.weight, std=0.0001)  # box regression\n",
    "\n",
    "        for la in [self.cls_score, self.bbox_pred, self.bbox_pred_std]:\n",
    "            nn.init.constant_(la.bias, 0)\n",
    "\n",
    "        self.box2box_transform = box2box_transform\n",
    "        self.smooth_l1_beta = smooth_l1_beta\n",
    "        self.test_score_thresh = test_score_thresh\n",
    "        self.test_nms_thresh = test_nms_thresh\n",
    "        self.test_topk_per_image = test_topk_per_image\n",
    "        self.box_reg_loss_type = box_reg_loss_type\n",
    "        self.box_pseudo_reg_loss_type = box_pseudo_reg_loss_type\n",
    "        if isinstance(loss_weight, float):\n",
    "            loss_weight = {\"loss_cls\": loss_weight, \"loss_box_reg\": loss_weight}\n",
    "        self.loss_weight = loss_weight\n",
    "\n",
    "        # pseudo-labeling\n",
    "        self.ts_better = ts_better\n",
    "        self.t_cert = t_cert\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg, input_shape):\n",
    "        return {\n",
    "            \"input_shape\": input_shape,\n",
    "            \"box2box_transform\": Box2BoxXYXYTransform(\n",
    "                weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS\n",
    "            ),\n",
    "            # fmt: off\n",
    "            \"num_classes\"               : cfg.MODEL.ROI_HEADS.NUM_CLASSES,\n",
    "            \"cls_agnostic_bbox_reg\"     : cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG,\n",
    "            \"smooth_l1_beta\"            : cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA,\n",
    "            \"test_score_thresh\"         : cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST,\n",
    "            \"test_nms_thresh\"           : cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST,\n",
    "            \"test_topk_per_image\"       : cfg.TEST.DETECTIONS_PER_IMAGE,\n",
    "            \"box_reg_loss_type\"         : cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_LOSS_TYPE,\n",
    "            \"box_pseudo_reg_loss_type\"  : cfg.MODEL.ROI_BOX_HEAD.BBOX_PSEUDO_REG_LOSS_TYPE,\n",
    "            \"loss_weight\"               : {\"loss_box_reg\": cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_LOSS_WEIGHT},\n",
    "            \"ts_better\"                 : cfg.SEMISUPNET.TS_BETTER,\n",
    "            \"t_cert\"                    : cfg.SEMISUPNET.T_CERT,\n",
    "            # fmt: on\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: per-region features of shape (N, ...) for N bounding boxes to predict.\n",
    "        Returns:\n",
    "            (Tensor, Tensor):\n",
    "            First tensor: shape (N,K+1), scores for each of the N box. Each row contains the\n",
    "            scores for K object categories and 1 background class.\n",
    "            Second tensor: bounding box regression deltas for each box. Shape is shape (N,Kx4),\n",
    "            or (N,4) for class-agnostic regression.\n",
    "        \"\"\"\n",
    "        if x.dim() > 2:\n",
    "            x = torch.flatten(x, start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        proposal_deltas = self.bbox_pred(x)\n",
    "        proposal_deltas_std = self.bbox_pred_std(x)\n",
    "\n",
    "        return scores, proposal_deltas, proposal_deltas_std\n",
    "\n",
    "    def losses(self, predictions, proposals, branch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were used\n",
    "                to compute predictions. The fields ``proposal_boxes``, ``gt_boxes``,\n",
    "                ``gt_classes`` are expected.\n",
    "        Returns:\n",
    "            Dict[str, Tensor]: dict of losses\n",
    "        \"\"\"\n",
    "        scores, proposal_deltas, proposal_deltas_std = predictions\n",
    "\n",
    "        # parse classification outputs\n",
    "        gt_classes = (\n",
    "            cat([p.gt_classes for p in proposals], dim=0)\n",
    "            if len(proposals)\n",
    "            else torch.empty(0)\n",
    "        )\n",
    "        _log_classification_stats(scores, gt_classes)\n",
    "\n",
    "        # parse box regression outputs\n",
    "        if len(proposals):\n",
    "            proposal_boxes = cat(\n",
    "                [p.proposal_boxes.tensor for p in proposals], dim=0\n",
    "            )  # Nx4\n",
    "            assert (\n",
    "                not proposal_boxes.requires_grad\n",
    "            ), \"Proposals should not require gradients!\"\n",
    "            # If \"gt_boxes\" does not exist, the proposals must be all negative and\n",
    "            # should not be included in regression loss computation.\n",
    "            # Here we just use proposal_boxes as an arbitrary placeholder because its\n",
    "            # value won't be used in self.box_reg_loss().\n",
    "            if branch == \"unsup_data_train\":\n",
    "                gt_loc_std = cat(\n",
    "                    [\n",
    "                        (\n",
    "                            p.gt_loc_std\n",
    "                            if p.has(\"gt_loc_std\")\n",
    "                            else torch.zeros_like(p.proposal_boxes.tensor)\n",
    "                        )\n",
    "                        for p in proposals\n",
    "                    ],\n",
    "                    dim=0,\n",
    "                )\n",
    "\n",
    "            gt_boxes = cat(\n",
    "                [\n",
    "                    (p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor\n",
    "                    for p in proposals\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "        else:\n",
    "            proposal_boxes = gt_boxes = torch.empty(\n",
    "                (0, 4), device=proposal_deltas.device\n",
    "            )\n",
    "\n",
    "        if branch == \"supervised\":\n",
    "            losses = {\n",
    "                \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n",
    "                \"loss_box_reg\": self.box_reg_loss(\n",
    "                    proposal_boxes,\n",
    "                    gt_boxes,\n",
    "                    proposal_deltas,\n",
    "                    proposal_deltas_std,\n",
    "                    gt_classes,\n",
    "                ),\n",
    "            }\n",
    "        elif branch == \"unsup_data_train\":\n",
    "            losses = {\n",
    "                \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n",
    "                \"loss_box_reg\": self.box_reg_pseudo_loss(\n",
    "                    proposal_boxes,\n",
    "                    gt_boxes,\n",
    "                    proposal_deltas,\n",
    "                    proposal_deltas_std,\n",
    "                    gt_loc_std,\n",
    "                    gt_classes,\n",
    "                ),\n",
    "            }\n",
    "        else:\n",
    "            losses = {\n",
    "                \"loss_cls\": cross_entropy(scores, gt_classes, reduction=\"mean\"),\n",
    "                \"loss_box_reg\": self.box_reg_loss(\n",
    "                    proposal_boxes,\n",
    "                    gt_boxes,\n",
    "                    proposal_deltas,\n",
    "                    proposal_deltas_std,\n",
    "                    gt_classes,\n",
    "                ),\n",
    "            }\n",
    "\n",
    "        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n",
    "\n",
    "    def box_reg_loss(\n",
    "        self, proposal_boxes, gt_boxes, pred_deltas, pred_deltas_std, gt_classes\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            All boxes are tensors with the same shape Rx(4 or 5).\n",
    "            gt_classes is a long tensor of shape R, the gt class label of each proposal.\n",
    "            R shall be the number of proposals.\n",
    "        \"\"\"\n",
    "        box_dim = proposal_boxes.shape[1]  # 4 or 5\n",
    "        # Regression loss is only computed for foreground proposals (those matched to a GT)\n",
    "        fg_inds = nonzero_tuple((gt_classes >= 0) & (gt_classes < self.num_classes))[0]\n",
    "        if pred_deltas.shape[1] == box_dim:  # cls-agnostic regression\n",
    "            fg_pred_deltas = pred_deltas[fg_inds]\n",
    "            fg_pred_deltas_std = pred_deltas_std[fg_inds]\n",
    "        else:\n",
    "            fg_pred_deltas = pred_deltas.view(-1, self.num_classes, box_dim)[\n",
    "                fg_inds, gt_classes[fg_inds]\n",
    "            ]\n",
    "            fg_pred_deltas_std = pred_deltas_std.view(-1, self.num_classes, box_dim)[\n",
    "                fg_inds, gt_classes[fg_inds]\n",
    "            ]\n",
    "\n",
    "        if self.box_reg_loss_type == \"smooth_l1\":\n",
    "            gt_pred_deltas = self.box2box_transform.get_deltas(\n",
    "                proposal_boxes[fg_inds],\n",
    "                gt_boxes[fg_inds],\n",
    "            )\n",
    "            loss_box_reg = smooth_l1_loss(\n",
    "                fg_pred_deltas, gt_pred_deltas, self.smooth_l1_beta, reduction=\"sum\"\n",
    "            )\n",
    "        elif self.box_reg_loss_type == \"nlloss\":\n",
    "\n",
    "            # compute iou_weight\n",
    "            fg_pred_boxes = self.box2box_transform.apply_deltas(\n",
    "                fg_pred_deltas, proposal_boxes[fg_inds]\n",
    "            )\n",
    "            # Nx(KxB)\n",
    "            iou_weight = matched_boxlist_iou(\n",
    "                Boxes(gt_boxes[fg_inds]), Boxes(fg_pred_boxes)\n",
    "            )\n",
    "\n",
    "            # compute loss\n",
    "            gt_pred_deltas = self.box2box_transform.get_deltas(\n",
    "                proposal_boxes[fg_inds], gt_boxes[fg_inds]\n",
    "            )\n",
    "            loss_box_nll = nl_loss(\n",
    "                input=fg_pred_deltas,\n",
    "                input_std=fg_pred_deltas_std,\n",
    "                target=gt_pred_deltas,\n",
    "                beta=self.smooth_l1_beta,\n",
    "                iou_weight=iou_weight,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "            # loss_box_iou = giou_loss(fg_pred_boxes, gt_boxes[fg_inds], reduction=\"sum\")\n",
    "            loss_box_l1 = smooth_l1_loss(\n",
    "                fg_pred_deltas, gt_pred_deltas, self.smooth_l1_beta, reduction=\"sum\"\n",
    "            )\n",
    "\n",
    "            loss_box_reg = loss_box_l1 + 0.05 * loss_box_nll\n",
    "        elif self.box_reg_loss_type == \"giou\":\n",
    "            fg_pred_boxes = self.box2box_transform.apply_deltas(\n",
    "                fg_pred_deltas, proposal_boxes[fg_inds]\n",
    "            )\n",
    "            loss_box_reg = giou_loss(fg_pred_boxes, gt_boxes[fg_inds], reduction=\"sum\")\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid bbox reg loss type '{self.box_reg_loss_type}'\")\n",
    "        # The reg loss is normalized using the total number of regions (R), not the number\n",
    "        # of foreground regions even though the box regression loss is only defined on\n",
    "        # foreground regions. Why? Because doing so gives equal training influence to\n",
    "        # each foreground example. To see how, consider two different minibatches:\n",
    "        #  (1) Contains a single foreground region\n",
    "        #  (2) Contains 100 foreground regions\n",
    "        # If we normalize by the number of foreground regions, the single example in\n",
    "        # minibatch (1) will be given 100 times as much influence as each foreground\n",
    "        # example in minibatch (2). Normalizing by the total number of regions, R,\n",
    "        # means that the single example in minibatch (1) and each of the 100 examples\n",
    "        # in minibatch (2) are given equal influence.\n",
    "        return loss_box_reg / max(gt_classes.numel(), 1.0)  # return 0 if empty\n",
    "\n",
    "    def box_reg_pseudo_loss(\n",
    "        self,\n",
    "        proposal_boxes,\n",
    "        gt_boxes,\n",
    "        pred_deltas,\n",
    "        pred_deltas_std,\n",
    "        gt_loc_std,\n",
    "        gt_classes,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            All boxes are tensors with the same shape Rx(4 or 5).\n",
    "            gt_classes is a long tensor of shape R, the gt class label of each proposal.\n",
    "            R shall be the number of proposals.\n",
    "        \"\"\"\n",
    "\n",
    "        box_dim = proposal_boxes.shape[1]  # 4 or 5\n",
    "        # Regression loss is only computed for foreground proposals (those matched to a GT)\n",
    "        fg_inds = nonzero_tuple((gt_classes >= 0) & (gt_classes < self.num_classes))[0]\n",
    "        if pred_deltas.shape[1] == box_dim:  # cls-agnostic regression\n",
    "            fg_pred_deltas = pred_deltas[fg_inds]\n",
    "            fg_pred_deltas_std = pred_deltas_std[fg_inds]\n",
    "        else:\n",
    "            fg_pred_deltas = pred_deltas.view(-1, self.num_classes, box_dim)[\n",
    "                fg_inds, gt_classes[fg_inds]\n",
    "            ]\n",
    "            fg_pred_deltas_std = pred_deltas_std.view(-1, self.num_classes, box_dim)[\n",
    "                fg_inds, gt_classes[fg_inds]\n",
    "            ]\n",
    "\n",
    "        if self.box_pseudo_reg_loss_type == \"smooth_l1\":\n",
    "            gt_pred_deltas = self.box2box_transform.get_deltas(\n",
    "                proposal_boxes[fg_inds],\n",
    "                gt_boxes[fg_inds],\n",
    "            )\n",
    "            loss_box_reg = smooth_l1_loss(\n",
    "                fg_pred_deltas, gt_pred_deltas, self.smooth_l1_beta, reduction=\"sum\"\n",
    "            )\n",
    "        elif self.box_pseudo_reg_loss_type == \"tsbetter\":\n",
    "\n",
    "            gt_pred_deltas = self.box2box_transform.get_deltas(\n",
    "                proposal_boxes[fg_inds],\n",
    "                gt_boxes[fg_inds],\n",
    "            )\n",
    "            gt_bbox_loc_conf = 1 - gt_loc_std[fg_inds].sigmoid()\n",
    "            pred_bbox_loc_conf = 1 - fg_pred_deltas_std.sigmoid()\n",
    "\n",
    "            TS_BETTER = self.ts_better\n",
    "            T_CERT = self.t_cert\n",
    "\n",
    "            tchbetter_idx = (gt_bbox_loc_conf > pred_bbox_loc_conf + TS_BETTER) * (\n",
    "                gt_bbox_loc_conf > T_CERT\n",
    "            )\n",
    "            loss_box_reg = smooth_l1_loss(\n",
    "                fg_pred_deltas[tchbetter_idx],\n",
    "                gt_pred_deltas[tchbetter_idx],\n",
    "                0.0,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid bbox pseudo reg loss type '{self.box_pseudo_reg_loss_type}'\"\n",
    "            )\n",
    "\n",
    "        # The reg loss is normalized using the total number of regions (R), not the number\n",
    "        # of foreground regions even though the box regression loss is only defined on\n",
    "        # foreground regions. Why? Because doing so gives equal training influence to\n",
    "        # each foreground example. To see how, consider two different minibatches:\n",
    "        #  (1) Contains a single foreground region\n",
    "        #  (2) Contains 100 foreground regions\n",
    "        # If we normalize by the number of foreground regions, the single example in\n",
    "        # minibatch (1) will be given 100 times as much influence as each foreground\n",
    "        # example in minibatch (2). Normalizing by the total number of regions, R,\n",
    "        # means that the single example in minibatch (1) and each of the 100 examples\n",
    "        # in minibatch (2) are given equal influence.\n",
    "        return loss_box_reg / max(gt_classes.numel(), 1.0)  # return 0 if empty\n",
    "\n",
    "    def inference(\n",
    "        self, predictions: Tuple[torch.Tensor, torch.Tensor], proposals: List[Instances]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were\n",
    "                used to compute predictions. The ``proposal_boxes`` field is expected.\n",
    "        Returns:\n",
    "            list[Instances]: same as `fast_rcnn_inference`.\n",
    "            list[Tensor]: same as `fast_rcnn_inference`.\n",
    "        \"\"\"\n",
    "        boxes = self.predict_boxes(predictions, proposals)\n",
    "        boxes_std = self.predict_boxes_std(predictions, proposals)\n",
    "        scores = self.predict_probs(predictions, proposals)\n",
    "        image_shapes = [x.image_size for x in proposals]\n",
    "\n",
    "        # NMS: note that localization uncertainties are not used in the inference\n",
    "        nms_results, keep_idx = fast_rcnn_inference(\n",
    "            boxes,\n",
    "            scores,\n",
    "            image_shapes,\n",
    "            self.test_score_thresh,\n",
    "            self.test_nms_thresh,\n",
    "            self.test_topk_per_image,\n",
    "        )\n",
    "\n",
    "        # add additional metrics\n",
    "        for i in range(len(nms_results)):\n",
    "            nms_results[i].pred_boxes_std = boxes_std[i][keep_idx[i]]\n",
    "\n",
    "        return (nms_results, keep_idx)\n",
    "\n",
    "    def predict_boxes_for_gt_classes(self, predictions, proposals):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were used\n",
    "                to compute predictions. The fields ``proposal_boxes``, ``gt_classes`` are expected.\n",
    "        Returns:\n",
    "            list[Tensor]:\n",
    "                A list of Tensors of predicted boxes for GT classes in case of\n",
    "                class-specific box head. Element i of the list has shape (Ri, B), where Ri is\n",
    "                the number of proposals for image i and B is the box dimension (4 or 5)\n",
    "        \"\"\"\n",
    "        if not len(proposals):\n",
    "            return []\n",
    "        scores, proposal_deltas, proposal_deltas_std = predictions\n",
    "        proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n",
    "        N, B = proposal_boxes.shape\n",
    "        predict_boxes = self.box2box_transform.apply_deltas(\n",
    "            proposal_deltas, proposal_boxes\n",
    "        )  # Nx(KxB)\n",
    "\n",
    "        K = predict_boxes.shape[1] // B\n",
    "        if K > 1:\n",
    "            gt_classes = torch.cat([p.gt_classes for p in proposals], dim=0)\n",
    "            # Some proposals are ignored or have a background class. Their gt_classes\n",
    "            # cannot be used as index.\n",
    "            gt_classes = gt_classes.clamp_(0, K - 1)\n",
    "\n",
    "            predict_boxes = predict_boxes.view(N, K, B)[\n",
    "                torch.arange(N, dtype=torch.long, device=predict_boxes.device),\n",
    "                gt_classes,\n",
    "            ]\n",
    "        num_prop_per_image = [len(p) for p in proposals]\n",
    "        return predict_boxes.split(num_prop_per_image)\n",
    "\n",
    "    def predict_boxes(\n",
    "        self, predictions: Tuple[torch.Tensor, torch.Tensor], proposals: List[Instances]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were\n",
    "                used to compute predictions. The ``proposal_boxes`` field is expected.\n",
    "        Returns:\n",
    "            list[Tensor]:\n",
    "                A list of Tensors of predicted class-specific or class-agnostic boxes\n",
    "                for each image. Element i has shape (Ri, K * B) or (Ri, B), where Ri is\n",
    "                the number of proposals for image i and B is the box dimension (4 or 5)\n",
    "        \"\"\"\n",
    "        if not len(proposals):\n",
    "            return []\n",
    "        _, proposal_deltas, proposal_deltas_std = predictions\n",
    "        num_prop_per_image = [len(p) for p in proposals]\n",
    "        proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n",
    "        predict_boxes = self.box2box_transform.apply_deltas(\n",
    "            proposal_deltas,\n",
    "            proposal_boxes,\n",
    "        )  # Nx(KxB)\n",
    "        return predict_boxes.split(num_prop_per_image)\n",
    "\n",
    "    def predict_boxes_std(\n",
    "        self, predictions: Tuple[torch.Tensor, torch.Tensor], proposals: List[Instances]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were\n",
    "                used to compute predictions. The ``proposal_boxes`` field is expected.\n",
    "        Returns:\n",
    "            list[Tensor]:\n",
    "                A list of Tensors of predicted class-specific or class-agnostic boxes\n",
    "                for each image. Element i has shape (Ri, K * B) or (Ri, B), where Ri is\n",
    "                the number of proposals for image i and B is the box dimension (4 or 5)\n",
    "        \"\"\"\n",
    "        if not len(proposals):\n",
    "            return []\n",
    "        _, _, proposal_std = predictions\n",
    "        num_prop_per_image = [len(p) for p in proposals]\n",
    "\n",
    "        return proposal_std.split(num_prop_per_image)\n",
    "\n",
    "    def predict_probs(\n",
    "        self, predictions: Tuple[torch.Tensor, torch.Tensor], proposals: List[Instances]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were\n",
    "                used to compute predictions.\n",
    "        Returns:\n",
    "            list[Tensor]:\n",
    "                A list of Tensors of predicted class probabilities for each image.\n",
    "                Element i has shape (Ri, K + 1), where Ri is the number of proposals for image i.\n",
    "        \"\"\"\n",
    "        scores, _, proposal_deltas_std = predictions\n",
    "        num_inst_per_image = [len(p) for p in proposals]\n",
    "        probs = F.softmax(scores, dim=-1)\n",
    "        return probs.split(num_inst_per_image, dim=0)\n",
    "\n",
    "\n",
    "# focal loss + variance prediction\n",
    "class FastRCNNFocaltLossBoundaryVarOutputLayers(FastRCNNOutputLayers):\n",
    "    @configurable\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: ShapeSpec,\n",
    "        *,\n",
    "        box2box_transform,\n",
    "        num_classes: int,\n",
    "        test_score_thresh: float = 0.0,\n",
    "        test_nms_thresh: float = 0.5,\n",
    "        test_topk_per_image: int = 100,\n",
    "        cls_agnostic_bbox_reg: bool = False,\n",
    "        smooth_l1_beta: float = 0.0,\n",
    "        box_reg_loss_type: str = \"smooth_l1\",\n",
    "        box_pseudo_reg_loss_type: str = \"smooth_l1\",\n",
    "        loss_weight: Union[float, Dict[str, float]] = 1.0,\n",
    "        ts_better: float = 0.1,\n",
    "        t_cert: float = 0.5,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        NOTE: this interface is experimental.\n",
    "        Args:\n",
    "            input_shape (ShapeSpec): shape of the input feature to this module\n",
    "            box2box_transform (Box2BoxTransform or Box2BoxTransformRotated):\n",
    "            num_classes (int): number of foreground classes\n",
    "            test_score_thresh (float): threshold to filter predictions results.\n",
    "            test_nms_thresh (float): NMS threshold for prediction results.\n",
    "            test_topk_per_image (int): number of top predictions to produce per image.\n",
    "            cls_agnostic_bbox_reg (bool): whether to use class agnostic for bbox regression\n",
    "            smooth_l1_beta (float): transition point from L1 to L2 loss. Only used if\n",
    "                `box_reg_loss_type` is \"smooth_l1\"\n",
    "            box_reg_loss_type (str): Box regression loss type. One of: \"smooth_l1\", \"giou\"\n",
    "            loss_weight (float|dict): weights to use for losses. Can be single float for weighting\n",
    "                all losses, or a dict of individual weightings. Valid dict keys are:\n",
    "                    * \"loss_cls\": applied to classification loss\n",
    "                    * \"loss_box_reg\": applied to box regression loss\n",
    "        \"\"\"\n",
    "        super(FastRCNNOutputLayers, self).__init__()\n",
    "        if isinstance(input_shape, int):  # some backward compatibility\n",
    "            input_shape = ShapeSpec(channels=input_shape)\n",
    "        self.num_classes = num_classes\n",
    "        input_size = (\n",
    "            input_shape.channels * (input_shape.width or 1) * (input_shape.height or 1)\n",
    "        )\n",
    "        # prediction layer for num_classes foreground classes and one background class (hence + 1)\n",
    "        self.cls_score = nn.Linear(input_size, num_classes + 1)\n",
    "        num_bbox_reg_classes = 1 if cls_agnostic_bbox_reg else num_classes\n",
    "        box_dim = len(box2box_transform.weights)\n",
    "        self.bbox_pred = nn.Linear(input_size, num_bbox_reg_classes * box_dim)\n",
    "\n",
    "        # box regression (std)\n",
    "        self.bbox_pred_std = nn.Linear(input_size, num_bbox_reg_classes * box_dim)\n",
    "\n",
    "        nn.init.normal_(self.cls_score.weight, std=0.01)\n",
    "        nn.init.normal_(self.bbox_pred.weight, std=0.001)\n",
    "        nn.init.normal_(self.bbox_pred_std.weight, std=0.0001)  # box regression\n",
    "\n",
    "        for la in [self.cls_score, self.bbox_pred, self.bbox_pred_std]:\n",
    "            nn.init.constant_(la.bias, 0)\n",
    "\n",
    "        self.box2box_transform = box2box_transform\n",
    "        self.smooth_l1_beta = smooth_l1_beta\n",
    "        self.test_score_thresh = test_score_thresh\n",
    "        self.test_nms_thresh = test_nms_thresh\n",
    "        self.test_topk_per_image = test_topk_per_image\n",
    "        self.box_reg_loss_type = box_reg_loss_type\n",
    "        self.box_pseudo_reg_loss_type = box_pseudo_reg_loss_type\n",
    "        if isinstance(loss_weight, float):\n",
    "            loss_weight = {\"loss_cls\": loss_weight, \"loss_box_reg\": loss_weight}\n",
    "        self.loss_weight = loss_weight\n",
    "\n",
    "        # pseudo-labeling\n",
    "        self.ts_better = ts_better\n",
    "        self.t_cert = t_cert\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, cfg, input_shape):\n",
    "        return {\n",
    "            \"input_shape\": input_shape,\n",
    "            \"box2box_transform\": Box2BoxXYXYTransform(\n",
    "                weights=cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_WEIGHTS\n",
    "            ),\n",
    "            # fmt: off\n",
    "            \"num_classes\"               : cfg.MODEL.ROI_HEADS.NUM_CLASSES,\n",
    "            \"cls_agnostic_bbox_reg\"     : cfg.MODEL.ROI_BOX_HEAD.CLS_AGNOSTIC_BBOX_REG,\n",
    "            \"smooth_l1_beta\"            : cfg.MODEL.ROI_BOX_HEAD.SMOOTH_L1_BETA,\n",
    "            \"test_score_thresh\"         : cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST,\n",
    "            \"test_nms_thresh\"           : cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST,\n",
    "            \"test_topk_per_image\"       : cfg.TEST.DETECTIONS_PER_IMAGE,\n",
    "            \"box_reg_loss_type\"         : cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_LOSS_TYPE,\n",
    "            \"box_pseudo_reg_loss_type\"  : cfg.MODEL.ROI_BOX_HEAD.BBOX_PSEUDO_REG_LOSS_TYPE,\n",
    "            \"loss_weight\"               : {\"loss_box_reg\": cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_LOSS_WEIGHT},\n",
    "            \"ts_better\"                 : cfg.SEMISUPNET.TS_BETTER,\n",
    "            \"t_cert\"                    : cfg.SEMISUPNET.T_CERT,\n",
    "            # fmt: on\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: per-region features of shape (N, ...) for N bounding boxes to predict.\n",
    "        Returns:\n",
    "            (Tensor, Tensor):\n",
    "            First tensor: shape (N,K+1), scores for each of the N box. Each row contains the\n",
    "            scores for K object categories and 1 background class.\n",
    "            Second tensor: bounding box regression deltas for each box. Shape is shape (N,Kx4),\n",
    "            or (N,4) for class-agnostic regression.\n",
    "        \"\"\"\n",
    "        if x.dim() > 2:\n",
    "            x = torch.flatten(x, start_dim=1)\n",
    "        scores = self.cls_score(x)\n",
    "        proposal_deltas = self.bbox_pred(x)\n",
    "        proposal_deltas_std = self.bbox_pred_std(x)\n",
    "\n",
    "        return scores, proposal_deltas, proposal_deltas_std\n",
    "\n",
    "    def losses(self, predictions, proposals, branch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were used\n",
    "                to compute predictions. The fields ``proposal_boxes``, ``gt_boxes``,\n",
    "                ``gt_classes`` are expected.\n",
    "        Returns:\n",
    "            Dict[str, Tensor]: dict of losses\n",
    "        \"\"\"\n",
    "        scores, proposal_deltas, proposal_deltas_std = predictions\n",
    "\n",
    "        # parse classification outputs\n",
    "        gt_classes = (\n",
    "            cat([p.gt_classes for p in proposals], dim=0)\n",
    "            if len(proposals)\n",
    "            else torch.empty(0)\n",
    "        )\n",
    "        _log_classification_stats(scores, gt_classes)\n",
    "\n",
    "        # parse box regression outputs\n",
    "        if len(proposals):\n",
    "            proposal_boxes = cat(\n",
    "                [p.proposal_boxes.tensor for p in proposals], dim=0\n",
    "            )  # Nx4\n",
    "            assert (\n",
    "                not proposal_boxes.requires_grad\n",
    "            ), \"Proposals should not require gradients!\"\n",
    "            # If \"gt_boxes\" does not exist, the proposals must be all negative and\n",
    "            # should not be included in regression loss computation.\n",
    "            # Here we just use proposal_boxes as an arbitrary placeholder because its\n",
    "            # value won't be used in self.box_reg_loss().\n",
    "            if branch == \"unsup_data_train\":\n",
    "                gt_loc_std = cat(\n",
    "                    [\n",
    "                        (\n",
    "                            p.gt_loc_std\n",
    "                            if p.has(\"gt_loc_std\")\n",
    "                            else torch.zeros_like(p.proposal_boxes.tensor)\n",
    "                        )\n",
    "                        for p in proposals\n",
    "                    ],\n",
    "                    dim=0,\n",
    "                )\n",
    "\n",
    "            gt_boxes = cat(\n",
    "                [\n",
    "                    (p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes).tensor\n",
    "                    for p in proposals\n",
    "                ],\n",
    "                dim=0,\n",
    "            )\n",
    "        else:\n",
    "            proposal_boxes = gt_boxes = torch.empty(\n",
    "                (0, 4), device=proposal_deltas.device\n",
    "            )\n",
    "\n",
    "        if branch == \"supervised\":\n",
    "            losses = {\n",
    "                \"loss_cls\": self.comput_focal_loss(scores, gt_classes),\n",
    "                \"loss_box_reg\": self.box_reg_loss(\n",
    "                    proposal_boxes,\n",
    "                    gt_boxes,\n",
    "                    proposal_deltas,\n",
    "                    proposal_deltas_std,\n",
    "                    gt_classes,\n",
    "                ),\n",
    "            }\n",
    "        elif branch == \"unsup_data_train\":\n",
    "            losses = {\n",
    "                \"loss_cls\": self.comput_focal_loss(scores, gt_classes),\n",
    "                \"loss_box_reg\": self.box_reg_pseudo_loss(\n",
    "                    proposal_boxes,\n",
    "                    gt_boxes,\n",
    "                    proposal_deltas,\n",
    "                    proposal_deltas_std,\n",
    "                    gt_loc_std,\n",
    "                    gt_classes,\n",
    "                ),\n",
    "            }\n",
    "        else:\n",
    "            losses = {\n",
    "                \"loss_cls\": self.comput_focal_loss(scores, gt_classes),\n",
    "                \"loss_box_reg\": self.box_reg_loss(\n",
    "                    proposal_boxes,\n",
    "                    gt_boxes,\n",
    "                    proposal_deltas,\n",
    "                    proposal_deltas_std,\n",
    "                    gt_classes,\n",
    "                ),\n",
    "            }\n",
    "\n",
    "        return {k: v * self.loss_weight.get(k, 1.0) for k, v in losses.items()}\n",
    "\n",
    "    def comput_focal_loss(self, pred_class_logits, gt_classes):\n",
    "        if gt_classes.numel() == 0:\n",
    "            return 0.0 * pred_class_logits.sum()\n",
    "        else:\n",
    "            FC_loss = FocalLoss(\n",
    "                gamma=1.5,\n",
    "                num_classes=self.num_classes,\n",
    "            )\n",
    "            total_loss = FC_loss(input=pred_class_logits, target=gt_classes)\n",
    "            total_loss = total_loss / gt_classes.shape[0]\n",
    "\n",
    "            return total_loss\n",
    "\n",
    "    def box_reg_loss(\n",
    "        self, proposal_boxes, gt_boxes, pred_deltas, pred_deltas_std, gt_classes\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            All boxes are tensors with the same shape Rx(4 or 5).\n",
    "            gt_classes is a long tensor of shape R, the gt class label of each proposal.\n",
    "            R shall be the number of proposals.\n",
    "        \"\"\"\n",
    "        box_dim = proposal_boxes.shape[1]  # 4 or 5\n",
    "        # Regression loss is only computed for foreground proposals (those matched to a GT)\n",
    "        fg_inds = nonzero_tuple((gt_classes >= 0) & (gt_classes < self.num_classes))[0]\n",
    "        if pred_deltas.shape[1] == box_dim:  # cls-agnostic regression\n",
    "            fg_pred_deltas = pred_deltas[fg_inds]\n",
    "            fg_pred_deltas_std = pred_deltas_std[fg_inds]\n",
    "        else:\n",
    "            fg_pred_deltas = pred_deltas.view(-1, self.num_classes, box_dim)[\n",
    "                fg_inds, gt_classes[fg_inds]\n",
    "            ]\n",
    "            fg_pred_deltas_std = pred_deltas_std.view(-1, self.num_classes, box_dim)[\n",
    "                fg_inds, gt_classes[fg_inds]\n",
    "            ]\n",
    "\n",
    "        if self.box_reg_loss_type == \"smooth_l1\":\n",
    "            gt_pred_deltas = self.box2box_transform.get_deltas(\n",
    "                proposal_boxes[fg_inds],\n",
    "                gt_boxes[fg_inds],\n",
    "            )\n",
    "            loss_box_reg = smooth_l1_loss(\n",
    "                fg_pred_deltas, gt_pred_deltas, self.smooth_l1_beta, reduction=\"sum\"\n",
    "            )\n",
    "        elif self.box_reg_loss_type == \"nlloss\":\n",
    "\n",
    "            # compute iou_weight\n",
    "            fg_pred_boxes = self.box2box_transform.apply_deltas(\n",
    "                fg_pred_deltas, proposal_boxes[fg_inds]\n",
    "            )\n",
    "            # Nx(KxB)\n",
    "            iou_weight = matched_boxlist_iou(\n",
    "                Boxes(gt_boxes[fg_inds]), Boxes(fg_pred_boxes)\n",
    "            )\n",
    "\n",
    "            # compute loss\n",
    "            gt_pred_deltas = self.box2box_transform.get_deltas(\n",
    "                proposal_boxes[fg_inds], gt_boxes[fg_inds]\n",
    "            )\n",
    "            loss_box_nll = nl_loss(\n",
    "                input=fg_pred_deltas,\n",
    "                input_std=fg_pred_deltas_std,\n",
    "                target=gt_pred_deltas,\n",
    "                beta=self.smooth_l1_beta,\n",
    "                iou_weight=iou_weight,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "            # loss_box_iou = giou_loss(fg_pred_boxes, gt_boxes[fg_inds], reduction=\"sum\")\n",
    "            loss_box_l1 = smooth_l1_loss(\n",
    "                fg_pred_deltas, gt_pred_deltas, self.smooth_l1_beta, reduction=\"sum\"\n",
    "            )\n",
    "\n",
    "            loss_box_reg = loss_box_l1 + 0.05 * loss_box_nll\n",
    "        elif self.box_reg_loss_type == \"giou\":\n",
    "            fg_pred_boxes = self.box2box_transform.apply_deltas(\n",
    "                fg_pred_deltas, proposal_boxes[fg_inds]\n",
    "            )\n",
    "            loss_box_reg = giou_loss(fg_pred_boxes, gt_boxes[fg_inds], reduction=\"sum\")\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid bbox reg loss type '{self.box_reg_loss_type}'\")\n",
    "        # The reg loss is normalized using the total number of regions (R), not the number\n",
    "        # of foreground regions even though the box regression loss is only defined on\n",
    "        # foreground regions. Why? Because doing so gives equal training influence to\n",
    "        # each foreground example. To see how, consider two different minibatches:\n",
    "        #  (1) Contains a single foreground region\n",
    "        #  (2) Contains 100 foreground regions\n",
    "        # If we normalize by the number of foreground regions, the single example in\n",
    "        # minibatch (1) will be given 100 times as much influence as each foreground\n",
    "        # example in minibatch (2). Normalizing by the total number of regions, R,\n",
    "        # means that the single example in minibatch (1) and each of the 100 examples\n",
    "        # in minibatch (2) are given equal influence.\n",
    "        return loss_box_reg / max(gt_classes.numel(), 1.0)  # return 0 if empty\n",
    "\n",
    "    def box_reg_pseudo_loss(\n",
    "        self,\n",
    "        proposal_boxes,\n",
    "        gt_boxes,\n",
    "        pred_deltas,\n",
    "        pred_deltas_std,\n",
    "        gt_loc_std,\n",
    "        gt_classes,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            All boxes are tensors with the same shape Rx(4 or 5).\n",
    "            gt_classes is a long tensor of shape R, the gt class label of each proposal.\n",
    "            R shall be the number of proposals.\n",
    "        \"\"\"\n",
    "        box_dim = proposal_boxes.shape[1]  # 4 or 5\n",
    "        # Regression loss is only computed for foreground proposals (those matched to a GT)\n",
    "        fg_inds = nonzero_tuple((gt_classes >= 0) & (gt_classes < self.num_classes))[0]\n",
    "        if pred_deltas.shape[1] == box_dim:  # cls-agnostic regression\n",
    "            fg_pred_deltas = pred_deltas[fg_inds]\n",
    "            fg_pred_deltas_std = pred_deltas_std[fg_inds]\n",
    "        else:\n",
    "            fg_pred_deltas = pred_deltas.view(-1, self.num_classes, box_dim)[\n",
    "                fg_inds, gt_classes[fg_inds]\n",
    "            ]\n",
    "            fg_pred_deltas_std = pred_deltas_std.view(-1, self.num_classes, box_dim)[\n",
    "                fg_inds, gt_classes[fg_inds]\n",
    "            ]\n",
    "\n",
    "        if self.box_pseudo_reg_loss_type == \"smooth_l1\":\n",
    "            gt_pred_deltas = self.box2box_transform.get_deltas(\n",
    "                proposal_boxes[fg_inds],\n",
    "                gt_boxes[fg_inds],\n",
    "            )\n",
    "            loss_box_reg = smooth_l1_loss(\n",
    "                fg_pred_deltas, gt_pred_deltas, self.smooth_l1_beta, reduction=\"sum\"\n",
    "            )\n",
    "        elif self.box_pseudo_reg_loss_type == \"tsbetter\":\n",
    "\n",
    "            gt_pred_deltas = self.box2box_transform.get_deltas(\n",
    "                proposal_boxes[fg_inds],\n",
    "                gt_boxes[fg_inds],\n",
    "            )\n",
    "            gt_bbox_loc_conf = 1 - gt_loc_std[fg_inds].sigmoid()\n",
    "            pred_bbox_loc_conf = 1 - fg_pred_deltas_std.sigmoid()\n",
    "\n",
    "            TS_BETTER = self.ts_better\n",
    "            T_CERT = self.t_cert\n",
    "\n",
    "            tchbetter_idx = (gt_bbox_loc_conf > pred_bbox_loc_conf + TS_BETTER) * (\n",
    "                gt_bbox_loc_conf > T_CERT\n",
    "            )\n",
    "            loss_box_reg = smooth_l1_loss(\n",
    "                fg_pred_deltas[tchbetter_idx],\n",
    "                gt_pred_deltas[tchbetter_idx],\n",
    "                0.0,\n",
    "                reduction=\"sum\",\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid bbox pseudo reg loss type '{self.box_pseudo_reg_loss_type}'\"\n",
    "            )\n",
    "\n",
    "        # The reg loss is normalized using the total number of regions (R), not the number\n",
    "        # of foreground regions even though the box regression loss is only defined on\n",
    "        # foreground regions. Why? Because doing so gives equal training influence to\n",
    "        # each foreground example. To see how, consider two different minibatches:\n",
    "        #  (1) Contains a single foreground region\n",
    "        #  (2) Contains 100 foreground regions\n",
    "        # If we normalize by the number of foreground regions, the single example in\n",
    "        # minibatch (1) will be given 100 times as much influence as each foreground\n",
    "        # example in minibatch (2). Normalizing by the total number of regions, R,\n",
    "        # means that the single example in minibatch (1) and each of the 100 examples\n",
    "        # in minibatch (2) are given equal influence.\n",
    "        return loss_box_reg / max(gt_classes.numel(), 1.0)  # return 0 if empty\n",
    "\n",
    "    def inference(\n",
    "        self, predictions: Tuple[torch.Tensor, torch.Tensor], proposals: List[Instances]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were\n",
    "                used to compute predictions. The ``proposal_boxes`` field is expected.\n",
    "        Returns:\n",
    "            list[Instances]: same as `fast_rcnn_inference`.\n",
    "            list[Tensor]: same as `fast_rcnn_inference`.\n",
    "        \"\"\"\n",
    "        boxes = self.predict_boxes(predictions, proposals)\n",
    "        boxes_std = self.predict_boxes_std(predictions, proposals)\n",
    "        scores = self.predict_probs(predictions, proposals)\n",
    "        image_shapes = [x.image_size for x in proposals]\n",
    "\n",
    "        # NMS\n",
    "        nms_results, keep_idx = fast_rcnn_inference(\n",
    "            boxes,\n",
    "            scores,\n",
    "            image_shapes,\n",
    "            self.test_score_thresh,\n",
    "            self.test_nms_thresh,\n",
    "            self.test_topk_per_image,\n",
    "        )\n",
    "\n",
    "        # add additional metrics\n",
    "        for i in range(len(nms_results)):\n",
    "            nms_results[i].pred_boxes_std = boxes_std[i][keep_idx[i]]\n",
    "\n",
    "        return (nms_results, keep_idx)\n",
    "\n",
    "    def predict_boxes_for_gt_classes(self, predictions, proposals):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were used\n",
    "                to compute predictions. The fields ``proposal_boxes``, ``gt_classes`` are expected.\n",
    "        Returns:\n",
    "            list[Tensor]:\n",
    "                A list of Tensors of predicted boxes for GT classes in case of\n",
    "                class-specific box head. Element i of the list has shape (Ri, B), where Ri is\n",
    "                the number of proposals for image i and B is the box dimension (4 or 5)\n",
    "        \"\"\"\n",
    "        if not len(proposals):\n",
    "            return []\n",
    "        scores, proposal_deltas, proposal_deltas_std = predictions\n",
    "        proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n",
    "        N, B = proposal_boxes.shape\n",
    "        predict_boxes = self.box2box_transform.apply_deltas(\n",
    "            proposal_deltas, proposal_boxes\n",
    "        )  # Nx(KxB)\n",
    "\n",
    "        K = predict_boxes.shape[1] // B\n",
    "        if K > 1:\n",
    "            gt_classes = torch.cat([p.gt_classes for p in proposals], dim=0)\n",
    "            # Some proposals are ignored or have a background class. Their gt_classes\n",
    "            # cannot be used as index.\n",
    "            gt_classes = gt_classes.clamp_(0, K - 1)\n",
    "\n",
    "            predict_boxes = predict_boxes.view(N, K, B)[\n",
    "                torch.arange(N, dtype=torch.long, device=predict_boxes.device),\n",
    "                gt_classes,\n",
    "            ]\n",
    "        num_prop_per_image = [len(p) for p in proposals]\n",
    "        return predict_boxes.split(num_prop_per_image)\n",
    "\n",
    "    def predict_boxes(\n",
    "        self, predictions: Tuple[torch.Tensor, torch.Tensor], proposals: List[Instances]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were\n",
    "                used to compute predictions. The ``proposal_boxes`` field is expected.\n",
    "        Returns:\n",
    "            list[Tensor]:\n",
    "                A list of Tensors of predicted class-specific or class-agnostic boxes\n",
    "                for each image. Element i has shape (Ri, K * B) or (Ri, B), where Ri is\n",
    "                the number of proposals for image i and B is the box dimension (4 or 5)\n",
    "        \"\"\"\n",
    "        if not len(proposals):\n",
    "            return []\n",
    "        _, proposal_deltas, proposal_deltas_std = predictions\n",
    "        num_prop_per_image = [len(p) for p in proposals]\n",
    "        proposal_boxes = cat([p.proposal_boxes.tensor for p in proposals], dim=0)\n",
    "        predict_boxes = self.box2box_transform.apply_deltas(\n",
    "            proposal_deltas,\n",
    "            proposal_boxes,\n",
    "        )  # Nx(KxB)\n",
    "        return predict_boxes.split(num_prop_per_image)\n",
    "\n",
    "    def predict_boxes_std(\n",
    "        self, predictions: Tuple[torch.Tensor, torch.Tensor], proposals: List[Instances]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were\n",
    "                used to compute predictions. The ``proposal_boxes`` field is expected.\n",
    "        Returns:\n",
    "            list[Tensor]:\n",
    "                A list of Tensors of predicted class-specific or class-agnostic boxes\n",
    "                for each image. Element i has shape (Ri, K * B) or (Ri, B), where Ri is\n",
    "                the number of proposals for image i and B is the box dimension (4 or 5)\n",
    "        \"\"\"\n",
    "        if not len(proposals):\n",
    "            return []\n",
    "        _, _, proposal_std = predictions\n",
    "        num_prop_per_image = [len(p) for p in proposals]\n",
    "\n",
    "        return proposal_std.split(num_prop_per_image)\n",
    "\n",
    "    def predict_probs(\n",
    "        self, predictions: Tuple[torch.Tensor, torch.Tensor], proposals: List[Instances]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features that were\n",
    "                used to compute predictions.\n",
    "        Returns:\n",
    "            list[Tensor]:\n",
    "                A list of Tensors of predicted class probabilities for each image.\n",
    "                Element i has shape (Ri, K + 1), where Ri is the number of proposals for image i.\n",
    "        \"\"\"\n",
    "\n",
    "        scores, _, proposal_deltas_std = predictions\n",
    "        num_inst_per_image = [len(p) for p in proposals]\n",
    "        probs = F.softmax(scores, dim=-1)\n",
    "        return probs.split(num_inst_per_image, dim=0)\n",
    "\n",
    "\n",
    "def nl_loss(\n",
    "    input: torch.Tensor,\n",
    "    input_std: torch.Tensor,\n",
    "    target: torch.Tensor,\n",
    "    beta: float,\n",
    "    iou_weight: torch.Tensor,\n",
    "    reduction: str = \"none\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Smooth L1 loss defined in the Fast R-CNN paper as:\n",
    "                  | 0.5 * x ** 2 / beta   if abs(x) < beta\n",
    "    smoothl1(x) = |\n",
    "                  | abs(x) - 0.5 * beta   otherwise,\n",
    "    where x = input - target.\n",
    "    Smooth L1 loss is related to Huber loss, which is defined as:\n",
    "                | 0.5 * x ** 2                  if abs(x) < beta\n",
    "     huber(x) = |\n",
    "                | beta * (abs(x) - 0.5 * beta)  otherwise\n",
    "    Smooth L1 loss is equal to huber(x) / beta. This leads to the following\n",
    "    differences:\n",
    "     - As beta -> 0, Smooth L1 loss converges to L1 loss, while Huber loss\n",
    "       converges to a constant 0 loss.\n",
    "     - As beta -> +inf, Smooth L1 converges to a constant 0 loss, while Huber loss\n",
    "       converges to L2 loss.\n",
    "     - For Smooth L1 loss, as beta varies, the L1 segment of the loss has a constant\n",
    "       slope of 1. For Huber loss, the slope of the L1 segment is beta.\n",
    "    Smooth L1 loss can be seen as exactly L1 loss, but with the abs(x) < beta\n",
    "    portion replaced with a quadratic function such that at abs(x) = beta, its\n",
    "    slope is 1. The quadratic segment smooths the L1 loss near x = 0.\n",
    "    Args:\n",
    "        input (Tensor): input tensor of any shape\n",
    "        target (Tensor): target value tensor with the same shape as input\n",
    "        beta (float): L1 to L2 change point.\n",
    "            For beta values < 1e-5, L1 loss is computed.\n",
    "        reduction: 'none' | 'mean' | 'sum'\n",
    "                 'none': No reduction will be applied to the output.\n",
    "                 'mean': The output will be averaged.\n",
    "                 'sum': The output will be summed.\n",
    "    Returns:\n",
    "        The loss with the reduction option applied.\n",
    "    Note:\n",
    "        PyTorch's builtin \"Smooth L1 loss\" implementation does not actually\n",
    "        implement Smooth L1 loss, nor does it implement Huber loss. It implements\n",
    "        the special case of both in which they are equal (beta=1).\n",
    "        See: https://pytorch.org/docs/stable/nn.html#torch.nn.SmoothL1Loss.\n",
    "    \"\"\"\n",
    "\n",
    "    mean = input\n",
    "    sigma = input_std.sigmoid()\n",
    "    sigma_sq = torch.square(sigma)\n",
    "\n",
    "    # smooth l1 ?\n",
    "    # Gradient explosion and predict log(2*sigma) instead?\n",
    "    first_term = torch.square(target - mean) / (2 * sigma_sq)\n",
    "    second_term = 0.5 * torch.log(sigma_sq)\n",
    "    sum_before_iou = (first_term + second_term).sum(dim=1) + 2 * torch.log(\n",
    "        2 * torch.Tensor([math.pi]).cuda()\n",
    "    )\n",
    "    loss_m = sum_before_iou * iou_weight\n",
    "\n",
    "    if reduction == \"mean\":\n",
    "        loss = loss_m.mean()\n",
    "    elif reduction == \"sum\":\n",
    "        loss = loss_m.sum()\n",
    "    return loss\n",
    "\n",
    "\n",
    "# focal loss (ICLR 2021 unbiased teacher)\n",
    "class FastRCNNFocaltLossOutputLayers(FastRCNNOutputLayers):\n",
    "    def __init__(self, cfg, input_shape):\n",
    "        super(FastRCNNFocaltLossOutputLayers, self).__init__(cfg, input_shape)\n",
    "        self.num_classes = cfg.MODEL.ROI_HEADS.NUM_CLASSES\n",
    "\n",
    "    def losses(self, predictions, proposals, branch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            predictions: return values of :meth:`forward()`.\n",
    "            proposals (list[Instances]): proposals that match the features\n",
    "                that were used to compute predictions.\n",
    "        \"\"\"\n",
    "        scores, proposal_deltas = predictions\n",
    "        losses = FastRCNNFocalLoss(\n",
    "            self.box2box_transform,\n",
    "            scores,\n",
    "            proposal_deltas,\n",
    "            proposals,\n",
    "            self.smooth_l1_beta,\n",
    "            self.box_reg_loss_type,\n",
    "            num_classes=self.num_classes,\n",
    "        ).losses()\n",
    "\n",
    "        return losses\n",
    "\n",
    "\n",
    "class FastRCNNFocalLoss(FastRCNNOutputs):\n",
    "    \"\"\"\n",
    "    A class that stores information about outputs of a Fast R-CNN head.\n",
    "    It provides methods that are used to decode the outputs of a Fast R-CNN head.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        box2box_transform,\n",
    "        pred_class_logits,\n",
    "        pred_proposal_deltas,\n",
    "        proposals,\n",
    "        smooth_l1_beta=0.0,\n",
    "        box_reg_loss_type=\"smooth_l1\",\n",
    "        num_classes=80,\n",
    "    ):\n",
    "        self.box2box_transform = box2box_transform\n",
    "        self.num_preds_per_image = [len(p) for p in proposals]\n",
    "        self.pred_class_logits = pred_class_logits\n",
    "        self.pred_proposal_deltas = pred_proposal_deltas\n",
    "        self.smooth_l1_beta = smooth_l1_beta\n",
    "        self.box_reg_loss_type = box_reg_loss_type\n",
    "\n",
    "        self.image_shapes = [x.image_size for x in proposals]\n",
    "\n",
    "        if len(proposals):\n",
    "            box_type = type(proposals[0].proposal_boxes)\n",
    "            # cat(..., dim=0) concatenates over all images in the batch\n",
    "            self.proposals = box_type.cat([p.proposal_boxes for p in proposals])\n",
    "            assert (\n",
    "                not self.proposals.tensor.requires_grad\n",
    "            ), \"Proposals should not require gradients!\"\n",
    "\n",
    "            # \"gt_classes\" exists if and only if training. But other gt fields may\n",
    "            # not necessarily exist in training for images that have no groundtruth.\n",
    "            if proposals[0].has(\"gt_classes\"):\n",
    "                self.gt_classes = cat([p.gt_classes for p in proposals], dim=0)\n",
    "\n",
    "                # If \"gt_boxes\" does not exist, the proposals must be all negative and\n",
    "                # should not be included in regression loss computation.\n",
    "                # Here we just use proposal_boxes as an arbitrary placeholder because its\n",
    "                # value won't be used in self.box_reg_loss().\n",
    "                gt_boxes = [\n",
    "                    p.gt_boxes if p.has(\"gt_boxes\") else p.proposal_boxes\n",
    "                    for p in proposals\n",
    "                ]\n",
    "                self.gt_boxes = box_type.cat(gt_boxes)\n",
    "            if proposals[0].has(\"gt_confid\"):\n",
    "                self.gt_confids = cat([p.gt_confid for p in proposals], dim=0)\n",
    "            else:\n",
    "                self.gt_confids = None\n",
    "        else:\n",
    "            self.proposals = Boxes(\n",
    "                torch.zeros(0, 4, device=self.pred_proposal_deltas.device)\n",
    "            )\n",
    "        self._no_instances = len(self.proposals) == 0  # no instances found\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def losses(self):\n",
    "        return {\n",
    "            \"loss_cls\": self.comput_focal_loss(),\n",
    "            \"loss_box_reg\": self.box_reg_loss(),\n",
    "        }\n",
    "\n",
    "    def comput_focal_loss(self):\n",
    "        if self._no_instances:\n",
    "            return 0.0 * self.pred_class_logits.sum()\n",
    "        else:\n",
    "            FC_loss = FocalLoss(\n",
    "                gamma=1.5,\n",
    "                num_classes=self.num_classes,\n",
    "            )\n",
    "            total_loss = FC_loss(\n",
    "                input=self.pred_class_logits,\n",
    "                target=self.gt_classes,\n",
    "                confid=self.gt_confids,\n",
    "            )\n",
    "            total_loss = total_loss / self.gt_classes.shape[0]\n",
    "\n",
    "            return total_loss\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weight=None,\n",
    "        gamma=1.0,\n",
    "        num_classes=80,\n",
    "    ):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        assert gamma >= 0\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, input, target, confid=None):\n",
    "\n",
    "        # focal loss\n",
    "        CE = F.cross_entropy(input, target, reduction=\"none\")\n",
    "        p = torch.exp(-CE)\n",
    "        loss = (1 - p) ** self.gamma * CE\n",
    "\n",
    "        if confid is not None:\n",
    "            loss = loss * confid\n",
    "\n",
    "        return loss.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "from typing import Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from detectron2.layers import ShapeSpec\n",
    "from detectron2.modeling.poolers import ROIPooler\n",
    "from detectron2.modeling.proposal_generator.proposal_utils import (\n",
    "    add_ground_truth_to_proposals,\n",
    ")\n",
    "from detectron2.modeling.roi_heads import ROI_HEADS_REGISTRY, StandardROIHeads\n",
    "from detectron2.modeling.roi_heads.box_head import build_box_head\n",
    "from detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers\n",
    "from detectron2.structures import Boxes, ImageList, Instances, pairwise_iou\n",
    "from detectron2.utils.events import get_event_storage\n",
    "# from ubteacher.modeling.roi_heads.fast_rcnn import (\n",
    "#     FastRCNNCrossEntropyBoundaryVarOutputLayers,\n",
    "#     FastRCNNFocaltLossBoundaryVarOutputLayers,\n",
    "#     FastRCNNFocaltLossOutputLayers,\n",
    "# )\n",
    "\n",
    "\n",
    "@ROI_HEADS_REGISTRY.register()\n",
    "class StandardROIHeadsPseudoLab(StandardROIHeads):\n",
    "    @classmethod\n",
    "    def _init_box_head(cls, cfg, input_shape):\n",
    "        # fmt: off\n",
    "        in_features       = cfg.MODEL.ROI_HEADS.IN_FEATURES\n",
    "        pooler_resolution = cfg.MODEL.ROI_BOX_HEAD.POOLER_RESOLUTION\n",
    "        pooler_scales     = tuple(1.0 / input_shape[k].stride for k in in_features)\n",
    "        sampling_ratio    = cfg.MODEL.ROI_BOX_HEAD.POOLER_SAMPLING_RATIO\n",
    "        pooler_type       = cfg.MODEL.ROI_BOX_HEAD.POOLER_TYPE\n",
    "        # fmt: on\n",
    "\n",
    "        in_channels = [input_shape[f].channels for f in in_features]\n",
    "        # Check all channel counts are equal\n",
    "        assert len(set(in_channels)) == 1, in_channels\n",
    "        in_channels = in_channels[0]\n",
    "\n",
    "        box_pooler = ROIPooler(\n",
    "            output_size=pooler_resolution,\n",
    "            scales=pooler_scales,\n",
    "            sampling_ratio=sampling_ratio,\n",
    "            pooler_type=pooler_type,\n",
    "        )\n",
    "        box_head = build_box_head(\n",
    "            cfg,\n",
    "            ShapeSpec(\n",
    "                channels=in_channels, height=pooler_resolution, width=pooler_resolution\n",
    "            ),\n",
    "        )\n",
    "        if cfg.MODEL.ROI_HEADS.LOSS == \"CrossEntropy\":\n",
    "            box_predictor = FastRCNNOutputLayers(cfg, box_head.output_shape)\n",
    "        elif cfg.MODEL.ROI_HEADS.LOSS == \"FocalLoss\":\n",
    "            box_predictor = FastRCNNFocaltLossOutputLayers(cfg, box_head.output_shape)\n",
    "        elif cfg.MODEL.ROI_HEADS.LOSS == \"FocalLoss_BoundaryVar\":\n",
    "            box_predictor = FastRCNNFocaltLossBoundaryVarOutputLayers(\n",
    "                cfg, box_head.output_shape\n",
    "            )\n",
    "        elif cfg.MODEL.ROI_HEADS.LOSS == \"CrossEntropy_BoundaryVar\":\n",
    "            box_predictor = FastRCNNCrossEntropyBoundaryVarOutputLayers(\n",
    "                cfg, box_head.output_shape\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown ROI head loss.\")\n",
    "\n",
    "        return {\n",
    "            \"box_in_features\": in_features,\n",
    "            \"box_pooler\": box_pooler,\n",
    "            \"box_head\": box_head,\n",
    "            \"box_predictor\": box_predictor,\n",
    "        }\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        images: ImageList,\n",
    "        features: Dict[str, torch.Tensor],\n",
    "        proposals: List[Instances],\n",
    "        targets: Optional[List[Instances]] = None,\n",
    "        compute_loss=True,\n",
    "        branch=\"\",\n",
    "    ) -> Tuple[List[Instances], Dict[str, torch.Tensor]]:\n",
    "\n",
    "        del images\n",
    "        if self.training and compute_loss:  # apply if training loss\n",
    "            assert targets\n",
    "            # 1000 --> 512\n",
    "            if targets[0].has(\"scores\"):  # has confidence; then weight loss\n",
    "                proposals = self.label_and_sample_proposals_pseudo(\n",
    "                    proposals, targets, branch=branch\n",
    "                )\n",
    "            else:\n",
    "                proposals = self.label_and_sample_proposals(\n",
    "                    proposals, targets, branch=branch\n",
    "                )\n",
    "\n",
    "        del targets\n",
    "\n",
    "        if self.training and compute_loss:\n",
    "            losses, _ = self._forward_box(features, proposals, compute_loss, branch)\n",
    "            return proposals, losses\n",
    "        else:\n",
    "            pred_instances, predictions = self._forward_box(\n",
    "                features, proposals, compute_loss, branch\n",
    "            )\n",
    "\n",
    "            return pred_instances, predictions\n",
    "\n",
    "    def _forward_box(\n",
    "        self,\n",
    "        features: Dict[str, torch.Tensor],\n",
    "        proposals: List[Instances],\n",
    "        compute_loss: bool = True,\n",
    "        branch: str = \"\",\n",
    "    ) -> Union[Dict[str, torch.Tensor], List[Instances]]:\n",
    "        features = [features[f] for f in self.box_in_features]\n",
    "        box_features = self.box_pooler(features, [x.proposal_boxes for x in proposals])\n",
    "        box_features = self.box_head(box_features)\n",
    "        predictions = self.box_predictor(box_features)\n",
    "        del box_features\n",
    "\n",
    "        if self.training and compute_loss:  # apply if training loss or val loss\n",
    "            losses = self.box_predictor.losses(predictions, proposals, branch)\n",
    "\n",
    "            if self.train_on_pred_boxes:\n",
    "                with torch.no_grad():\n",
    "                    pred_boxes = self.box_predictor.predict_boxes_for_gt_classes(\n",
    "                        predictions, proposals\n",
    "                    )\n",
    "                    for proposals_per_image, pred_boxes_per_image in zip(\n",
    "                        proposals, pred_boxes\n",
    "                    ):\n",
    "                        proposals_per_image.proposal_boxes = Boxes(pred_boxes_per_image)\n",
    "            return losses, predictions\n",
    "        else:\n",
    "            pred_instances, _ = self.box_predictor.inference(predictions, proposals)\n",
    "\n",
    "            return pred_instances, predictions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def label_and_sample_proposals(\n",
    "        self, proposals: List[Instances], targets: List[Instances], branch: str = \"\"\n",
    "    ) -> List[Instances]:\n",
    "        gt_boxes = [x.gt_boxes for x in targets]\n",
    "\n",
    "        if self.proposal_append_gt:\n",
    "            proposals = add_ground_truth_to_proposals(gt_boxes, proposals)\n",
    "\n",
    "        proposals_with_gt = []\n",
    "\n",
    "        num_fg_samples = []\n",
    "        num_bg_samples = []\n",
    "        for proposals_per_image, targets_per_image in zip(proposals, targets):\n",
    "            has_gt = len(targets_per_image) > 0\n",
    "            match_quality_matrix = pairwise_iou(\n",
    "                targets_per_image.gt_boxes, proposals_per_image.proposal_boxes\n",
    "            )\n",
    "            matched_idxs, matched_labels = self.proposal_matcher(match_quality_matrix)\n",
    "            sampled_idxs, gt_classes = self._sample_proposals(\n",
    "                matched_idxs, matched_labels, targets_per_image.gt_classes\n",
    "            )\n",
    "\n",
    "            proposals_per_image = proposals_per_image[sampled_idxs]\n",
    "            proposals_per_image.gt_classes = gt_classes\n",
    "\n",
    "            if has_gt:\n",
    "                sampled_targets = matched_idxs[sampled_idxs]\n",
    "                for (trg_name, trg_value) in targets_per_image.get_fields().items():\n",
    "                    if trg_name.startswith(\"gt_\") and not proposals_per_image.has(\n",
    "                        trg_name\n",
    "                    ):\n",
    "                        proposals_per_image.set(trg_name, trg_value[sampled_targets])\n",
    "            else:\n",
    "                gt_boxes = Boxes(\n",
    "                    targets_per_image.gt_boxes.tensor.new_zeros((len(sampled_idxs), 4))\n",
    "                )\n",
    "                proposals_per_image.gt_boxes = gt_boxes\n",
    "\n",
    "            num_bg_samples.append((gt_classes == self.num_classes).sum().item())\n",
    "            num_fg_samples.append(gt_classes.numel() - num_bg_samples[-1])\n",
    "            proposals_with_gt.append(proposals_per_image)\n",
    "\n",
    "        storage = get_event_storage()\n",
    "        storage.put_scalar(\n",
    "            \"roi_head/num_target_fg_samples_\" + branch, np.mean(num_fg_samples)\n",
    "        )\n",
    "        storage.put_scalar(\n",
    "            \"roi_head/num_target_bg_samples_\" + branch, np.mean(num_bg_samples)\n",
    "        )\n",
    "\n",
    "        return proposals_with_gt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def label_and_sample_proposals_pseudo(\n",
    "        self, proposals: List[Instances], targets: List[Instances], branch: str = \"\"\n",
    "    ) -> List[Instances]:\n",
    "        gt_boxes = [x.gt_boxes for x in targets]\n",
    "        gt_confids = [x.scores for x in targets]\n",
    "        if targets[0].has(\"pred_boxes_std\"):\n",
    "            gt_loc_std = [x.pred_boxes_std for x in targets]\n",
    "        else:\n",
    "            gt_loc_std = [None for x in targets]\n",
    "\n",
    "        if self.proposal_append_gt:\n",
    "            proposals = add_ground_truth_to_proposals(gt_boxes, proposals)\n",
    "\n",
    "        proposals_with_gt = []\n",
    "\n",
    "        num_fg_samples = []\n",
    "        num_bg_samples = []\n",
    "\n",
    "        for (\n",
    "            proposals_per_image,\n",
    "            targets_per_image,\n",
    "            confids_per_image,\n",
    "            loc_std_per_image,\n",
    "        ) in zip(proposals, targets, gt_confids, gt_loc_std):\n",
    "            has_gt = len(targets_per_image) > 0\n",
    "            match_quality_matrix = pairwise_iou(\n",
    "                targets_per_image.gt_boxes, proposals_per_image.proposal_boxes\n",
    "            )\n",
    "            matched_idxs, matched_labels = self.proposal_matcher(match_quality_matrix)\n",
    "            sampled_idxs, gt_classes = self._sample_proposals(\n",
    "                matched_idxs, matched_labels, targets_per_image.gt_classes\n",
    "            )\n",
    "\n",
    "            proposals_per_image = proposals_per_image[sampled_idxs]\n",
    "            proposals_per_image.gt_classes = gt_classes\n",
    "\n",
    "            if has_gt:\n",
    "                sampled_targets = matched_idxs[sampled_idxs]\n",
    "                for (trg_name, trg_value) in targets_per_image.get_fields().items():\n",
    "                    if trg_name.startswith(\"gt_\") and not proposals_per_image.has(\n",
    "                        trg_name\n",
    "                    ):\n",
    "                        proposals_per_image.set(trg_name, trg_value[sampled_targets])\n",
    "                proposals_per_image.set(\"gt_confid\", confids_per_image[sampled_targets])\n",
    "                if loc_std_per_image is not None:\n",
    "                    proposals_per_image.set(\n",
    "                        \"gt_loc_std\", loc_std_per_image[sampled_targets, :]\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "                gt_boxes = Boxes(\n",
    "                    targets_per_image.gt_boxes.tensor.new_zeros((len(sampled_idxs), 4))\n",
    "                )\n",
    "                proposals_per_image.gt_boxes = gt_boxes\n",
    "                proposals_per_image.set(\"gt_confid\", torch.zeros_like(sampled_idxs))\n",
    "                if loc_std_per_image is not None:\n",
    "                    proposals_per_image.set(\n",
    "                        \"gt_loc_std\",\n",
    "                        targets_per_image.gt_boxes.tensor.new_zeros(\n",
    "                            (len(sampled_idxs), 4)\n",
    "                        ),\n",
    "                    )\n",
    "\n",
    "            num_bg_samples.append((gt_classes == self.num_classes).sum().item())\n",
    "            num_fg_samples.append(gt_classes.numel() - num_bg_samples[-1])\n",
    "            proposals_with_gt.append(proposals_per_image)\n",
    "\n",
    "        storage = get_event_storage()\n",
    "        storage.put_scalar(\n",
    "            \"roi_head/num_target_fg_samples_\" + branch, np.mean(num_fg_samples)\n",
    "        )\n",
    "        storage.put_scalar(\n",
    "            \"roi_head/num_target_bg_samples_\" + branch, np.mean(num_bg_samples)\n",
    "        )\n",
    "\n",
    "        return proposals_with_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "# for ema scheduler\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "import detectron2.utils.comm as comm\n",
    "import numpy as np\n",
    "import torch\n",
    "from detectron2.engine import DefaultTrainer, hooks, SimpleTrainer, TrainerBase\n",
    "from detectron2.engine.train_loop import AMPTrainer\n",
    "from detectron2.evaluation import (\n",
    "    COCOEvaluator,\n",
    "    DatasetEvaluator,\n",
    "    print_csv_format,\n",
    "    verify_results,\n",
    ")\n",
    "from detectron2.structures import Boxes\n",
    "from detectron2.structures.instances import Instances\n",
    "from detectron2.utils.events import EventStorage\n",
    "from fvcore.nn.precise_bn import get_bn_modules\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "# from ubteacher.checkpoint.detection_checkpoint import DetectionTSCheckpointer\n",
    "\n",
    "# from ubteacher.data.build import (\n",
    "#     build_detection_semisup_train_loader_two_crops,\n",
    "#     build_detection_test_loader,\n",
    "# )\n",
    "# from ubteacher.data.dataset_mapper import DatasetMapperTwoCropSeparate\n",
    "# from ubteacher.evaluation.evaluator import inference_on_dataset\n",
    "# from ubteacher.modeling.meta_arch.ts_ensemble import EnsembleTSModel\n",
    "# from ubteacher.modeling.pseudo_generator import PseudoGenerator\n",
    "# from ubteacher.solver.build import build_lr_scheduler\n",
    "\n",
    "# Unbiased Teacher Trainer for FCOS\n",
    "class UBTeacherTrainer(DefaultTrainer):\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (CfgNode):\n",
    "        Use the custom checkpointer, which loads other backbone models\n",
    "        with matching heuristics.\n",
    "        \"\"\"\n",
    "        cfg = DefaultTrainer.auto_scale_workers(cfg, comm.get_world_size())\n",
    "\n",
    "        # create an student model\n",
    "        model = self.build_model(cfg)\n",
    "        optimizer = self.build_optimizer(cfg, model)\n",
    "\n",
    "        # create an teacher model\n",
    "        model_teacher = self.build_model(cfg)\n",
    "        self.model_teacher = model_teacher\n",
    "        self.model_teacher.eval()\n",
    "\n",
    "        data_loader = self.build_train_loader(cfg)\n",
    "\n",
    "        # For training, wrap with DDP. But don't need this for inference.\n",
    "        if comm.get_world_size() > 1:\n",
    "            model = DistributedDataParallel(\n",
    "                model, device_ids=[comm.get_local_rank()], broadcast_buffers=False\n",
    "            )\n",
    "\n",
    "        TrainerBase.__init__(self)\n",
    "        self._trainer = (AMPTrainer if cfg.SOLVER.AMP.ENABLED else SimpleTrainer)(\n",
    "            model, data_loader, optimizer\n",
    "        )\n",
    "        self.scheduler = self.build_lr_scheduler(cfg, optimizer)\n",
    "\n",
    "        # Ensemble teacher and student model is for model saving and loading\n",
    "        ensem_ts_model = EnsembleTSModel(model_teacher, model)\n",
    "\n",
    "        self.checkpointer = DetectionTSCheckpointer(\n",
    "            ensem_ts_model,\n",
    "            cfg.OUTPUT_DIR,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=self.scheduler,\n",
    "        )\n",
    "        self.start_iter = 0\n",
    "        self.max_iter = cfg.SOLVER.MAX_ITER\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.pseudo_generator = PseudoGenerator(cfg)\n",
    "\n",
    "        self.register_hooks(self.build_hooks())\n",
    "\n",
    "    def resume_or_load(self, resume=True):\n",
    "        \"\"\"\n",
    "        If `resume==True` and `cfg.OUTPUT_DIR` contains the last checkpoint (defined by\n",
    "        a `last_checkpoint` file), resume from the file. Resuming means loading all\n",
    "        available states (eg. optimizer and scheduler) and update iteration counter\n",
    "        from the checkpoint. ``cfg.MODEL.WEIGHTS`` will not be used.\n",
    "\n",
    "        Otherwise, this is considered as an independent training. The method will load model\n",
    "        weights from the file `cfg.MODEL.WEIGHTS` (but will not load other states) and start\n",
    "        from iteration 0.\n",
    "\n",
    "        Args:\n",
    "            resume (bool): whether to do resume or not\n",
    "        \"\"\"\n",
    "        checkpoint = self.checkpointer.resume_or_load(\n",
    "            self.cfg.MODEL.WEIGHTS, resume=resume\n",
    "        )\n",
    "        if resume and self.checkpointer.has_checkpoint():\n",
    "            self.start_iter = checkpoint.get(\"iteration\", -1) + 1\n",
    "            # The checkpoint stores the training iteration that just finished, thus we start\n",
    "            # at the next iteration (or iter zero if there's no checkpoint).\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "\n",
    "        if cfg.TEST.EVALUATOR == \"COCOeval\":\n",
    "            return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "        # elif cfg.TEST.EVALUATOR == \"COCOTIDEeval\":\n",
    "        #     return COCOTIDEEvaluator(dataset_name, cfg, True, output_folder)\n",
    "        else:\n",
    "            return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        mapper = DatasetMapperTwoCropSeparate(cfg, True)\n",
    "        return build_detection_semisup_train_loader_two_crops(cfg, mapper)\n",
    "\n",
    "    @classmethod\n",
    "    def build_lr_scheduler(cls, cfg, optimizer):\n",
    "        return build_lr_scheduler(cfg, optimizer)\n",
    "\n",
    "    def train(self):\n",
    "        self.train_loop(self.start_iter, self.max_iter)\n",
    "        if hasattr(self, \"_last_eval_results\") and comm.is_main_process():\n",
    "            verify_results(self.cfg, self._last_eval_results)\n",
    "            return self._last_eval_results\n",
    "\n",
    "    def train_loop(self, start_iter: int, max_iter: int):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.info(\"Starting training from iteration {}\".format(start_iter))\n",
    "\n",
    "        self.iter = self.start_iter = start_iter\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        with EventStorage(start_iter) as self.storage:\n",
    "            try:\n",
    "                self.before_train()\n",
    "\n",
    "                for self.iter in range(start_iter, max_iter):\n",
    "                    self.before_step()\n",
    "                    self.run_step_full_semisup()\n",
    "                    self.after_step()\n",
    "            except Exception:\n",
    "                logger.exception(\"Exception during training:\")\n",
    "                raise\n",
    "            finally:\n",
    "                self.after_train()\n",
    "\n",
    "    # =====================================================\n",
    "    # ================== Pseduo-labeling ==================\n",
    "    # =====================================================\n",
    "    def remove_label(self, label_data):\n",
    "        for label_datum in label_data:\n",
    "            if \"instances\" in label_datum.keys():\n",
    "                del label_datum[\"instances\"]\n",
    "        return label_data\n",
    "\n",
    "    def add_label(self, unlabled_data, label, labeltype=\"\"):\n",
    "        for unlabel_datum, lab_inst in zip(unlabled_data, label):\n",
    "            if labeltype == \"class\":\n",
    "                unlabel_datum[\"instances_class\"] = lab_inst\n",
    "            elif labeltype == \"reg\":\n",
    "                unlabel_datum[\"instances_reg\"] = lab_inst\n",
    "            else:\n",
    "                unlabel_datum[\"instances\"] = lab_inst\n",
    "        return unlabled_data\n",
    "\n",
    "    # =====================================================\n",
    "    # =================== Training Flow ===================\n",
    "    # =====================================================\n",
    "\n",
    "    def run_step_full_semisup(self):\n",
    "        self._trainer.iter = self.iter\n",
    "        assert self.model.training, \"[UBTeacherTrainer] model was changed to eval mode!\"\n",
    "        start = time.perf_counter()\n",
    "        data = next(self._trainer._data_loader_iter)\n",
    "        # data_q and data_k from different augmentations (q:strong, k:weak)\n",
    "        # label_strong, label_weak, unlabed_strong, unlabled_weak\n",
    "        label_data_q, label_data_k, unlabel_data_q, unlabel_data_k = data\n",
    "        data_time = time.perf_counter() - start\n",
    "        # burn-in stage (supervised training with labeled data)\n",
    "        if self.iter < self.cfg.SEMISUPNET.BURN_UP_STEP:\n",
    "            # input both strong and weak supervised data into model\n",
    "            label_data_q.extend(label_data_k)\n",
    "            if self.cfg.SOLVER.AMP.ENABLED:\n",
    "                with autocast():\n",
    "                    record_dict = self.model(label_data_q, branch=\"labeled\")\n",
    "            else:\n",
    "                record_dict = self.model(label_data_q, branch=\"labeled\")\n",
    "\n",
    "            # weight losses\n",
    "            loss_dict = {}\n",
    "            for key in record_dict.keys():\n",
    "                if key[:4] == \"loss\" and key[-3:] != \"val\":\n",
    "                    loss_dict[key] = record_dict[key]\n",
    "\n",
    "            if self.cfg.SOLVER.AMP.ENABLED:\n",
    "                with autocast():\n",
    "                    losses = sum(loss_dict.values())\n",
    "            else:\n",
    "                losses = sum(loss_dict.values())\n",
    "\n",
    "        else:\n",
    "            if self.iter == self.cfg.SEMISUPNET.BURN_UP_STEP:\n",
    "                self._update_teacher_model(keep_rate=0.00)\n",
    "                ema_keep_rate = self.cfg.SEMISUPNET.EMA_KEEP_RATE\n",
    "\n",
    "            elif (\n",
    "                self.iter - self.cfg.SEMISUPNET.BURN_UP_STEP\n",
    "            ) % self.cfg.SEMISUPNET.TEACHER_UPDATE_ITER == 0:\n",
    "\n",
    "                ema_keep_rate = self.cfg.SEMISUPNET.EMA_KEEP_RATE\n",
    "                self._update_teacher_model(keep_rate=ema_keep_rate)\n",
    "\n",
    "            record_dict = {}\n",
    "            record_dict[\"ema_rate_1000x\"] = ema_keep_rate * 1000\n",
    "            # generate the pseudo-label using teacher model\n",
    "            # note that we do not convert to eval mode, as 1) there is no gradient computed in\n",
    "            # teacher model and 2) batch norm layers are not updated as well\n",
    "\n",
    "            # produce raw prediction from teacher and predicted box after NMS (NMS_CRITERIA_TRAIN)\n",
    "            with torch.no_grad():\n",
    "                pred_teacher, raw_pred_teacher = self.model_teacher(\n",
    "                    unlabel_data_k,\n",
    "                    output_raw=True,\n",
    "                    nms_method=self.cfg.MODEL.FCOS.NMS_CRITERIA_TRAIN,\n",
    "                    branch=\"teacher_weak\",\n",
    "                )\n",
    "\n",
    "            # use the above raw teacher prediction and perform another NMS (NMS_CRITERIA_REG_TRAIN)\n",
    "            pred_teacher_loc = self.pseudo_generator.nms_from_dense(\n",
    "                raw_pred_teacher, self.cfg.MODEL.FCOS.NMS_CRITERIA_REG_TRAIN\n",
    "            )\n",
    "\n",
    "            # set up threshold for pseudo-labeling\n",
    "            ## pseudo-labeling for classification pseudo-labels\n",
    "            if self.cfg.SEMISUPNET.PSEUDO_BBOX_SAMPLE == \"thresholding\":\n",
    "                cur_threshold = self.cfg.SEMISUPNET.BBOX_THRESHOLD\n",
    "            elif self.cfg.SEMISUPNET.PSEUDO_BBOX_SAMPLE == \"thresholding_cls_ctr\":\n",
    "                cur_threshold = (\n",
    "                    self.cfg.SEMISUPNET.BBOX_THRESHOLD,\n",
    "                    self.cfg.SEMISUPNET.BBOX_CTR_THRESHOLD,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            ## pseudo-labeling for regression pseudo-labels\n",
    "            if self.cfg.SEMISUPNET.PSEUDO_BBOX_SAMPLE_REG == \"thresholding\":\n",
    "                cur_threshold_reg = self.cfg.SEMISUPNET.BBOX_THRESHOLD_REG\n",
    "            elif self.cfg.SEMISUPNET.PSEUDO_BBOX_SAMPLE_REG == \"thresholding_cls_ctr\":\n",
    "                cur_threshold_reg = (\n",
    "                    self.cfg.SEMISUPNET.BBOX_THRESHOLD_REG,\n",
    "                    self.cfg.SEMISUPNET.BBOX_CTR_THRESHOLD_REG,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            # produce pseudo-labels\n",
    "            joint_proposal_dict = {}\n",
    "\n",
    "            # classification\n",
    "            (\n",
    "                pesudo_proposals_roih_unsup_k,\n",
    "                _,\n",
    "            ) = self.pseudo_generator.process_pseudo_label(\n",
    "                pred_teacher,\n",
    "                cur_threshold,\n",
    "                \"roih\",\n",
    "                self.cfg.SEMISUPNET.PSEUDO_BBOX_SAMPLE,\n",
    "            )\n",
    "            joint_proposal_dict[\"proposals_pseudo_cls\"] = pesudo_proposals_roih_unsup_k\n",
    "\n",
    "            # regression\n",
    "            (\n",
    "                pesudo_proposals_roih_unsup_k_reg,\n",
    "                _,\n",
    "            ) = self.pseudo_generator.process_pseudo_label(\n",
    "                pred_teacher_loc,\n",
    "                cur_threshold_reg,\n",
    "                \"roih\",\n",
    "                self.cfg.SEMISUPNET.PSEUDO_BBOX_SAMPLE_REG,\n",
    "            )\n",
    "            joint_proposal_dict[\n",
    "                \"proposals_pseudo_reg\"\n",
    "            ] = pesudo_proposals_roih_unsup_k_reg\n",
    "\n",
    "            #  remove ground-truth labels from unlabeled data\n",
    "            unlabel_data_q = self.remove_label(unlabel_data_q)\n",
    "            unlabel_data_k = self.remove_label(unlabel_data_k)\n",
    "\n",
    "            #  add pseudo-label to unlabeled data\n",
    "            unlabel_data_q = self.add_label(\n",
    "                unlabel_data_q, joint_proposal_dict[\"proposals_pseudo_cls\"], \"class\"\n",
    "            )\n",
    "            unlabel_data_k = self.add_label(\n",
    "                unlabel_data_k, joint_proposal_dict[\"proposals_pseudo_cls\"], \"class\"\n",
    "            )\n",
    "\n",
    "            unlabel_data_q = self.add_label(\n",
    "                unlabel_data_q, joint_proposal_dict[\"proposals_pseudo_reg\"], \"reg\"\n",
    "            )\n",
    "            unlabel_data_k = self.add_label(\n",
    "                unlabel_data_k, joint_proposal_dict[\"proposals_pseudo_reg\"], \"reg\"\n",
    "            )\n",
    "\n",
    "            all_label_data = label_data_q + label_data_k\n",
    "            all_unlabel_data = unlabel_data_q\n",
    "\n",
    "            if self.cfg.SOLVER.AMP.ENABLED:\n",
    "                with autocast():\n",
    "                    record_all_label_data = self.model(all_label_data, branch=\"labeled\")\n",
    "            else:\n",
    "                record_all_label_data = self.model(all_label_data, branch=\"labeled\")\n",
    "            record_dict.update(record_all_label_data)\n",
    "\n",
    "            # unlabeled data pseudo-labeling\n",
    "            for unlabel_data in all_unlabel_data:\n",
    "                assert (\n",
    "                    len(unlabel_data) != 0\n",
    "                ), \"unlabeled data must have at least one pseudo-box\"\n",
    "\n",
    "            if self.cfg.SOLVER.AMP.ENABLED:\n",
    "                with autocast():\n",
    "                    (\n",
    "                        record_all_unlabel_data,\n",
    "                        raw_pred_student,\n",
    "                        instance_reg,\n",
    "                    ) = self.model(\n",
    "                        all_unlabel_data,\n",
    "                        output_raw=True,\n",
    "                        ignore_near=self.cfg.SEMISUPNET.PSEUDO_CLS_IGNORE_NEAR,\n",
    "                        branch=\"unlabeled\",\n",
    "                    )\n",
    "            else:\n",
    "                record_all_unlabel_data, raw_pred_student, instance_reg = self.model(\n",
    "                    all_unlabel_data,\n",
    "                    output_raw=True,\n",
    "                    ignore_near=self.cfg.SEMISUPNET.PSEUDO_CLS_IGNORE_NEAR,\n",
    "                    branch=\"unlabeled\",\n",
    "                )\n",
    "\n",
    "            new_record_all_unlabel_data = {}\n",
    "            for key in record_all_unlabel_data.keys():\n",
    "                new_record_all_unlabel_data[key + \"_pseudo\"] = record_all_unlabel_data[\n",
    "                    key\n",
    "                ]\n",
    "            record_dict.update(new_record_all_unlabel_data)\n",
    "\n",
    "            # weight losses\n",
    "            loss_loc_unsup_list = [\n",
    "                \"loss_fcos_loc_pseudo\",\n",
    "            ]\n",
    "            loss_ctr_unsup_list = [\n",
    "                \"loss_fcos_ctr_pseudo\",\n",
    "            ]\n",
    "            loss_cls_unsup_list = [\n",
    "                \"loss_fcos_cls_pseudo\",\n",
    "            ]\n",
    "            loss_loc_sup_list = [\n",
    "                \"loss_fcos_loc\",\n",
    "            ]\n",
    "            loss_ctr_sup_list = [\n",
    "                \"loss_fcos_ctr\",\n",
    "            ]\n",
    "            loss_cls_sup_list = [\n",
    "                \"loss_fcos_cls\",\n",
    "            ]\n",
    "\n",
    "            loss_dict = {}\n",
    "            for key in record_dict.keys():\n",
    "                if key[:4] == \"loss\":\n",
    "                    if (\n",
    "                        key in loss_ctr_sup_list + loss_cls_sup_list\n",
    "                    ):  # supervised classification + centerness loss\n",
    "                        loss_dict[key] = record_dict[key] / (\n",
    "                            self.cfg.SEMISUPNET.UNSUP_LOSS_WEIGHT + 1.0\n",
    "                        )\n",
    "                    elif (\n",
    "                        key in loss_ctr_unsup_list + loss_cls_unsup_list\n",
    "                    ):  # unsupervised classifciation + centerness loss\n",
    "                        loss_dict[key] = (\n",
    "                            record_dict[key]\n",
    "                            * self.cfg.SEMISUPNET.UNSUP_LOSS_WEIGHT\n",
    "                            / (self.cfg.SEMISUPNET.UNSUP_LOSS_WEIGHT + 1.0)\n",
    "                        )\n",
    "\n",
    "                    elif key in loss_loc_sup_list:  # supervised regression loss\n",
    "                        loss_dict[key] = record_dict[key] / (\n",
    "                            self.cfg.SEMISUPNET.UNSUP_REG_LOSS_WEIGHT + 1.0\n",
    "                        )\n",
    "                    elif key in loss_loc_unsup_list:  # unsupervised regression loss\n",
    "                        loss_dict[key] = (\n",
    "                            record_dict[key]\n",
    "                            * self.cfg.SEMISUPNET.UNSUP_REG_LOSS_WEIGHT\n",
    "                            / (self.cfg.SEMISUPNET.UNSUP_REG_LOSS_WEIGHT + 1.0)\n",
    "                        )\n",
    "\n",
    "                    else:  # supervised loss\n",
    "                        loss_dict[key] = record_dict[key] / (\n",
    "                            self.cfg.SEMISUPNET.UNSUP_LOSS_WEIGHT + 1.0\n",
    "                        )\n",
    "\n",
    "            if self.cfg.SOLVER.AMP.ENABLED:\n",
    "                with autocast():\n",
    "                    losses = sum(loss_dict.values())\n",
    "            else:\n",
    "                losses = sum(loss_dict.values())\n",
    "\n",
    "        metrics_dict = record_dict\n",
    "        metrics_dict[\"data_time\"] = data_time\n",
    "        self._write_metrics(metrics_dict)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        if self.cfg.SOLVER.AMP.ENABLED:\n",
    "            self._trainer.grad_scaler.scale(losses).backward()\n",
    "            self._trainer.grad_scaler.step(self.optimizer)\n",
    "            self._trainer.grad_scaler.update()\n",
    "        else:\n",
    "            losses.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "    def _write_metrics(self, metrics_dict):\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "\n",
    "        # gather metrics among all workers for logging\n",
    "        # This assumes we do DDP-style training, which is currently the only\n",
    "        # supported method in detectron2.\n",
    "        all_metrics_dict = comm.gather(metrics_dict)\n",
    "        # all_hg_dict = comm.gather(hg_dict)\n",
    "\n",
    "        if comm.is_main_process():\n",
    "            if \"data_time\" in all_metrics_dict[0]:\n",
    "                # data_time among workers can have high variance. The actual latency\n",
    "                # caused by data_time is the maximum among workers.\n",
    "                data_time = np.max([x.pop(\"data_time\") for x in all_metrics_dict])\n",
    "                self.storage.put_scalar(\"data_time\", data_time)\n",
    "\n",
    "            # average the rest metrics\n",
    "            metrics_dict = {\n",
    "                k: np.mean([x[k] for x in all_metrics_dict])\n",
    "                for k in all_metrics_dict[0].keys()\n",
    "            }\n",
    "\n",
    "            # append the list\n",
    "            loss_dict = {}\n",
    "            for key in metrics_dict.keys():\n",
    "                if key[:4] == \"loss\":\n",
    "                    loss_dict[key] = metrics_dict[key]\n",
    "\n",
    "            total_losses_reduced = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            self.storage.put_scalar(\"total_loss\", total_losses_reduced)\n",
    "            if len(metrics_dict) > 1:\n",
    "                self.storage.put_scalars(**metrics_dict)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _update_teacher_model(self, keep_rate=0.996):\n",
    "        if comm.get_world_size() > 1:\n",
    "            student_model_dict = {\n",
    "                key[7:]: value for key, value in self.model.state_dict().items()\n",
    "            }\n",
    "        else:\n",
    "            student_model_dict = self.model.state_dict()\n",
    "\n",
    "        new_teacher_dict = OrderedDict()\n",
    "        for key, value in self.model_teacher.state_dict().items():\n",
    "            if key in student_model_dict.keys():\n",
    "                new_teacher_dict[key] = (\n",
    "                    student_model_dict[key] * (1 - keep_rate) + value * keep_rate\n",
    "                )\n",
    "            else:\n",
    "                raise Exception(\"{} is not found in student model\".format(key))\n",
    "\n",
    "        self.model_teacher.load_state_dict(new_teacher_dict)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _copy_main_model(self):\n",
    "        # initialize all parameters\n",
    "        if comm.get_world_size() > 1:\n",
    "            rename_model_dict = {\n",
    "                key[7:]: value for key, value in self.model.state_dict().items()\n",
    "            }\n",
    "            self.model_teacher.load_state_dict(rename_model_dict)\n",
    "        else:\n",
    "            self.model_teacher.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        return build_detection_test_loader(cfg, dataset_name)\n",
    "\n",
    "    def build_hooks(self):\n",
    "        cfg = self.cfg.clone()\n",
    "        cfg.defrost()\n",
    "        cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN\n",
    "\n",
    "        ret = [\n",
    "            hooks.IterationTimer(),\n",
    "            hooks.LRScheduler(self.optimizer, self.scheduler),\n",
    "            hooks.PreciseBN(\n",
    "                # Run at the same freq as (but before) evaluation.\n",
    "                cfg.TEST.EVAL_PERIOD,\n",
    "                self.model,\n",
    "                # Build a new data loader to not affect training\n",
    "                self.build_train_loader(cfg),\n",
    "                cfg.TEST.PRECISE_BN.NUM_ITER,\n",
    "            )\n",
    "            if cfg.TEST.PRECISE_BN.ENABLED and get_bn_modules(self.model)\n",
    "            else None,\n",
    "        ]\n",
    "\n",
    "        # Do PreciseBN before checkpointer, because it updates the model and need to\n",
    "        # be saved by checkpointer.\n",
    "        # This is not always the best: if checkpointing has a different frequency,\n",
    "        # some checkpoints may have more precise statistics than others.\n",
    "        if comm.is_main_process():\n",
    "            ret.append(\n",
    "                hooks.PeriodicCheckpointer(\n",
    "                    self.checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD\n",
    "                )\n",
    "            )\n",
    "\n",
    "        def test_and_save_results_student():\n",
    "            self._last_eval_results_student = self.test(self.cfg, self.model)\n",
    "            _last_eval_results_student = {\n",
    "                k + \"_student\": self._last_eval_results_student[k]\n",
    "                for k in self._last_eval_results_student.keys()\n",
    "            }\n",
    "            return _last_eval_results_student\n",
    "\n",
    "        def test_and_save_results_teacher():\n",
    "            self._last_eval_results_teacher = self.test(self.cfg, self.model_teacher)\n",
    "            return self._last_eval_results_teacher\n",
    "\n",
    "        ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results_student))\n",
    "        ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results_teacher))\n",
    "\n",
    "        if comm.is_main_process():\n",
    "            # run writers in the end, so that evaluation metrics are written\n",
    "            ret.append(hooks.PeriodicWriter(self.build_writers(), period=20))\n",
    "        return ret\n",
    "\n",
    "    @classmethod\n",
    "    def test(cls, cfg, model, evaluators=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (CfgNode):\n",
    "            model (nn.Module):\n",
    "            evaluators (list[DatasetEvaluator] or None): if None, will call\n",
    "                :meth:`build_evaluator`. Otherwise, must have the same length as\n",
    "                ``cfg.DATASETS.TEST``.\n",
    "        Returns:\n",
    "            dict: a dict of result metrics\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        if isinstance(evaluators, DatasetEvaluator):\n",
    "            evaluators = [evaluators]\n",
    "        if evaluators is not None:\n",
    "            assert len(cfg.DATASETS.TEST) == len(evaluators), \"{} != {}\".format(\n",
    "                len(cfg.DATASETS.TEST), len(evaluators)\n",
    "            )\n",
    "\n",
    "        results = OrderedDict()\n",
    "        for idx, dataset_name in enumerate(cfg.DATASETS.TEST):\n",
    "            data_loader = cls.build_test_loader(cfg, dataset_name)\n",
    "            # When evaluators are passed in as arguments,\n",
    "            # implicitly assume that evaluators can be created before data_loader.\n",
    "            if evaluators is not None:\n",
    "                evaluator = evaluators[idx]\n",
    "            else:\n",
    "                try:\n",
    "                    evaluator = cls.build_evaluator(cfg, dataset_name)\n",
    "                except NotImplementedError:\n",
    "                    logger.warn(\n",
    "                        \"No evaluator found. Use `DefaultTrainer.test(evaluators=)`, \"\n",
    "                        \"or implement its `build_evaluator` method.\"\n",
    "                    )\n",
    "                    results[dataset_name] = {}\n",
    "                    continue\n",
    "            results_i = inference_on_dataset(model, data_loader, evaluator, cfg)\n",
    "            # results_i = inference_on_dataset(model, data_loader, evaluator)\n",
    "\n",
    "            results[dataset_name] = results_i\n",
    "            if comm.is_main_process():\n",
    "                assert isinstance(\n",
    "                    results_i, dict\n",
    "                ), \"Evaluator must return a dict on the main process. Got {} instead.\".format(\n",
    "                    results_i\n",
    "                )\n",
    "                logger.info(\n",
    "                    \"Evaluation results for {} in csv format:\".format(dataset_name)\n",
    "                )\n",
    "                print_csv_format(results_i)\n",
    "\n",
    "        if len(results) == 1:\n",
    "            results = list(results.values())[0]\n",
    "        return results\n",
    "\n",
    "\n",
    "# Unbiased Teacher Trainer for Faster RCNN\n",
    "class UBRCNNTeacherTrainer(DefaultTrainer):\n",
    "    def __init__(self, cfg):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (CfgNode):\n",
    "        Use the custom checkpointer, which loads other backbone models\n",
    "        with matching heuristics.\n",
    "        \"\"\"\n",
    "        cfg = DefaultTrainer.auto_scale_workers(cfg, comm.get_world_size())\n",
    "        data_loader = self.build_train_loader(cfg)\n",
    "\n",
    "        # create an student model\n",
    "        model = self.build_model(cfg)\n",
    "        optimizer = self.build_optimizer(cfg, model)\n",
    "\n",
    "        # create an teacher model\n",
    "        model_teacher = self.build_model(cfg)\n",
    "        self.model_teacher = model_teacher\n",
    "\n",
    "        # For training, wrap with DDP. But don't need this for inference.\n",
    "        if comm.get_world_size() > 1:\n",
    "            model = DistributedDataParallel(\n",
    "                model, device_ids=[comm.get_local_rank()], broadcast_buffers=False\n",
    "            )\n",
    "\n",
    "        TrainerBase.__init__(self)\n",
    "        self._trainer = (AMPTrainer if cfg.SOLVER.AMP.ENABLED else SimpleTrainer)(\n",
    "            model, data_loader, optimizer\n",
    "        )\n",
    "        self.scheduler = self.build_lr_scheduler(cfg, optimizer)\n",
    "\n",
    "        # Ensemble teacher and student model is for model saving and loading\n",
    "        ensem_ts_model = EnsembleTSModel(model_teacher, model)\n",
    "\n",
    "        self.checkpointer = DetectionTSCheckpointer(\n",
    "            ensem_ts_model,\n",
    "            cfg.OUTPUT_DIR,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=self.scheduler,\n",
    "        )\n",
    "        self.start_iter = 0\n",
    "        self.max_iter = cfg.SOLVER.MAX_ITER\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.register_hooks(self.build_hooks())\n",
    "\n",
    "    def resume_or_load(self, resume=True):\n",
    "        \"\"\"\n",
    "        If `resume==True` and `cfg.OUTPUT_DIR` contains the last checkpoint (defined by\n",
    "        a `last_checkpoint` file), resume from the file. Resuming means loading all\n",
    "        available states (eg. optimizer and scheduler) and update iteration counter\n",
    "        from the checkpoint. ``cfg.MODEL.WEIGHTS`` will not be used.\n",
    "\n",
    "        Otherwise, this is considered as an independent training. The method will load model\n",
    "        weights from the file `cfg.MODEL.WEIGHTS` (but will not load other states) and start\n",
    "        from iteration 0.\n",
    "\n",
    "        Args:\n",
    "            resume (bool): whether to do resume or not\n",
    "        \"\"\"\n",
    "        checkpoint = self.checkpointer.resume_or_load(\n",
    "            self.cfg.MODEL.WEIGHTS, resume=resume\n",
    "        )\n",
    "        if resume and self.checkpointer.has_checkpoint():\n",
    "            self.start_iter = checkpoint.get(\"iteration\", -1) + 1\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "\n",
    "        if cfg.TEST.EVALUATOR == \"COCOeval\":\n",
    "            return COCOEvaluator(dataset_name, cfg, True, output_folder)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown test evaluator.\")\n",
    "\n",
    "    @classmethod\n",
    "    def build_train_loader(cls, cfg):\n",
    "        mapper = DatasetMapperTwoCropSeparate(cfg, True)\n",
    "        return build_detection_semisup_train_loader_two_crops(cfg, mapper)\n",
    "\n",
    "    @classmethod\n",
    "    def build_lr_scheduler(cls, cfg, optimizer):\n",
    "        return build_lr_scheduler(cfg, optimizer)\n",
    "\n",
    "    def train(self):\n",
    "        self.train_loop(self.start_iter, self.max_iter)\n",
    "        if hasattr(self, \"_last_eval_results\") and comm.is_main_process():\n",
    "            verify_results(self.cfg, self._last_eval_results)\n",
    "            return self._last_eval_results\n",
    "\n",
    "    def train_loop(self, start_iter: int, max_iter: int):\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.info(\"Starting training from iteration {}\".format(start_iter))\n",
    "\n",
    "        self.iter = self.start_iter = start_iter\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "        with EventStorage(start_iter) as self.storage:\n",
    "            try:\n",
    "                self.before_train()\n",
    "\n",
    "                for self.iter in range(start_iter, max_iter):\n",
    "                    self.before_step()\n",
    "                    self.run_step_full_semisup()\n",
    "                    self.after_step()\n",
    "            except Exception:\n",
    "                logger.exception(\"Exception during training:\")\n",
    "                raise\n",
    "            finally:\n",
    "                self.after_train()\n",
    "\n",
    "    # =====================================================\n",
    "    # ================== Pseduo-labeling ==================\n",
    "    # =====================================================\n",
    "    def threshold_bbox(self, proposal_bbox_inst, thres=0.7, proposal_type=\"roih\"):\n",
    "        if proposal_type == \"roih\":\n",
    "            valid_map = proposal_bbox_inst.scores > thres\n",
    "\n",
    "            # create instances containing boxes and gt_classes\n",
    "            image_shape = proposal_bbox_inst.image_size\n",
    "            new_proposal_inst = Instances(image_shape)\n",
    "\n",
    "            # create box\n",
    "            new_bbox_loc = proposal_bbox_inst.pred_boxes.tensor[valid_map, :]\n",
    "            new_boxes = Boxes(new_bbox_loc)\n",
    "\n",
    "            # add boxes to instances\n",
    "            new_proposal_inst.gt_boxes = new_boxes\n",
    "            new_proposal_inst.gt_classes = proposal_bbox_inst.pred_classes[valid_map]\n",
    "            new_proposal_inst.scores = proposal_bbox_inst.scores[valid_map]\n",
    "\n",
    "            if proposal_bbox_inst.has(\"pred_boxes_std\"):\n",
    "                new_proposal_inst.pred_boxes_std = proposal_bbox_inst.pred_boxes_std[\n",
    "                    valid_map, :\n",
    "                ]\n",
    "        else:\n",
    "            raise ValueError(\"Error in proposal type.\")\n",
    "\n",
    "        return new_proposal_inst\n",
    "\n",
    "    def process_pseudo_label(\n",
    "        self, proposals_rpn_unsup_k, cur_threshold, proposal_type, psedo_label_method=\"\"\n",
    "    ):\n",
    "        list_instances = []\n",
    "        num_proposal_output = 0.0\n",
    "        for proposal_bbox_inst in proposals_rpn_unsup_k:\n",
    "            # thresholding\n",
    "            if psedo_label_method == \"thresholding\":\n",
    "                proposal_bbox_inst = self.threshold_bbox(\n",
    "                    proposal_bbox_inst, thres=cur_threshold, proposal_type=proposal_type\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(\"Unkown pseudo label boxes methods\")\n",
    "            num_proposal_output += len(proposal_bbox_inst)\n",
    "            list_instances.append(proposal_bbox_inst)\n",
    "        num_proposal_output = num_proposal_output / len(proposals_rpn_unsup_k)\n",
    "        return list_instances, num_proposal_output\n",
    "\n",
    "    def remove_label(self, label_data):\n",
    "        for label_datum in label_data:\n",
    "            if \"instances\" in label_datum.keys():\n",
    "                del label_datum[\"instances\"]\n",
    "        return label_data\n",
    "\n",
    "    def add_label(self, unlabled_data, label):\n",
    "        for unlabel_datum, lab_inst in zip(unlabled_data, label):\n",
    "            unlabel_datum[\"instances\"] = lab_inst\n",
    "        return unlabled_data\n",
    "\n",
    "    # =====================================================\n",
    "    # =================== Training Flow ===================\n",
    "    # =====================================================\n",
    "\n",
    "    def run_step_full_semisup(self):\n",
    "        self._trainer.iter = self.iter\n",
    "        assert self.model.training, \"[UBTeacherTrainer] model was changed to eval mode!\"\n",
    "        start = time.perf_counter()\n",
    "        data = next(self._trainer._data_loader_iter)\n",
    "        # data_q and data_k from different augmentations (q:strong, k:weak)\n",
    "        # label_strong, label_weak, unlabed_strong, unlabled_weak\n",
    "        label_data_q, label_data_k, unlabel_data_q, unlabel_data_k = data\n",
    "        data_time = time.perf_counter() - start\n",
    "\n",
    "        # burn-in stage (supervised training with labeled data)\n",
    "        if self.iter < self.cfg.SEMISUPNET.BURN_UP_STEP:\n",
    "\n",
    "            # input both strong and weak supervised data into model\n",
    "            if self.cfg.SEMISUPNET.USE_SUP_STRONG == \"both\":\n",
    "                all_label_data = label_data_q + label_data_k\n",
    "            else:\n",
    "                all_label_data = label_data_k\n",
    "\n",
    "            record_dict, _, _, _ = self.model(all_label_data, branch=\"supervised\")\n",
    "\n",
    "            # weight losses\n",
    "            loss_dict = {}\n",
    "            for key in record_dict.keys():\n",
    "                if key[:4] == \"loss\":\n",
    "                    loss_dict[key] = record_dict[key]\n",
    "            losses = sum(loss_dict.values())\n",
    "\n",
    "        else:\n",
    "            # copy student model to teacher model\n",
    "            if self.iter == self.cfg.SEMISUPNET.BURN_UP_STEP:\n",
    "                self._update_teacher_model(keep_rate=0.0)\n",
    "\n",
    "            if (\n",
    "                self.iter - self.cfg.SEMISUPNET.BURN_UP_STEP\n",
    "            ) % self.cfg.SEMISUPNET.TEACHER_UPDATE_ITER == 0:\n",
    "\n",
    "                cur_ema_rate = self.cfg.SEMISUPNET.EMA_KEEP_RATE\n",
    "                self._update_teacher_model(keep_rate=cur_ema_rate)\n",
    "\n",
    "            record_dict = {}\n",
    "            record_dict[\"EMA_rate\"] = cur_ema_rate\n",
    "\n",
    "            #  generate the pseudo-label using teacher model\n",
    "            # note that we do not convert to eval mode, as 1) there is no gradient computed in\n",
    "            # teacher model and 2) batch norm layers are not updated as well\n",
    "            with torch.no_grad():\n",
    "                (\n",
    "                    _,\n",
    "                    proposals_rpn_unsup_k,\n",
    "                    proposals_roih_unsup_k,\n",
    "                    _,\n",
    "                ) = self.model_teacher(unlabel_data_k, branch=\"unsup_data_weak\")\n",
    "\n",
    "            #  Pseudo-labeling\n",
    "            cur_threshold = self.cfg.SEMISUPNET.BBOX_THRESHOLD\n",
    "            joint_proposal_dict = {}\n",
    "\n",
    "            # Pseudo_labeling for ROI head (bbox location/objectness)\n",
    "            pesudo_proposals_roih_unsup_k, _ = self.process_pseudo_label(\n",
    "                proposals_roih_unsup_k, cur_threshold, \"roih\", \"thresholding\"\n",
    "            )\n",
    "            joint_proposal_dict[\"proposals_pseudo_roih\"] = pesudo_proposals_roih_unsup_k\n",
    "\n",
    "            #  add pseudo-label to unlabeled data\n",
    "            unlabel_data_q = self.remove_label(unlabel_data_q)\n",
    "            unlabel_data_k = self.remove_label(unlabel_data_k)\n",
    "\n",
    "            unlabel_data_q = self.add_label(\n",
    "                unlabel_data_q, joint_proposal_dict[\"proposals_pseudo_roih\"]\n",
    "            )\n",
    "            unlabel_data_k = self.add_label(\n",
    "                unlabel_data_k, joint_proposal_dict[\"proposals_pseudo_roih\"]\n",
    "            )\n",
    "\n",
    "            if self.cfg.SEMISUPNET.USE_SUP_STRONG == \"both\":\n",
    "                all_label_data = label_data_q + label_data_k\n",
    "            else:\n",
    "                all_label_data = label_data_k\n",
    "\n",
    "            all_unlabel_data = unlabel_data_q\n",
    "\n",
    "            record_all_label_data, _, _, _ = self.model(\n",
    "                all_label_data, branch=\"supervised\"\n",
    "            )\n",
    "            record_dict.update(record_all_label_data)\n",
    "\n",
    "            record_all_unlabel_data, _, _, _ = self.model(\n",
    "                all_unlabel_data, branch=\"unsup_data_train\"\n",
    "            )\n",
    "\n",
    "            new_record_all_unlabel_data = {}\n",
    "            for key in record_all_unlabel_data.keys():\n",
    "                new_record_all_unlabel_data[key + \"_pseudo\"] = record_all_unlabel_data[\n",
    "                    key\n",
    "                ]\n",
    "            record_dict.update(new_record_all_unlabel_data)\n",
    "\n",
    "            # weight losses\n",
    "            loss_dict = {}\n",
    "            for key in record_dict.keys():\n",
    "                if key[:4] == \"loss\":\n",
    "                    if key == \"loss_rpn_loc_pseudo\":\n",
    "                        # pseudo RPN bbox regression <- 0\n",
    "                        loss_dict[key] = record_dict[key] * 0\n",
    "                    elif key == \"loss_box_reg_pseudo\":\n",
    "                        # pseudo ROIhead box regression\n",
    "                        loss_dict[key] = (\n",
    "                            record_dict[key] * self.cfg.SEMISUPNET.UNSUP_REG_LOSS_WEIGHT\n",
    "                        )\n",
    "                    elif key[-6:] == \"pseudo\":\n",
    "                        # pseudo RPN, ROIhead classification\n",
    "                        loss_dict[key] = (\n",
    "                            record_dict[key] * self.cfg.SEMISUPNET.UNSUP_LOSS_WEIGHT\n",
    "                        )\n",
    "                    else:  # supervised loss\n",
    "                        loss_dict[key] = record_dict[key]\n",
    "\n",
    "            losses = sum(loss_dict.values())\n",
    "\n",
    "        metrics_dict = record_dict\n",
    "        metrics_dict[\"data_time\"] = data_time\n",
    "        self._write_metrics(metrics_dict)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def _write_metrics(self, metrics_dict):\n",
    "        metrics_dict = {\n",
    "            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\n",
    "            for k, v in metrics_dict.items()\n",
    "        }\n",
    "\n",
    "        # gather metrics among all workers for logging\n",
    "        # This assumes we do DDP-style training, which is currently the only\n",
    "        # supported method in detectron2.\n",
    "        all_metrics_dict = comm.gather(metrics_dict)\n",
    "\n",
    "        if comm.is_main_process():\n",
    "            if \"data_time\" in all_metrics_dict[0]:\n",
    "                # data_time among workers can have high variance. The actual latency\n",
    "                # caused by data_time is the maximum among workers.\n",
    "                data_time = np.max([x.pop(\"data_time\") for x in all_metrics_dict])\n",
    "                self.storage.put_scalar(\"data_time\", data_time)\n",
    "\n",
    "            # average the rest metrics\n",
    "            metrics_dict = {\n",
    "                k: np.mean([x[k] for x in all_metrics_dict])\n",
    "                for k in all_metrics_dict[0].keys()\n",
    "            }\n",
    "\n",
    "            # append the list\n",
    "            loss_dict = {}\n",
    "            for key in metrics_dict.keys():\n",
    "                if key[:4] == \"loss\":\n",
    "                    loss_dict[key] = metrics_dict[key]\n",
    "\n",
    "            total_losses_reduced = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            self.storage.put_scalar(\"total_loss\", total_losses_reduced)\n",
    "            if len(metrics_dict) > 1:\n",
    "                self.storage.put_scalars(**metrics_dict)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _update_teacher_model(self, keep_rate=0.996):\n",
    "        if comm.get_world_size() > 1:\n",
    "            student_model_dict = {\n",
    "                key[7:]: value for key, value in self.model.state_dict().items()\n",
    "            }\n",
    "        else:\n",
    "            student_model_dict = self.model.state_dict()\n",
    "\n",
    "        new_teacher_dict = OrderedDict()\n",
    "        for key, value in self.model_teacher.state_dict().items():\n",
    "            if key in student_model_dict.keys():\n",
    "                new_teacher_dict[key] = (\n",
    "                    student_model_dict[key] * (1 - keep_rate) + value * keep_rate\n",
    "                )\n",
    "            else:\n",
    "                raise Exception(\"{} is not found in student model\".format(key))\n",
    "\n",
    "        self.model_teacher.load_state_dict(new_teacher_dict)\n",
    "\n",
    "    @classmethod\n",
    "    def build_test_loader(cls, cfg, dataset_name):\n",
    "        return build_detection_test_loader(cfg, dataset_name)\n",
    "\n",
    "    def build_hooks(self):\n",
    "        cfg = self.cfg.clone()\n",
    "        cfg.defrost()\n",
    "        cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN\n",
    "\n",
    "        ret = [\n",
    "            hooks.IterationTimer(),\n",
    "            hooks.LRScheduler(self.optimizer, self.scheduler),\n",
    "            hooks.PreciseBN(\n",
    "                # Run at the same freq as (but before) evaluation.\n",
    "                cfg.TEST.EVAL_PERIOD,\n",
    "                self.model,\n",
    "                # Build a new data loader to not affect training\n",
    "                self.build_train_loader(cfg),\n",
    "                cfg.TEST.PRECISE_BN.NUM_ITER,\n",
    "            )\n",
    "            if cfg.TEST.PRECISE_BN.ENABLED and get_bn_modules(self.model)\n",
    "            else None,\n",
    "        ]\n",
    "\n",
    "        # Do PreciseBN before checkpointer, because it updates the model and need to\n",
    "        # be saved by checkpointer.\n",
    "        # This is not always the best: if checkpointing has a different frequency,\n",
    "        # some checkpoints may have more precise statistics than others.\n",
    "        if comm.is_main_process():\n",
    "            ret.append(\n",
    "                hooks.PeriodicCheckpointer(\n",
    "                    self.checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD\n",
    "                )\n",
    "            )\n",
    "\n",
    "        def test_and_save_results_student():\n",
    "            self._last_eval_results_student = self.test(self.cfg, self.model)\n",
    "            _last_eval_results_student = {\n",
    "                k + \"_student\": self._last_eval_results_student[k]\n",
    "                for k in self._last_eval_results_student.keys()\n",
    "            }\n",
    "            return _last_eval_results_student\n",
    "\n",
    "        def test_and_save_results_teacher():\n",
    "            self._last_eval_results_teacher = self.test(self.cfg, self.model_teacher)\n",
    "            return self._last_eval_results_teacher\n",
    "\n",
    "        ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results_student))\n",
    "        ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results_teacher))\n",
    "\n",
    "        if comm.is_main_process():\n",
    "            # run writers in the end, so that evaluation metrics are written\n",
    "            ret.append(hooks.PeriodicWriter(self.build_writers(), period=20))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.engine import default_argument_parser, default_setup, launch\n",
    "\n",
    "# hacky way to register\n",
    "# from ubteacher.modeling import *\n",
    "# from ubteacher.engine import *\n",
    "# from ubteacher import add_ubteacher_config\n",
    "\n",
    "\n",
    "\n",
    "def setup(args):\n",
    "    \"\"\"\n",
    "    Create configs and perform basic setups.\n",
    "    \"\"\"\n",
    "    cfg = get_cfg()\n",
    "    add_ubteacher_config(cfg)\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    cfg.freeze()\n",
    "    default_setup(cfg, args)\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    cfg = setup(args)\n",
    "    if cfg.SEMISUPNET.Trainer == \"ubteacher\":\n",
    "        Trainer = UBTeacherTrainer\n",
    "    elif cfg.SEMISUPNET.Trainer == \"ubteacher_rcnn\":\n",
    "        Trainer = UBRCNNTeacherTrainer\n",
    "    else:\n",
    "        raise ValueError(\"Trainer Name is not found.\")\n",
    "\n",
    "    if args.eval_only:\n",
    "        if cfg.SEMISUPNET.Trainer == \"ubteacher\":\n",
    "            model = Trainer.build_model(cfg)\n",
    "            model_teacher = Trainer.build_model(cfg)\n",
    "            ensem_ts_model = EnsembleTSModel(model_teacher, model)\n",
    "\n",
    "            DetectionCheckpointer(\n",
    "                ensem_ts_model, save_dir=cfg.OUTPUT_DIR\n",
    "            ).resume_or_load(cfg.MODEL.WEIGHTS, resume=args.resume)\n",
    "            res = Trainer.test(cfg, ensem_ts_model.modelTeacher)\n",
    "\n",
    "        else:\n",
    "            model = Trainer.build_model(cfg)\n",
    "            DetectionCheckpointer(model, save_dir=cfg.OUTPUT_DIR).resume_or_load(\n",
    "                cfg.MODEL.WEIGHTS, resume=args.resume\n",
    "            )\n",
    "            res = Trainer.test(cfg, model)\n",
    "        return res\n",
    "\n",
    "    trainer = Trainer(cfg)\n",
    "    trainer.resume_or_load(resume=args.resume)\n",
    "\n",
    "    return trainer.train()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = default_argument_parser().parse_args()\n",
    "\n",
    "    print(\"Command Line Args:\", args)\n",
    "    launch(  \n",
    "        main,\n",
    "        args.num_gpus,\n",
    "        num_machines=args.num_machines,\n",
    "        machine_rank=args.machine_rank,\n",
    "        dist_url=args.dist_url,\n",
    "        args=(args,),\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "with_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
